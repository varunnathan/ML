{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "LOCAL_ROOT = \"/Users/varunn/Documents/ML\"\n",
    "PROJ_DIR = os.path.join(LOCAL_ROOT, \"AV_JanataHack_April10\")\n",
    "DATA_DIR = os.path.join(PROJ_DIR, \"data\")\n",
    "TRAIN_FN = os.path.join(DATA_DIR, \"train_8wry4cB.csv\")\n",
    "TEST_FN = os.path.join(DATA_DIR, \"test_Yix80N0.csv\")\n",
    "SUBMISSION_FN = os.path.join(DATA_DIR, \"sample_submission_opxHi4g.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.isfile(TRAIN_FN)\n",
    "assert os.path.isfile(TEST_FN)\n",
    "assert os.path.isfile(SUBMISSION_FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10500, 5)\n",
      "  session_id       startTime         endTime  \\\n",
      "0     u16159  15/12/14 18:11  15/12/14 18:12   \n",
      "1     u10253  16/12/14 14:35  16/12/14 14:41   \n",
      "2     u19037  01/12/14 15:58  01/12/14 15:58   \n",
      "3     u14556   23/11/14 2:57   23/11/14 3:00   \n",
      "4     u24295  17/12/14 16:44  17/12/14 16:46   \n",
      "\n",
      "                                         ProductList  gender  \n",
      "0  A00002/B00003/C00006/D28435/;A00002/B00003/C00...  female  \n",
      "1  A00001/B00009/C00031/D29404/;A00001/B00009/C00...    male  \n",
      "2                       A00002/B00001/C00020/D16944/  female  \n",
      "3  A00002/B00004/C00018/D10284/;A00002/B00004/C00...  female  \n",
      "4  A00001/B00001/C00012/D30805/;A00001/B00001/C00...    male  \n",
      "(4500, 4)\n",
      "  session_id       startTime         endTime  \\\n",
      "0     u12112  08/12/14 13:36  08/12/14 13:36   \n",
      "1     u19725  19/12/14 13:52  19/12/14 13:52   \n",
      "2     u11795  01/12/14 10:44  01/12/14 10:44   \n",
      "3     u22639  08/12/14 20:19  08/12/14 20:22   \n",
      "4     u18034  15/12/14 19:33  15/12/14 19:33   \n",
      "\n",
      "                                         ProductList  \n",
      "0                       A00002/B00003/C00006/D19956/  \n",
      "1                       A00002/B00005/C00067/D02026/  \n",
      "2                       A00002/B00002/C00004/D12538/  \n",
      "3  A00002/B00003/C00079/D22781/;A00002/B00003/C00...  \n",
      "4                       A00002/B00001/C00010/D23419/  \n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "train_df = pd.read_csv(TRAIN_FN)\n",
    "print(train_df.shape)\n",
    "print(train_df.head())\n",
    "\n",
    "test_df = pd.read_csv(TEST_FN)\n",
    "print(test_df.shape)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sessions:  10500 \n",
      "\n",
      "female    8192\n",
      "male      2308\n",
      "Name: gender, dtype: int64 \n",
      "\n",
      "Missing values\n",
      "session_id     0\n",
      "startTime      0\n",
      "endTime        0\n",
      "ProductList    0\n",
      "gender         0\n",
      "dtype: int64 \n",
      "\n",
      "dtype\n",
      "session_id     object\n",
      "startTime      object\n",
      "endTime        object\n",
      "ProductList    object\n",
      "gender         object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Train Stats\n",
    "print('# sessions: ', train_df['session_id'].nunique(), '\\n')\n",
    "assert train_df['session_id'].nunique() == train_df.shape[0]\n",
    "print(train_df['gender'].value_counts(), '\\n')\n",
    "print('Missing values')\n",
    "print(train_df.isnull().sum(), '\\n')\n",
    "print('dtype')\n",
    "print(train_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "ID_COL = 'session_id'\n",
    "DATE_COLS = ['startTime', 'endTime']\n",
    "DV_COL = 'gender'\n",
    "FEAT_PREFIX = 'JH_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding\n",
    "train_df[DV_COL] = train_df[DV_COL].apply(\n",
    "    lambda x: 1 if x=='male' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21980952380952382\n"
     ]
    }
   ],
   "source": [
    "print(train_df.gender.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startTime\n",
      "endTime\n"
     ]
    }
   ],
   "source": [
    "# datetime conversion\n",
    "for col in DATE_COLS:\n",
    "    print(col)\n",
    "    train_df[col] = train_df[col].apply(lambda x: pd.to_datetime(x))\n",
    "    test_df[col] = test_df[col].apply(lambda x: pd.to_datetime(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_part_of_day(hour):\n",
    "    return (\n",
    "        \"morning\" if 5 <= hour <= 11\n",
    "        else\n",
    "        \"afternoon\" if 12 <= hour <= 17\n",
    "        else\n",
    "        \"evening\" if 18 <= hour <= 22\n",
    "        else\n",
    "        \"night\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime features\n",
    "\n",
    "# session duration\n",
    "train_df['session_duration_sec'] = list(map(lambda x, y: (x-y).seconds,\n",
    "                                            train_df['endTime'],\n",
    "                                            train_df['startTime']))\n",
    "test_df['session_duration_sec'] = list(map(lambda x, y: (x-y).seconds,\n",
    "                                           test_df['endTime'],\n",
    "                                           test_df['startTime']))\n",
    "\n",
    "# month of year\n",
    "train_df['session_start_month'] = train_df['startTime'].dt.month\n",
    "test_df['session_start_month'] = test_df['startTime'].dt.month\n",
    "train_df['session_end_month'] = train_df['endTime'].dt.month\n",
    "test_df['session_end_month'] = test_df['endTime'].dt.month\n",
    "\n",
    "# day of week\n",
    "train_df['session_start_dow'] = train_df['startTime'].dt.dayofweek\n",
    "test_df['session_start_dow'] = test_df['startTime'].dt.dayofweek\n",
    "train_df['session_end_dow'] = train_df['endTime'].dt.dayofweek\n",
    "test_df['session_end_dow'] = test_df['endTime'].dt.dayofweek\n",
    "\n",
    "# week of year\n",
    "train_df['session_start_woy'] = train_df['startTime'].dt.weekofyear\n",
    "test_df['session_start_woy'] = test_df['startTime'].dt.weekofyear\n",
    "train_df['session_end_woy'] = train_df['endTime'].dt.weekofyear\n",
    "test_df['session_end_woy'] = test_df['endTime'].dt.weekofyear\n",
    "\n",
    "# hour of the day\n",
    "train_df['session_start_hod'] = train_df['startTime'].dt.hour\n",
    "test_df['session_start_hod'] = test_df['startTime'].dt.hour\n",
    "train_df['session_end_hod'] = train_df['endTime'].dt.hour\n",
    "test_df['session_end_hod'] = test_df['endTime'].dt.hour\n",
    "\n",
    "# minute of the hour\n",
    "train_df['session_start_moh'] = train_df['startTime'].dt.minute\n",
    "test_df['session_start_moh'] = test_df['startTime'].dt.minute\n",
    "train_df['session_end_moh'] = train_df['endTime'].dt.minute\n",
    "test_df['session_end_moh'] = test_df['endTime'].dt.minute\n",
    "\n",
    "# part of the day\n",
    "train_df['session_start_part_of_day'] = train_df['session_start_hod'].apply(\n",
    "    lambda x: get_part_of_day(x))\n",
    "test_df['session_start_part_of_day'] = test_df['session_start_hod'].apply(\n",
    "    lambda x: get_part_of_day(x))\n",
    "train_df['session_end_part_of_day'] = train_df['session_end_hod'].apply(\n",
    "    lambda x: get_part_of_day(x))\n",
    "test_df['session_end_part_of_day'] = test_df['session_end_hod'].apply(\n",
    "    lambda x: get_part_of_day(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [],
   "source": [
    "# productlist parsing\n",
    "\n",
    "def parse_product_list(productList):\n",
    "    out = [x.split('/') for x in productList.split(';')]\n",
    "    prods = list(set([x[0] for x in out]))\n",
    "    cats = list(set([x[1] for x in out]))\n",
    "    sub_cats = list(set([x[2] for x in out]))\n",
    "    sub_sub_cats = list(set([x[3] for x in out]))\n",
    "    return prods, cats, sub_cats, sub_sub_cats\n",
    "\n",
    "\n",
    "def get_prod_cat_lst(productlist):\n",
    "    return list(set(['/'.join(x.split('/')[:2])\n",
    "                     for x in productlist.split(';')]))\n",
    "\n",
    "\n",
    "def get_prod_cat_str(productlist):\n",
    "    return \";\".join(set(['/'.join(x.split('/')[:2])\n",
    "                         for x in productlist.split(';')]))\n",
    "\n",
    "\n",
    "def get_prod_cat_sub_cat_lst(productlist):\n",
    "    return list(set(['/'.join(x.split('/')[:3])\n",
    "                     for x in productlist.split(';')]))\n",
    "\n",
    "\n",
    "def get_prod_cat_sub_cat_str(productlist):\n",
    "    return \";\".join(set(['/'.join(x.split('/')[:3])\n",
    "                         for x in productlist.split(';')]))\n",
    "\n",
    "\n",
    "def get_prod_cat_sub_sub_cat_lst(productlist):\n",
    "    return list(set(['/'.join(x.split('/')[:4])\n",
    "                     for x in productlist.split(';')]))\n",
    "\n",
    "\n",
    "def get_prod_cat_sub_sub_cat_str(productlist):\n",
    "    return \";\".join(set(['/'.join(x.split('/')[:4])\n",
    "                         for x in productlist.split(';')]))\n",
    "\n",
    "\n",
    "def parse_product_list_df(data):\n",
    "    \n",
    "    parsed_prod_lists = data['ProductList'].apply(\n",
    "        lambda x: parse_product_list(x))\n",
    "    for idx, col in [(0, 'prod_lst'), (1, 'cat_lst'), (2, 'sub_cat_lst'),\n",
    "                     (3, 'sub_sub_cat_lst')]:\n",
    "        data[col] = parsed_prod_lists.apply(lambda x: x[idx])\n",
    "    \n",
    "    data['prod_str'] = data['prod_lst'].apply(lambda x: '/'.join(x))\n",
    "    data['cat_str'] = data['cat_lst'].apply(lambda x: '/'.join(x))\n",
    "    data['sub_cat_str'] = data['sub_cat_lst'].apply(lambda x: '/'.join(x))\n",
    "    data['sub_sub_cat_str'] = data['sub_sub_cat_lst'].apply(lambda x: '/'.join(x))\n",
    "    data['prod_cat_lst'] = data['ProductList'].apply(get_prod_cat_lst)\n",
    "    data['prod_cat_str'] = data['ProductList'].apply(get_prod_cat_str)\n",
    "    data['prod_cat_sub_cat_lst'] = data['ProductList'].apply(\n",
    "        get_prod_cat_sub_cat_lst)\n",
    "    data['prod_cat_sub_cat_str'] = data['ProductList'].apply(\n",
    "        get_prod_cat_sub_cat_str)\n",
    "    data['prod_cat_sub_sub_cat_lst'] = data['ProductList'].apply(\n",
    "        get_prod_cat_sub_sub_cat_lst)\n",
    "    data['prod_cat_sub_sub_cat_str'] = data['ProductList'].apply(\n",
    "        get_prod_cat_sub_sub_cat_str)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = parse_product_list_df(train_df)\n",
    "test_df = parse_product_list_df(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10500, 32)\n",
      "  session_id           startTime             endTime  \\\n",
      "0     u16159 2014-12-15 18:11:00 2014-12-15 18:12:00   \n",
      "1     u10253 2014-12-16 14:35:00 2014-12-16 14:41:00   \n",
      "2     u19037 2014-01-12 15:58:00 2014-01-12 15:58:00   \n",
      "3     u14556 2014-11-23 02:57:00 2014-11-23 03:00:00   \n",
      "4     u24295 2014-12-17 16:44:00 2014-12-17 16:46:00   \n",
      "\n",
      "                                         ProductList  gender  \\\n",
      "0  A00002/B00003/C00006/D28435/;A00002/B00003/C00...       0   \n",
      "1  A00001/B00009/C00031/D29404/;A00001/B00009/C00...       1   \n",
      "2                       A00002/B00001/C00020/D16944/       0   \n",
      "3  A00002/B00004/C00018/D10284/;A00002/B00004/C00...       0   \n",
      "4  A00001/B00001/C00012/D30805/;A00001/B00001/C00...       1   \n",
      "\n",
      "   session_duration_sec  session_start_month  session_end_month  \\\n",
      "0                    60                   12                 12   \n",
      "1                   360                   12                 12   \n",
      "2                     0                    1                  1   \n",
      "3                   180                   11                 11   \n",
      "4                   120                   12                 12   \n",
      "\n",
      "   session_start_dow  session_end_dow  ...  prod_str  cat_str  sub_cat_str  \\\n",
      "0                  0                0  ...    A00002   B00003       C00006   \n",
      "1                  1                1  ...    A00001   B00009       C00031   \n",
      "2                  6                6  ...    A00002   B00001       C00020   \n",
      "3                  6                6  ...    A00002   B00004       C00018   \n",
      "4                  2                2  ...    A00001   B00001       C00012   \n",
      "\n",
      "                                    sub_sub_cat_str     prod_cat_lst  \\\n",
      "0                       D28435/D28437/D28436/D02554  [A00002/B00003]   \n",
      "1  D29418/D25444/D29407/D29404/D29410/D02617/D29411  [A00001/B00009]   \n",
      "2                                            D16944  [A00002/B00001]   \n",
      "3                              D10284/D10286/D10285  [A00002/B00004]   \n",
      "4                                     D30806/D30805  [A00001/B00001]   \n",
      "\n",
      "    prod_cat_str    prod_cat_sub_cat_lst  prod_cat_sub_cat_str  \\\n",
      "0  A00002/B00003  [A00002/B00003/C00006]  A00002/B00003/C00006   \n",
      "1  A00001/B00009  [A00001/B00009/C00031]  A00001/B00009/C00031   \n",
      "2  A00002/B00001  [A00002/B00001/C00020]  A00002/B00001/C00020   \n",
      "3  A00002/B00004  [A00002/B00004/C00018]  A00002/B00004/C00018   \n",
      "4  A00001/B00001  [A00001/B00001/C00012]  A00001/B00001/C00012   \n",
      "\n",
      "                            prod_cat_sub_sub_cat_lst  \\\n",
      "0  [A00002/B00003/C00006/D28437, A00002/B00003/C0...   \n",
      "1  [A00001/B00009/C00031/D29407, A00001/B00009/C0...   \n",
      "2                      [A00002/B00001/C00020/D16944]   \n",
      "3  [A00002/B00004/C00018/D10286, A00002/B00004/C0...   \n",
      "4  [A00001/B00001/C00012/D30806, A00001/B00001/C0...   \n",
      "\n",
      "                            prod_cat_sub_sub_cat_str  \n",
      "0  A00002/B00003/C00006/D28437;A00002/B00003/C000...  \n",
      "1  A00001/B00009/C00031/D29407;A00001/B00009/C000...  \n",
      "2                        A00002/B00001/C00020/D16944  \n",
      "3  A00002/B00004/C00018/D10286;A00002/B00004/C000...  \n",
      "4  A00001/B00001/C00012/D30806;A00001/B00001/C000...  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "497\n",
      "329\n"
     ]
    }
   ],
   "source": [
    "print(train_df['prod_cat_str'].nunique())\n",
    "print(test_df['prod_cat_str'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "\n",
    "def _find_cats(data, id_col, cat_col, freq_thresh):\n",
    "    \"\"\"\n",
    "    identifies values of discrete variables that satisfy the frequency threshold\n",
    "    id_col: deal_id or the entity of choice\n",
    "    cat_col: discrete variable\n",
    "    \"\"\"\n",
    "    cats = data.groupby(cat_col)[id_col].nunique().to_dict()\n",
    "    freq_thresh = int(freq_thresh*data[id_col].nunique())\n",
    "    good_cats = [k for k, v in cats.items() if v >= freq_thresh]\n",
    "\n",
    "    return good_cats\n",
    "\n",
    "\n",
    "def _group_cats(data, cat_col, id_col=ID_COL, dv_col=DV_COL,\n",
    "                freq_thresh=0.003):\n",
    "\n",
    "    df = pd.DataFrame({id_col: np.repeat(data[id_col].values,\n",
    "                                         data[cat_col].str.len()),\n",
    "                       dv_col: np.repeat(data[dv_col].values,\n",
    "                                         data[cat_col].str.len()),\n",
    "                       cat_col: np.concatenate(data[cat_col].values)})\n",
    "    grouped_cats = _find_cats(df, id_col, cat_col, freq_thresh)\n",
    "    mask = df[cat_col].isin(grouped_cats)\n",
    "    df.loc[~mask, cat_col] = 'others'\n",
    "\n",
    "    return grouped_cats, df\n",
    "\n",
    "\n",
    "def _apply_grouping(test_data, cat_col, grouped_cats, id_col=ID_COL,\n",
    "                    dv_col=DV_COL):\n",
    "    \n",
    "    df = pd.DataFrame({id_col: np.repeat(test_data[id_col].values,\n",
    "                                         test_data[cat_col].str.len()),\n",
    "                       cat_col: np.concatenate(test_data[cat_col].values)})\n",
    "    mask = df[cat_col].isin(grouped_cats)\n",
    "    df.loc[~mask, cat_col] = 'others'\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_woe(data, cat_col, id_col=ID_COL, dv_col=DV_COL):\n",
    "    table = pd.pivot_table(data, index=cat_col, columns=dv_col,\n",
    "                           values=id_col,\n",
    "                           aggfunc=lambda x: len(x.unique()),\n",
    "                           margins=True)\n",
    "    table.fillna(value=0, inplace=True)\n",
    "    table.reset_index(inplace=True)\n",
    "    table.rename(columns={0: '#good', 1: '#bad'}, inplace=True)\n",
    "    total_good = table.loc[len(table)-1, '#good']\n",
    "    total_bad = table.loc[len(table)-1, '#bad']\n",
    "    table['perc_good'] = table['#good'].apply(lambda x: 1.*x/total_good)\n",
    "    table['perc_bad'] = table['#bad'].apply(lambda x: 1.*x/total_bad)\n",
    "    mask = (table['perc_good'] != 0) & (table['perc_bad'] != 0)\n",
    "    table.loc[mask, 'WOE'] = list(map(\n",
    "     lambda x, y: log(x / float(y)), table.loc[mask, 'perc_good'],\n",
    "     table.loc[mask, 'perc_bad']))\n",
    "    return table\n",
    "\n",
    "\n",
    "def get_counts(data, cat_col, id_col=ID_COL):\n",
    "    table = pd.DataFrame(data.groupby(cat_col)[id_col].nunique())\n",
    "    table.reset_index(inplace=True)\n",
    "    table.rename(columns={id_col: 'num_sessions'}, inplace=True)\n",
    "    return table\n",
    "\n",
    "\n",
    "def getDVEncodeVar(compute_df, target_df, cat_col, dv_col=DV_COL,\n",
    "                   encode_type='woe'):\n",
    "\n",
    "    if encode_type == 'woe':\n",
    "        assert dv_col in target_df\n",
    "        grouped_df = get_woe(target_df, cat_col)\n",
    "        return_col = 'WOE'\n",
    "    elif encode_type == 'count':\n",
    "        grouped_df = get_counts(target_df, cat_col)\n",
    "        return_col = 'num_sessions'\n",
    "    merged_df = pd.merge(compute_df, grouped_df, how=\"left\", on=cat_col)\n",
    "    merged_df.fillna(-999999, inplace=True)\n",
    "    return merged_df[return_col].tolist()\n",
    "\n",
    "\n",
    "def _aggregation(data, cat_col, id_col=ID_COL, dv_col=DV_COL,\n",
    "                 enc_prefix='_enc'):\n",
    "    if dv_col in data:\n",
    "        group_cols = [id_col, dv_col]\n",
    "    else:\n",
    "        group_cols = [id_col]\n",
    "    df = pd.DataFrame(data.groupby(group_cols).agg({\n",
    "        cat_col: lambda x: list(x), cat_col+enc_prefix: lambda x: list(x)}))\n",
    "    df.reset_index(inplace=True)\n",
    "    assert df.shape[0] == df[id_col].nunique()\n",
    "    return df\n",
    "\n",
    "\n",
    "def do_target_encode_woe(train_data, test_data, discrete_cols,\n",
    "                         id_col=ID_COL, dv_col=DV_COL,\n",
    "                         grouping_flag=True, agg_flag=True):\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "    train_final = train_data.copy()\n",
    "    test_final = test_data.copy()\n",
    "    for col in discrete_cols:\n",
    "        print('Cat Col: ', col)\n",
    "        \n",
    "        if grouping_flag:\n",
    "            print('grouping')\n",
    "            grouped_cats, train_tmp = _group_cats(train_data, col)\n",
    "            test_tmp = _apply_grouping(test_data, col, grouped_cats)\n",
    "        else:\n",
    "            train_tmp = train_data.copy()\n",
    "            test_tmp = test_data.copy()\n",
    "        \n",
    "        print('encoding')\n",
    "        train_enc_values = np.zeros(train_tmp.shape[0])\n",
    "        test_enc_values = 0\n",
    "        for dev_index, val_index in kf.split(train_tmp):\n",
    "            dev_X, val_X = train_tmp.iloc[dev_index, :], train_tmp.iloc[val_index, :]\n",
    "            train_enc_values[val_index] =  np.array( \n",
    "                getDVEncodeVar(val_X[[col]], dev_X, col))\n",
    "            test_enc_values += np.array( \n",
    "                getDVEncodeVar(test_tmp[[col]], dev_X, col))\n",
    "        test_enc_values /= 5.\n",
    "        train_tmp[col + \"_enc\"] = train_enc_values\n",
    "        test_tmp[col + \"_enc\"] = test_enc_values\n",
    "        print(train_tmp[col + \"_enc\"].describe())\n",
    "        print(test_tmp[col + \"_enc\"].describe())\n",
    "        \n",
    "        if agg_flag:\n",
    "            print('aggregating')\n",
    "            train_tmp = _aggregation(train_tmp, col)\n",
    "            test_tmp = _aggregation(test_tmp, col)\n",
    "        \n",
    "        print('merge')\n",
    "        train_final.drop([col], axis=1, inplace=True)\n",
    "        group_cols = [id_col, dv_col]\n",
    "        needed_cols = [col, col+'_enc']\n",
    "        train_final = pd.merge(train_final,\n",
    "                               train_tmp[group_cols+needed_cols],\n",
    "                               on=group_cols, how='left')\n",
    "        test_final.drop([col], axis=1, inplace=True)\n",
    "        group_cols = [id_col]\n",
    "        test_final = pd.merge(test_final,\n",
    "                              test_tmp[group_cols+needed_cols],\n",
    "                              on=group_cols, how='left')\n",
    "    return train_final, test_final\n",
    "\n",
    "\n",
    "def do_target_encode_count(train_data, test_data, discrete_cols,\n",
    "                           id_col=ID_COL, prod_flag=True):\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "    train_final = train_data.copy()\n",
    "    test_final = test_data.copy()\n",
    "    \n",
    "    for col in discrete_cols:\n",
    "        print('Cat Col: ', col)\n",
    "        \n",
    "        if prod_flag:\n",
    "            print('product col')\n",
    "            train_tmp = pd.DataFrame(\n",
    "                {id_col: np.repeat(train_data[id_col].values,\n",
    "                                   train_data[col].str.len()),\n",
    "                 col: np.concatenate(train_data[col].values)})\n",
    "            test_tmp = pd.DataFrame(\n",
    "                {id_col: np.repeat(test_data[id_col].values,\n",
    "                                   test_data[col].str.len()),\n",
    "                 col: np.concatenate(test_data[col].values)})\n",
    "        else:\n",
    "            train_tmp = train_data.copy()\n",
    "            test_tmp = test_data.copy()\n",
    "        \n",
    "        print('encoding')\n",
    "        train_enc_values = np.zeros(train_tmp.shape[0])\n",
    "        test_enc_values = 0\n",
    "        for dev_index, val_index in kf.split(train_tmp):\n",
    "            dev_X, val_X = train_tmp.iloc[dev_index, :], train_tmp.iloc[val_index, :]\n",
    "            train_enc_values[val_index] =  np.array( \n",
    "                getDVEncodeVar(val_X[[col]], dev_X, col,\n",
    "                               encode_type='count'))\n",
    "            test_enc_values += np.array( \n",
    "                getDVEncodeVar(test_tmp[[col]], dev_X, col,\n",
    "                               encode_type='count'))\n",
    "\n",
    "        test_enc_values = test_enc_values/5.\n",
    "        train_tmp[col + \"_count_enc\"] = train_enc_values\n",
    "        test_tmp[col + \"_count_enc\"] = test_enc_values\n",
    "        print(train_tmp[col + \"_count_enc\"].describe())\n",
    "        print(test_tmp[col + \"_count_enc\"].describe())\n",
    "        \n",
    "        if prod_flag:\n",
    "            print('aggregating')\n",
    "            train_tmp = _aggregation(train_tmp, col,\n",
    "                                     enc_prefix='_count_enc')\n",
    "            test_tmp = _aggregation(test_tmp, col,\n",
    "                                    enc_prefix='_count_enc')\n",
    "        \n",
    "        print('merge')\n",
    "        train_final.drop([col], axis=1, inplace=True)\n",
    "        group_cols = [id_col]\n",
    "        needed_cols = [col, col+'_count_enc']\n",
    "        train_final = pd.merge(train_final,\n",
    "                               train_tmp[group_cols+needed_cols],\n",
    "                               on=group_cols, how='left')\n",
    "        test_final.drop([col], axis=1, inplace=True)\n",
    "        test_final = pd.merge(test_final,\n",
    "                              test_tmp[group_cols+needed_cols],\n",
    "                              on=group_cols, how='left')\n",
    "    return train_final, test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WOE encoding for prod_cols: \n",
      "\n",
      "Cat Col:  prod_lst\n",
      "grouping\n",
      "encoding\n",
      "count    10913.000000\n",
      "mean         0.275415\n",
      "std          1.222529\n",
      "min         -2.454873\n",
      "25%          0.811109\n",
      "50%          0.831041\n",
      "75%          0.833999\n",
      "max          1.103338\n",
      "Name: prod_lst_enc, dtype: float64\n",
      "count    4684.000000\n",
      "mean        0.301727\n",
      "std         1.204740\n",
      "min        -2.438714\n",
      "25%         0.826442\n",
      "50%         0.826442\n",
      "75%         0.826442\n",
      "max         1.032331\n",
      "Name: prod_lst_enc, dtype: float64\n",
      "aggregating\n",
      "merge\n",
      "Cat Col:  cat_lst\n",
      "grouping\n",
      "encoding\n",
      "count     12071.000000\n",
      "mean       -911.004159\n",
      "std       30174.801360\n",
      "min     -999999.000000\n",
      "25%          -0.577802\n",
      "50%           0.813316\n",
      "75%           1.147519\n",
      "max           2.045460\n",
      "Name: cat_lst_enc, dtype: float64\n",
      "count      5234.000000\n",
      "mean      -1298.891215\n",
      "std       16068.558742\n",
      "min     -199998.834773\n",
      "25%          -0.521762\n",
      "50%           0.777545\n",
      "75%           1.150714\n",
      "max           1.826808\n",
      "Name: cat_lst_enc, dtype: float64\n",
      "aggregating\n",
      "merge\n",
      "Cat Col:  sub_cat_lst\n",
      "grouping\n",
      "encoding\n",
      "count     14170.000000\n",
      "mean      -4939.706845\n",
      "std       70113.839282\n",
      "min     -999999.000000\n",
      "25%          -0.455354\n",
      "50%           0.809654\n",
      "75%           1.201063\n",
      "max           2.866404\n",
      "Name: sub_cat_lst_enc, dtype: float64\n",
      "count      6153.000000\n",
      "mean      -4940.362865\n",
      "std       31046.296439\n",
      "min     -200003.695448\n",
      "25%          -0.441696\n",
      "50%           0.780940\n",
      "75%           1.186445\n",
      "max           2.080808\n",
      "Name: sub_cat_lst_enc, dtype: float64\n",
      "aggregating\n",
      "merge\n",
      "Cat Col:  sub_sub_cat_lst\n",
      "grouping\n",
      "encoding\n",
      "count     23251.000000\n",
      "mean       -516.103509\n",
      "std       22712.587385\n",
      "min     -999999.000000\n",
      "25%          -0.004292\n",
      "50%          -0.003987\n",
      "75%          -0.003744\n",
      "max           2.525509\n",
      "Name: sub_sub_cat_lst_enc, dtype: float64\n",
      "count     10204.000000\n",
      "mean       -313.596821\n",
      "std        7913.729971\n",
      "min     -199998.127769\n",
      "25%          -0.004116\n",
      "50%          -0.004116\n",
      "75%          -0.004116\n",
      "max           2.055339\n",
      "Name: sub_sub_cat_lst_enc, dtype: float64\n",
      "aggregating\n",
      "merge\n",
      "WOE encoding for time_cols: \n",
      "\n",
      "Cat Col:  session_start_month\n",
      "encoding\n",
      "count    10500.000000\n",
      "mean         0.004730\n",
      "std          0.126940\n",
      "min         -0.338828\n",
      "25%         -0.049325\n",
      "50%         -0.018324\n",
      "75%          0.008630\n",
      "max          0.717529\n",
      "Name: session_start_month_enc, dtype: float64\n",
      "count    4500.000000\n",
      "mean        0.007924\n",
      "std         0.124311\n",
      "min        -0.184038\n",
      "25%        -0.052688\n",
      "50%        -0.001707\n",
      "75%        -0.001707\n",
      "max         0.584348\n",
      "Name: session_start_month_enc, dtype: float64\n",
      "merge\n",
      "Cat Col:  session_end_month\n",
      "encoding\n",
      "count    10500.000000\n",
      "mean         0.004617\n",
      "std          0.123289\n",
      "min         -0.330137\n",
      "25%         -0.050622\n",
      "50%         -0.024038\n",
      "75%          0.004456\n",
      "max          0.646886\n",
      "Name: session_end_month_enc, dtype: float64\n",
      "count    4500.000000\n",
      "mean        0.007046\n",
      "std         0.119242\n",
      "min        -0.166735\n",
      "25%        -0.052826\n",
      "50%        -0.006468\n",
      "75%        -0.006468\n",
      "max         0.525873\n",
      "Name: session_end_month_enc, dtype: float64\n",
      "merge\n",
      "Cat Col:  session_start_dow\n",
      "encoding\n",
      "count    10500.000000\n",
      "mean         0.006161\n",
      "std          0.145487\n",
      "min         -0.285205\n",
      "25%         -0.048392\n",
      "50%          0.032346\n",
      "75%          0.077276\n",
      "max          0.309544\n",
      "Name: session_start_dow_enc, dtype: float64\n",
      "count    4500.000000\n",
      "mean        0.008088\n",
      "std         0.143511\n",
      "min        -0.264754\n",
      "25%        -0.119628\n",
      "50%         0.019289\n",
      "75%         0.063873\n",
      "max         0.258465\n",
      "Name: session_start_dow_enc, dtype: float64\n",
      "merge\n",
      "Cat Col:  session_end_dow\n",
      "encoding\n",
      "count    10500.000000\n",
      "mean         0.006048\n",
      "std          0.144797\n",
      "min         -0.291290\n",
      "25%         -0.053119\n",
      "50%          0.029908\n",
      "75%          0.077276\n",
      "max          0.299184\n",
      "Name: session_end_dow_enc, dtype: float64\n",
      "count    4500.000000\n",
      "mean        0.007878\n",
      "std         0.142757\n",
      "min        -0.265787\n",
      "25%        -0.119504\n",
      "50%         0.023924\n",
      "75%         0.067901\n",
      "max         0.250845\n",
      "Name: session_end_dow_enc, dtype: float64\n",
      "merge\n",
      "Cat Col:  session_start_woy\n",
      "encoding\n",
      "count    10500.000000\n",
      "mean         0.006921\n",
      "std          0.159090\n",
      "min         -0.338828\n",
      "25%         -0.091553\n",
      "50%          0.030074\n",
      "75%          0.053043\n",
      "max          0.717529\n",
      "Name: session_start_woy_enc, dtype: float64\n",
      "count    4500.000000\n",
      "mean        0.011516\n",
      "std         0.155132\n",
      "min        -0.249991\n",
      "25%        -0.021888\n",
      "50%         0.032297\n",
      "75%         0.090465\n",
      "max         0.584348\n",
      "Name: session_start_woy_enc, dtype: float64\n",
      "merge\n",
      "Cat Col:  session_end_woy\n",
      "encoding\n",
      "count    10500.000000\n",
      "mean         0.007011\n",
      "std          0.158652\n",
      "min         -0.336155\n",
      "25%         -0.074797\n",
      "50%          0.025200\n",
      "75%          0.047028\n",
      "max          0.646886\n",
      "Name: session_end_woy_enc, dtype: float64\n",
      "count    4500.000000\n",
      "mean        0.010870\n",
      "std         0.153682\n",
      "min        -0.260702\n",
      "25%        -0.023312\n",
      "50%         0.033280\n",
      "75%         0.098782\n",
      "max         0.525873\n",
      "Name: session_end_woy_enc, dtype: float64\n",
      "merge\n",
      "Cat Col:  session_start_hod\n",
      "encoding\n",
      "count    10500.000000\n",
      "mean         0.029919\n",
      "std          0.335187\n",
      "min         -1.478434\n",
      "25%         -0.284807\n",
      "50%          0.085107\n",
      "75%          0.321251\n",
      "max          0.735522\n",
      "Name: session_start_hod_enc, dtype: float64\n",
      "count    4500.000000\n",
      "mean        0.033397\n",
      "std         0.333324\n",
      "min        -1.161211\n",
      "25%        -0.295900\n",
      "50%         0.091891\n",
      "75%         0.339579\n",
      "max         0.651465\n",
      "Name: session_start_hod_enc, dtype: float64\n",
      "merge\n",
      "Cat Col:  session_end_hod\n",
      "encoding\n",
      "count    10500.000000\n",
      "mean         0.029139\n",
      "std          0.330725\n",
      "min         -1.785919\n",
      "25%         -0.295972\n",
      "50%          0.098965\n",
      "75%          0.300195\n",
      "max          0.685659\n",
      "Name: session_end_hod_enc, dtype: float64\n",
      "count    4500.000000\n",
      "mean        0.030228\n",
      "std         0.330469\n",
      "min        -1.572949\n",
      "25%        -0.326742\n",
      "50%         0.114087\n",
      "75%         0.325258\n",
      "max         0.607791\n",
      "Name: session_end_hod_enc, dtype: float64\n",
      "merge\n",
      "Cat Col:  session_start_moh\n",
      "encoding\n",
      "count    10500.000000\n",
      "mean         0.010730\n",
      "std          0.187594\n",
      "min         -0.427659\n",
      "25%         -0.120745\n",
      "50%          0.001280\n",
      "75%          0.116175\n",
      "max          0.633521\n",
      "Name: session_start_moh_enc, dtype: float64\n",
      "count    4500.000000\n",
      "mean        0.008989\n",
      "std         0.157649\n",
      "min        -0.316692\n",
      "25%        -0.113590\n",
      "50%         0.008455\n",
      "75%         0.113874\n",
      "max         0.468879\n",
      "Name: session_start_moh_enc, dtype: float64\n",
      "merge\n",
      "Cat Col:  session_end_moh\n",
      "encoding\n",
      "count    10500.000000\n",
      "mean         0.009449\n",
      "std          0.189037\n",
      "min         -0.501976\n",
      "25%         -0.098146\n",
      "50%         -0.005217\n",
      "75%          0.134530\n",
      "max          0.554841\n",
      "Name: session_end_moh_enc, dtype: float64\n",
      "count    4500.000000\n",
      "mean        0.009362\n",
      "std         0.166768\n",
      "min        -0.393832\n",
      "25%        -0.070809\n",
      "50%         0.014821\n",
      "75%         0.104266\n",
      "max         0.367202\n",
      "Name: session_end_moh_enc, dtype: float64\n",
      "merge\n",
      "Cat Col:  session_start_part_of_day\n",
      "encoding\n",
      "count    10500.000000\n",
      "mean         0.016273\n",
      "std          0.241475\n",
      "min         -0.547629\n",
      "25%         -0.297885\n",
      "50%          0.165644\n",
      "75%          0.191173\n",
      "max          0.207697\n",
      "Name: session_start_part_of_day_enc, dtype: float64\n",
      "count    4500.000000\n",
      "mean        0.017779\n",
      "std         0.240329\n",
      "min        -0.451400\n",
      "25%        -0.301721\n",
      "50%         0.163979\n",
      "75%         0.199968\n",
      "max         0.199968\n",
      "Name: session_start_part_of_day_enc, dtype: float64\n",
      "merge\n",
      "Cat Col:  session_end_part_of_day\n",
      "encoding\n",
      "count    10500.000000\n",
      "mean         0.016097\n",
      "std          0.240628\n",
      "min         -0.547796\n",
      "25%         -0.293493\n",
      "50%          0.174919\n",
      "75%          0.179261\n",
      "max          0.200480\n",
      "Name: session_end_part_of_day_enc, dtype: float64\n",
      "count    4500.000000\n",
      "mean        0.017164\n",
      "std         0.239884\n",
      "min        -0.455570\n",
      "25%        -0.298748\n",
      "50%         0.171281\n",
      "75%         0.189826\n",
      "max         0.189826\n",
      "Name: session_end_part_of_day_enc, dtype: float64\n",
      "merge\n",
      "Cat Col:  prod_str\n",
      "encoding\n",
      "count     10500.000000\n",
      "mean      -4856.828896\n",
      "std       69527.007264\n",
      "min     -999999.000000\n",
      "25%           0.866913\n",
      "50%           0.886479\n",
      "75%           0.909148\n",
      "max           2.218310\n",
      "Name: prod_str_enc, dtype: float64\n",
      "count      4500.000000\n",
      "mean      -5244.102205\n",
      "std       70492.197207\n",
      "min     -999999.000000\n",
      "25%           0.885508\n",
      "50%           0.885508\n",
      "75%           0.885508\n",
      "max           1.967682\n",
      "Name: prod_str_enc, dtype: float64\n",
      "merge\n",
      "Cat Col:  cat_str\n",
      "encoding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     10500.000000\n",
      "mean     -67999.635520\n",
      "std      251757.720729\n",
      "min     -999999.000000\n",
      "25%          -0.677409\n",
      "50%           0.809038\n",
      "75%           1.200807\n",
      "max           2.382296\n",
      "Name: cat_str_enc, dtype: float64\n",
      "count      4500.000000\n",
      "mean     -72666.254699\n",
      "std      246368.455288\n",
      "min     -999999.000000\n",
      "25%          -0.657818\n",
      "50%           0.811030\n",
      "75%           1.234052\n",
      "max           1.951826\n",
      "Name: cat_str_enc, dtype: float64\n",
      "merge\n",
      "Cat Col:  sub_cat_str\n",
      "encoding\n",
      "count     10500.000000\n",
      "mean    -251999.455076\n",
      "std      434181.672035\n",
      "min     -999999.000000\n",
      "25%     -999999.000000\n",
      "50%           0.444606\n",
      "75%           1.130618\n",
      "max           2.850057\n",
      "Name: sub_cat_str_enc, dtype: float64\n",
      "count      4500.000000\n",
      "mean    -254710.543859\n",
      "std      423705.533843\n",
      "min     -999999.000000\n",
      "25%     -200001.507281\n",
      "50%           0.328750\n",
      "75%           1.113286\n",
      "max           2.099923\n",
      "Name: sub_cat_str_enc, dtype: float64\n",
      "merge\n",
      "Cat Col:  sub_sub_cat_str\n",
      "encoding\n",
      "count     10500.000000\n",
      "mean    -989903.778741\n",
      "std       99971.209670\n",
      "min     -999999.000000\n",
      "25%     -999999.000000\n",
      "50%     -999999.000000\n",
      "75%     -999999.000000\n",
      "max           1.572397\n",
      "Name: sub_sub_cat_str_enc, dtype: float64\n",
      "count      4500.000000\n",
      "mean    -989154.573308\n",
      "std       91940.947156\n",
      "min     -999999.000000\n",
      "25%     -999999.000000\n",
      "50%     -999999.000000\n",
      "75%     -999999.000000\n",
      "max          -0.631189\n",
      "Name: sub_sub_cat_str_enc, dtype: float64\n",
      "merge\n",
      "WOE encoding for prod_cat_lst: \n",
      "\n",
      "Cat Col:  prod_cat_lst\n",
      "grouping\n",
      "encoding\n",
      "count    12116.000000\n",
      "mean         0.314770\n",
      "std          1.321278\n",
      "min         -3.484642\n",
      "25%          0.380655\n",
      "50%          0.827177\n",
      "75%          1.160712\n",
      "max          2.255509\n",
      "Name: prod_cat_lst_enc, dtype: float64\n",
      "count    5254.000000\n",
      "mean        0.339144\n",
      "std         1.302088\n",
      "min        -3.325290\n",
      "25%         0.430080\n",
      "50%         0.778153\n",
      "75%         1.151305\n",
      "max         1.853897\n",
      "Name: prod_cat_lst_enc, dtype: float64\n",
      "aggregating\n",
      "merge\n",
      "Cat Col:  prod_cat_sub_cat_lst\n",
      "grouping\n",
      "encoding\n",
      "count     14221.000000\n",
      "mean      -4570.385927\n",
      "std       67454.634283\n",
      "min     -999999.000000\n",
      "25%          -0.429166\n",
      "50%           0.823694\n",
      "75%           1.219710\n",
      "max           2.841178\n",
      "Name: prod_cat_sub_cat_lst_enc, dtype: float64\n",
      "count      6189.000000\n",
      "mean      -4039.097588\n",
      "std       28136.963055\n",
      "min     -200003.726739\n",
      "25%          -0.442180\n",
      "50%           0.793873\n",
      "75%           1.185547\n",
      "max           2.327630\n",
      "Name: prod_cat_sub_cat_lst_enc, dtype: float64\n",
      "aggregating\n",
      "merge\n",
      "Cat Col:  prod_cat_sub_sub_cat_lst\n",
      "grouping\n",
      "encoding\n",
      "count     23251.000000\n",
      "mean       -903.184442\n",
      "std       30040.110233\n",
      "min     -999999.000000\n",
      "25%          -0.004292\n",
      "50%          -0.003987\n",
      "75%          -0.003744\n",
      "max           2.152834\n",
      "Name: prod_cat_sub_sub_cat_lst_enc, dtype: float64\n",
      "count     10204.000000\n",
      "mean       -980.003566\n",
      "std       13966.279278\n",
      "min     -199998.383215\n",
      "25%          -0.004046\n",
      "50%          -0.004046\n",
      "75%          -0.004046\n",
      "max          -0.004046\n",
      "Name: prod_cat_sub_sub_cat_lst_enc, dtype: float64\n",
      "aggregating\n",
      "merge\n",
      "WOE encoding for prod_cat_str: \n",
      "\n",
      "Cat Col:  prod_cat_str\n",
      "encoding\n",
      "count     10500.000000\n",
      "mean     -74380.543537\n",
      "std      262402.164574\n",
      "min     -999999.000000\n",
      "25%          -0.072828\n",
      "50%           0.822750\n",
      "75%           1.200807\n",
      "max           2.382296\n",
      "Name: prod_cat_str_enc, dtype: float64\n",
      "count      4500.000000\n",
      "mean     -81288.438444\n",
      "std      261301.211118\n",
      "min     -999999.000000\n",
      "25%          -1.290356\n",
      "50%           0.811030\n",
      "75%           1.234052\n",
      "max           1.951826\n",
      "Name: prod_cat_str_enc, dtype: float64\n",
      "merge\n",
      "Cat Col:  prod_cat_sub_cat_str\n",
      "encoding\n",
      "count     10500.000000\n",
      "mean    -274570.842276\n",
      "std      446319.060200\n",
      "min     -999999.000000\n",
      "25%     -999999.000000\n",
      "50%           0.487175\n",
      "75%           1.148867\n",
      "max           2.850057\n",
      "Name: prod_cat_sub_cat_str_enc, dtype: float64\n",
      "count      4500.000000\n",
      "mean    -277554.949811\n",
      "std      431688.815327\n",
      "min     -999999.000000\n",
      "25%     -999999.000000\n",
      "50%           0.144513\n",
      "75%           1.113286\n",
      "max           2.099923\n",
      "Name: prod_cat_sub_cat_str_enc, dtype: float64\n",
      "merge\n",
      "Cat Col:  prod_cat_sub_sub_cat_str\n",
      "encoding\n",
      "count     10500.000000\n",
      "mean    -989999.016555\n",
      "std       99503.317358\n",
      "min     -999999.000000\n",
      "25%     -999999.000000\n",
      "50%     -999999.000000\n",
      "75%     -999999.000000\n",
      "max           1.572397\n",
      "Name: prod_cat_sub_sub_cat_str_enc, dtype: float64\n",
      "count      4500.000000\n",
      "mean    -989154.573308\n",
      "std       91940.947156\n",
      "min     -999999.000000\n",
      "25%     -999999.000000\n",
      "50%     -999999.000000\n",
      "75%     -999999.000000\n",
      "max          -0.631189\n",
      "Name: prod_cat_sub_sub_cat_str_enc, dtype: float64\n",
      "merge\n",
      "Count encoding for prod_cols: \n",
      "\n",
      "Cat Col:  prod_lst\n",
      "product col\n",
      "encoding\n",
      "count    10913.000000\n",
      "mean      3899.530285\n",
      "std       2140.177803\n",
      "min         20.000000\n",
      "25%       1330.000000\n",
      "50%       5509.000000\n",
      "75%       5524.000000\n",
      "max       5532.000000\n",
      "Name: prod_lst_count_enc, dtype: float64\n",
      "count    4684.000000\n",
      "mean     3882.888301\n",
      "std      2138.261939\n",
      "min        24.800000\n",
      "25%      1328.800000\n",
      "50%      5518.400000\n",
      "75%      5518.400000\n",
      "max      5518.400000\n",
      "Name: prod_lst_count_enc, dtype: float64\n",
      "aggregating\n",
      "merge\n",
      "Cat Col:  cat_lst\n",
      "product col\n",
      "encoding\n",
      "count    12071.000000\n",
      "mean      1027.078867\n",
      "std        738.700989\n",
      "min         24.000000\n",
      "25%        339.000000\n",
      "50%        696.000000\n",
      "75%       1325.000000\n",
      "max       2188.000000\n",
      "Name: cat_lst_count_enc, dtype: float64\n",
      "count    5234.000000\n",
      "mean     1026.698280\n",
      "std       740.564107\n",
      "min        26.400000\n",
      "25%       340.800000\n",
      "50%       677.600000\n",
      "75%      1326.400000\n",
      "max      2162.400000\n",
      "Name: cat_lst_count_enc, dtype: float64\n",
      "aggregating\n",
      "merge\n",
      "Cat Col:  sub_cat_lst\n",
      "product col\n",
      "encoding\n",
      "count    14170.000000\n",
      "mean       384.746154\n",
      "std        366.274854\n",
      "min         20.000000\n",
      "25%        101.000000\n",
      "50%        254.000000\n",
      "75%        516.000000\n",
      "max       1163.000000\n",
      "Name: sub_cat_lst_count_enc, dtype: float64\n",
      "count    6153.000000\n",
      "mean      392.219340\n",
      "std       371.708946\n",
      "min        25.600000\n",
      "25%       103.200000\n",
      "50%       256.000000\n",
      "75%       520.000000\n",
      "max      1146.400000\n",
      "Name: sub_cat_lst_count_enc, dtype: float64\n",
      "aggregating\n",
      "merge\n",
      "Cat Col:  sub_sub_cat_lst\n",
      "product col\n",
      "encoding\n",
      "count    23251.000000\n",
      "mean      9217.739108\n",
      "std        575.709097\n",
      "min         24.000000\n",
      "25%       9238.000000\n",
      "50%       9255.000000\n",
      "75%       9257.000000\n",
      "max       9283.000000\n",
      "Name: sub_sub_cat_lst_count_enc, dtype: float64\n",
      "count    10204.000000\n",
      "mean      9208.647785\n",
      "std        643.478508\n",
      "min         28.800000\n",
      "25%       9253.800000\n",
      "50%       9253.800000\n",
      "75%       9253.800000\n",
      "max       9253.800000\n",
      "Name: sub_sub_cat_lst_count_enc, dtype: float64\n",
      "aggregating\n",
      "merge\n",
      "Count encoding for time_cols: \n",
      "\n",
      "Cat Col:  session_start_month\n",
      "encoding\n",
      "count    10500.000000\n",
      "mean      2495.446857\n",
      "std       1238.943069\n",
      "min         62.000000\n",
      "25%       3155.000000\n",
      "50%       3180.000000\n",
      "75%       3209.000000\n",
      "max       3234.000000\n",
      "Name: session_start_month_count_enc, dtype: float64\n",
      "count    4500.000000\n",
      "mean     2484.977244\n",
      "std      1246.316092\n",
      "min        67.200000\n",
      "25%      3184.000000\n",
      "50%      3184.000000\n",
      "75%      3200.000000\n",
      "max      3200.000000\n",
      "Name: session_start_month_count_enc, dtype: float64\n",
      "merge\n",
      "Cat Col:  session_end_month\n",
      "encoding\n",
      "count    10500.000000\n",
      "mean      2492.821143\n",
      "std       1238.521135\n",
      "min         61.000000\n",
      "25%       3129.000000\n",
      "50%       3186.000000\n",
      "75%       3201.000000\n",
      "max       3233.000000\n",
      "Name: session_end_month_count_enc, dtype: float64\n",
      "count    4500.000000\n",
      "mean     2482.932444\n",
      "std      1245.135670\n",
      "min        67.200000\n",
      "25%      3175.200000\n",
      "50%      3175.200000\n",
      "75%      3204.800000\n",
      "max      3204.800000\n",
      "Name: session_end_month_count_enc, dtype: float64\n",
      "merge\n",
      "Cat Col:  session_start_dow\n",
      "encoding\n",
      "count    10500.000000\n",
      "mean      1241.163429\n",
      "std        236.065274\n",
      "min        928.000000\n",
      "25%       1040.000000\n",
      "50%       1196.000000\n",
      "75%       1383.000000\n",
      "max       1655.000000\n",
      "Name: session_start_dow_count_enc, dtype: float64\n",
      "count    4500.000000\n",
      "mean     1233.681067\n",
      "std       231.412595\n",
      "min       951.200000\n",
      "25%      1057.600000\n",
      "50%      1191.200000\n",
      "75%      1381.600000\n",
      "max      1633.600000\n",
      "Name: session_start_dow_count_enc, dtype: float64\n",
      "merge\n",
      "Cat Col:  session_end_dow\n",
      "encoding\n",
      "count    10500.000000\n",
      "mean      1241.108381\n",
      "std        236.536810\n",
      "min        924.000000\n",
      "25%       1037.000000\n",
      "50%       1194.000000\n",
      "75%       1380.000000\n",
      "max       1657.000000\n",
      "Name: session_end_dow_count_enc, dtype: float64\n",
      "count    4500.000000\n",
      "mean     1234.153067\n",
      "std       231.546602\n",
      "min       948.000000\n",
      "25%      1056.800000\n",
      "50%      1188.000000\n",
      "75%      1379.200000\n",
      "max      1636.000000\n",
      "Name: session_end_dow_count_enc, dtype: float64\n",
      "merge\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat Col:  session_start_woy\n",
      "encoding\n",
      "count    10500.000000\n",
      "mean      1085.043048\n",
      "std        658.388318\n",
      "min         62.000000\n",
      "25%        354.000000\n",
      "50%       1157.500000\n",
      "75%       1248.000000\n",
      "max       2082.000000\n",
      "Name: session_start_woy_count_enc, dtype: float64\n",
      "count    4500.000000\n",
      "mean     1076.533867\n",
      "std       654.154100\n",
      "min        67.200000\n",
      "25%       352.000000\n",
      "50%      1180.000000\n",
      "75%      1229.600000\n",
      "max      2068.000000\n",
      "Name: session_start_woy_count_enc, dtype: float64\n",
      "merge\n",
      "Cat Col:  session_end_woy\n",
      "encoding\n",
      "count    10500.000000\n",
      "mean      1086.201714\n",
      "std        663.958600\n",
      "min         61.000000\n",
      "25%        355.000000\n",
      "50%       1122.000000\n",
      "75%       1236.000000\n",
      "max       2093.000000\n",
      "Name: session_end_woy_count_enc, dtype: float64\n",
      "count    4500.000000\n",
      "mean     1078.175289\n",
      "std       659.325398\n",
      "min        67.200000\n",
      "25%       353.600000\n",
      "50%      1176.000000\n",
      "75%      1216.800000\n",
      "max      2080.800000\n",
      "Name: session_end_woy_count_enc, dtype: float64\n",
      "merge\n",
      "Cat Col:  session_start_hod\n",
      "encoding\n",
      "count    10500.000000\n",
      "mean       495.425524\n",
      "std        147.298574\n",
      "min         19.000000\n",
      "25%        423.000000\n",
      "50%        553.000000\n",
      "75%        605.000000\n",
      "max        652.000000\n",
      "Name: session_start_hod_count_enc, dtype: float64\n",
      "count    4500.000000\n",
      "mean      495.744889\n",
      "std       146.115098\n",
      "min        24.000000\n",
      "25%       419.200000\n",
      "50%       536.000000\n",
      "75%       598.400000\n",
      "max       640.000000\n",
      "Name: session_start_hod_count_enc, dtype: float64\n",
      "merge\n",
      "Cat Col:  session_end_hod\n",
      "encoding\n",
      "count    10500.000000\n",
      "mean       496.619619\n",
      "std        149.097717\n",
      "min         17.000000\n",
      "25%        424.000000\n",
      "50%        553.000000\n",
      "75%        608.000000\n",
      "max        662.000000\n",
      "Name: session_end_hod_count_enc, dtype: float64\n",
      "count    4500.000000\n",
      "mean      496.511467\n",
      "std       148.460395\n",
      "min        22.400000\n",
      "25%       418.400000\n",
      "50%       558.400000\n",
      "75%       597.600000\n",
      "max       644.000000\n",
      "Name: session_end_hod_count_enc, dtype: float64\n",
      "merge\n",
      "Cat Col:  session_start_moh\n",
      "encoding\n",
      "count    10500.000000\n",
      "mean       140.017905\n",
      "std         12.049880\n",
      "min        109.000000\n",
      "25%        132.000000\n",
      "50%        140.000000\n",
      "75%        148.000000\n",
      "max        179.000000\n",
      "Name: session_start_moh_count_enc, dtype: float64\n",
      "count    4500.000000\n",
      "mean      139.783289\n",
      "std        10.745530\n",
      "min       116.800000\n",
      "25%       132.000000\n",
      "50%       139.200000\n",
      "75%       147.200000\n",
      "max       168.000000\n",
      "Name: session_start_moh_count_enc, dtype: float64\n",
      "merge\n",
      "Cat Col:  session_end_moh\n",
      "encoding\n",
      "count    10500.000000\n",
      "mean       139.823429\n",
      "std         11.637509\n",
      "min        103.000000\n",
      "25%        132.000000\n",
      "50%        139.000000\n",
      "75%        147.000000\n",
      "max        174.000000\n",
      "Name: session_end_moh_count_enc, dtype: float64\n",
      "count    4500.000000\n",
      "mean      139.958044\n",
      "std         9.954924\n",
      "min       116.800000\n",
      "25%       132.800000\n",
      "50%       139.200000\n",
      "75%       146.400000\n",
      "max       165.600000\n",
      "Name: session_end_moh_count_enc, dtype: float64\n",
      "merge\n",
      "Cat Col:  session_start_part_of_day\n",
      "encoding\n",
      "count    10500.000000\n",
      "mean      2553.724762\n",
      "std        648.910155\n",
      "min        501.000000\n",
      "25%       2183.000000\n",
      "50%       2562.000000\n",
      "75%       3130.000000\n",
      "max       3174.000000\n",
      "Name: session_start_part_of_day_count_enc, dtype: float64\n",
      "count    4500.000000\n",
      "mean     2562.188444\n",
      "std       651.815823\n",
      "min       516.000000\n",
      "25%      2180.800000\n",
      "50%      2560.800000\n",
      "75%      3142.400000\n",
      "max      3142.400000\n",
      "Name: session_start_part_of_day_count_enc, dtype: float64\n",
      "merge\n",
      "Cat Col:  session_end_part_of_day\n",
      "encoding\n",
      "count    10500.000000\n",
      "mean      2553.116571\n",
      "std        656.053489\n",
      "min        507.000000\n",
      "25%       2180.000000\n",
      "50%       2548.000000\n",
      "75%       3144.000000\n",
      "max       3184.000000\n",
      "Name: session_end_part_of_day_count_enc, dtype: float64\n",
      "count    4500.000000\n",
      "mean     2559.305956\n",
      "std       659.628002\n",
      "min       522.400000\n",
      "25%      2176.800000\n",
      "50%      2544.000000\n",
      "75%      3156.800000\n",
      "max      3156.800000\n",
      "Name: session_end_part_of_day_count_enc, dtype: float64\n",
      "merge\n",
      "Cat Col:  prod_str\n",
      "encoding\n",
      "count     10500.000000\n",
      "mean       1528.648952\n",
      "std       45942.337629\n",
      "min     -999999.000000\n",
      "25%        1190.000000\n",
      "50%        5201.000000\n",
      "75%        5252.000000\n",
      "max        5292.000000\n",
      "Name: prod_str_count_enc, dtype: float64\n",
      "count      4500.000000\n",
      "mean       1426.720933\n",
      "std       43285.300015\n",
      "min     -999999.000000\n",
      "25%        1201.600000\n",
      "50%        5247.200000\n",
      "75%        5247.200000\n",
      "max        5247.200000\n",
      "Name: prod_str_count_enc, dtype: float64\n",
      "merge\n",
      "Cat Col:  cat_str\n",
      "encoding\n",
      "count     10500.000000\n",
      "mean     -25182.907619\n",
      "std      158977.554198\n",
      "min     -999999.000000\n",
      "25%         130.000000\n",
      "50%         541.000000\n",
      "75%        1027.000000\n",
      "max        1772.000000\n",
      "Name: cat_str_count_enc, dtype: float64\n",
      "count      4500.000000\n",
      "mean     -27458.229244\n",
      "std      159719.626434\n",
      "min     -999999.000000\n",
      "25%         132.000000\n",
      "50%         562.400000\n",
      "75%        1012.800000\n",
      "max        1761.600000\n",
      "Name: cat_str_count_enc, dtype: float64\n",
      "merge\n",
      "Cat Col:  sub_cat_str\n",
      "encoding\n",
      "count     10500.000000\n",
      "mean    -128069.878476\n",
      "std      334363.593697\n",
      "min     -999999.000000\n",
      "25%           7.000000\n",
      "50%          57.000000\n",
      "75%         176.000000\n",
      "max         611.000000\n",
      "Name: sub_cat_str_count_enc, dtype: float64\n",
      "count      4500.000000\n",
      "mean    -132720.764222\n",
      "std      329174.296259\n",
      "min     -999999.000000\n",
      "25%           6.400000\n",
      "50%          60.800000\n",
      "75%         172.000000\n",
      "max         599.200000\n",
      "Name: sub_cat_str_count_enc, dtype: float64\n",
      "merge\n",
      "Cat Col:  sub_sub_cat_str\n",
      "encoding\n",
      "count     10500.000000\n",
      "mean    -851998.683619\n",
      "std      355117.655790\n",
      "min     -999999.000000\n",
      "25%     -999999.000000\n",
      "50%     -999999.000000\n",
      "75%     -999999.000000\n",
      "max          23.000000\n",
      "Name: sub_sub_cat_str_count_enc, dtype: float64\n",
      "count      4500.000000\n",
      "mean    -847109.769956\n",
      "std      340369.371993\n",
      "min     -999999.000000\n",
      "25%     -999999.000000\n",
      "50%     -999999.000000\n",
      "75%     -999999.000000\n",
      "max          20.000000\n",
      "Name: sub_sub_cat_str_count_enc, dtype: float64\n",
      "merge\n",
      "Count encoding for prod_cat_lst: \n",
      "\n",
      "Cat Col:  prod_cat_lst\n",
      "product col\n",
      "encoding\n",
      "count    12116.000000\n",
      "mean       924.813800\n",
      "std        752.716119\n",
      "min         24.000000\n",
      "25%        286.000000\n",
      "50%        662.000000\n",
      "75%       1284.000000\n",
      "max       2182.000000\n",
      "Name: prod_cat_lst_count_enc, dtype: float64\n",
      "count    5254.000000\n",
      "mean      928.638180\n",
      "std       756.521857\n",
      "min        26.400000\n",
      "25%       287.200000\n",
      "50%       677.600000\n",
      "75%      1272.800000\n",
      "max      2162.400000\n",
      "Name: prod_cat_lst_count_enc, dtype: float64\n",
      "aggregating\n",
      "merge\n",
      "Cat Col:  prod_cat_sub_cat_lst\n",
      "product col\n",
      "encoding\n",
      "count    14221.000000\n",
      "mean       476.303987\n",
      "std        523.471458\n",
      "min         21.000000\n",
      "25%         86.000000\n",
      "50%        251.000000\n",
      "75%        529.000000\n",
      "max       1575.000000\n",
      "Name: prod_cat_sub_cat_lst_count_enc, dtype: float64\n",
      "count    6189.000000\n",
      "mean      488.430797\n",
      "std       531.371728\n",
      "min        25.600000\n",
      "25%        88.800000\n",
      "50%       256.000000\n",
      "75%       956.000000\n",
      "max      1550.200000\n",
      "Name: prod_cat_sub_cat_lst_count_enc, dtype: float64\n",
      "aggregating\n",
      "merge\n",
      "Cat Col:  prod_cat_sub_sub_cat_lst\n",
      "product col\n",
      "encoding\n",
      "count    23251.000000\n",
      "mean      9217.335727\n",
      "std        575.668755\n",
      "min         26.000000\n",
      "25%       9238.000000\n",
      "50%       9254.000000\n",
      "75%       9256.000000\n",
      "max       9283.000000\n",
      "Name: prod_cat_sub_sub_cat_lst_count_enc, dtype: float64\n",
      "count    10204.000000\n",
      "mean      9208.249745\n",
      "std        643.450576\n",
      "min         28.800000\n",
      "25%       9253.400000\n",
      "50%       9253.400000\n",
      "75%       9253.400000\n",
      "max       9253.400000\n",
      "Name: prod_cat_sub_sub_cat_lst_count_enc, dtype: float64\n",
      "aggregating\n",
      "merge\n",
      "Count encoding for prod_cat_str: \n",
      "\n",
      "Cat Col:  prod_cat_str\n",
      "encoding\n",
      "count     10500.000000\n",
      "mean     -30111.673429\n",
      "std      172797.006982\n",
      "min     -999999.000000\n",
      "25%         118.000000\n",
      "50%         459.000000\n",
      "75%         941.000000\n",
      "max        1772.000000\n",
      "Name: prod_cat_str_count_enc, dtype: float64\n",
      "count      4500.000000\n",
      "mean     -32860.447200\n",
      "std      174186.786357\n",
      "min     -999999.000000\n",
      "25%         120.000000\n",
      "50%         456.000000\n",
      "75%         934.400000\n",
      "max        1761.600000\n",
      "Name: prod_cat_str_count_enc, dtype: float64\n",
      "merge\n",
      "Cat Col:  prod_cat_sub_cat_str\n",
      "encoding\n",
      "count     10500.000000\n",
      "mean    -136650.245714\n",
      "std      343656.262304\n",
      "min     -999999.000000\n",
      "25%           5.000000\n",
      "50%          43.000000\n",
      "75%         151.000000\n",
      "max         611.000000\n",
      "Name: prod_cat_sub_cat_str_count_enc, dtype: float64\n",
      "count      4500.000000\n",
      "mean    -140418.906756\n",
      "std      336340.903242\n",
      "min     -999999.000000\n",
      "25%           4.800000\n",
      "50%          44.800000\n",
      "75%         153.600000\n",
      "max         599.200000\n",
      "Name: prod_cat_sub_cat_str_count_enc, dtype: float64\n",
      "merge\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat Col:  prod_cat_sub_sub_cat_str\n",
      "encoding\n",
      "count     10500.000000\n",
      "mean    -851998.685333\n",
      "std      355117.651677\n",
      "min     -999999.000000\n",
      "25%     -999999.000000\n",
      "50%     -999999.000000\n",
      "75%     -999999.000000\n",
      "max          23.000000\n",
      "Name: prod_cat_sub_sub_cat_str_count_enc, dtype: float64\n",
      "count      4500.000000\n",
      "mean    -847465.327289\n",
      "std      340110.890235\n",
      "min     -999999.000000\n",
      "25%     -999999.000000\n",
      "50%     -999999.000000\n",
      "75%     -999999.000000\n",
      "max          20.000000\n",
      "Name: prod_cat_sub_sub_cat_str_count_enc, dtype: float64\n",
      "merge\n"
     ]
    }
   ],
   "source": [
    "print('WOE encoding for prod_cols: \\n')\n",
    "prod_cols = ['prod_lst', 'cat_lst', 'sub_cat_lst', 'sub_sub_cat_lst']\n",
    "train_df1, test_df1 = do_target_encode_woe(train_df, test_df,\n",
    "                                           prod_cols)\n",
    "\n",
    "print('WOE encoding for time_cols: \\n')\n",
    "time_cols = ['session_start_month', 'session_end_month',\n",
    "             'session_start_dow', 'session_end_dow',\n",
    "             'session_start_woy', 'session_end_woy',\n",
    "             'session_start_hod', 'session_end_hod',\n",
    "             'session_start_moh', 'session_end_moh',\n",
    "             'session_start_part_of_day', 'session_end_part_of_day',\n",
    "             'prod_str', 'cat_str', 'sub_cat_str', 'sub_sub_cat_str']\n",
    "train_df1, test_df1 = do_target_encode_woe(train_df1, test_df1,\n",
    "                                           time_cols,\n",
    "                                           grouping_flag=False,\n",
    "                                           agg_flag=False)\n",
    "\n",
    "print('WOE encoding for prod_cat_lst: \\n')\n",
    "train_df1, test_df1 = do_target_encode_woe(train_df1, test_df1,\n",
    "                                           ['prod_cat_lst',\n",
    "                                            'prod_cat_sub_cat_lst',\n",
    "                                            'prod_cat_sub_sub_cat_lst'])\n",
    "\n",
    "print('WOE encoding for prod_cat_str: \\n')\n",
    "train_df1, test_df1 = do_target_encode_woe(train_df1, test_df1,\n",
    "                                           ['prod_cat_str',\n",
    "                                            'prod_cat_sub_cat_str',\n",
    "                                            'prod_cat_sub_sub_cat_str'],\n",
    "                                           grouping_flag=False,\n",
    "                                           agg_flag=False)\n",
    "\n",
    "print('Count encoding for prod_cols: \\n')\n",
    "train_df1, test_df1 = do_target_encode_count(train_df1, test_df1,\n",
    "                                             prod_cols)\n",
    "\n",
    "print('Count encoding for time_cols: \\n')\n",
    "train_df1, test_df1 = do_target_encode_count(train_df1, test_df1,\n",
    "                                             time_cols, prod_flag=False)\n",
    "\n",
    "print('Count encoding for prod_cat_lst: \\n')\n",
    "train_df1, test_df1 = do_target_encode_count(train_df1, test_df1,\n",
    "                                             ['prod_cat_lst',\n",
    "                                              'prod_cat_sub_cat_lst',\n",
    "                                              'prod_cat_sub_sub_cat_lst'])\n",
    "\n",
    "print('Count encoding for prod_cat_str: \\n')\n",
    "train_df1, test_df1 = do_target_encode_count(train_df1, test_df1,\n",
    "                                             ['prod_cat_str',\n",
    "                                              'prod_cat_sub_cat_str',\n",
    "                                              'prod_cat_sub_sub_cat_str'],\n",
    "                                             prod_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10500, 84)\n",
      "(4500, 83)\n",
      "  session_id           startTime             endTime  \\\n",
      "0     u16159 2014-12-15 18:11:00 2014-12-15 18:12:00   \n",
      "1     u10253 2014-12-16 14:35:00 2014-12-16 14:41:00   \n",
      "2     u19037 2014-01-12 15:58:00 2014-01-12 15:58:00   \n",
      "3     u14556 2014-11-23 02:57:00 2014-11-23 03:00:00   \n",
      "4     u24295 2014-12-17 16:44:00 2014-12-17 16:46:00   \n",
      "\n",
      "                                         ProductList  gender  \\\n",
      "0  A00002/B00003/C00006/D28435/;A00002/B00003/C00...       0   \n",
      "1  A00001/B00009/C00031/D29404/;A00001/B00009/C00...       1   \n",
      "2                       A00002/B00001/C00020/D16944/       0   \n",
      "3  A00002/B00004/C00018/D10284/;A00002/B00004/C00...       0   \n",
      "4  A00001/B00001/C00012/D30805/;A00001/B00001/C00...       1   \n",
      "\n",
      "   session_duration_sec           prod_lst_enc            cat_lst_enc  \\\n",
      "0                    60   [0.8339991858704539]   [0.8195565322658352]   \n",
      "1                   360  [-2.4236070247273154]   [-2.250655947210555]   \n",
      "2                     0   [0.8111093048419072]  [-0.6299154692024713]   \n",
      "3                   180   [0.8310409466537441]   [-0.531213280949413]   \n",
      "4                   120  [-2.4236070247273154]  [-0.6299154692024713]   \n",
      "\n",
      "         sub_cat_lst_enc                                sub_sub_cat_lst_enc  \\\n",
      "0    [1.120746270905334]  [-0.004291555004967625, -0.003987081654953575,...   \n",
      "1   [-2.884063684083734]  [-0.003743505238282633, -0.004839276640749268,...   \n",
      "2  [-1.3208431813445425]                            [-0.003987081654953575]   \n",
      "3   [0.8871570922904375]  [-0.0037187564376528967, -0.003987081654953575...   \n",
      "4  [-0.7801645944288708]    [-0.004839276640749268, -0.0037187564376528967]   \n",
      "\n",
      "   ...    prod_cat_sub_cat_lst  prod_cat_sub_cat_lst_count_enc  \\\n",
      "0  ...  [A00002/B00003/C00006]                         [391.0]   \n",
      "1  ...  [A00001/B00009/C00031]                          [26.0]   \n",
      "2  ...  [A00002/B00001/C00020]                          [51.0]   \n",
      "3  ...  [A00002/B00004/C00018]                         [196.0]   \n",
      "4  ...  [A00001/B00001/C00012]                          [39.0]   \n",
      "\n",
      "                            prod_cat_sub_sub_cat_lst  \\\n",
      "0                   [others, others, others, others]   \n",
      "1  [others, others, others, others, others, other...   \n",
      "2                                           [others]   \n",
      "3                           [others, others, others]   \n",
      "4                                   [others, others]   \n",
      "\n",
      "                  prod_cat_sub_sub_cat_lst_count_enc   prod_cat_str  \\\n",
      "0                   [9238.0, 9283.0, 9236.0, 9236.0]  A00002/B00003   \n",
      "1  [9236.0, 9254.0, 9238.0, 9256.0, 9283.0, 9256....  A00001/B00009   \n",
      "2                                           [9283.0]  A00002/B00001   \n",
      "3                           [9256.0, 9283.0, 9254.0]  A00002/B00004   \n",
      "4                                   [9254.0, 9256.0]  A00001/B00001   \n",
      "\n",
      "   prod_cat_str_count_enc  prod_cat_sub_cat_str  \\\n",
      "0                   927.0  A00002/B00003/C00006   \n",
      "1                   223.0  A00001/B00009/C00031   \n",
      "2                   689.0  A00002/B00001/C00020   \n",
      "3                   277.0  A00002/B00004/C00018   \n",
      "4                   329.0  A00001/B00001/C00012   \n",
      "\n",
      "   prod_cat_sub_cat_str_count_enc  \\\n",
      "0                           210.0   \n",
      "1                            20.0   \n",
      "2                            21.0   \n",
      "3                           141.0   \n",
      "4                            21.0   \n",
      "\n",
      "                            prod_cat_sub_sub_cat_str  \\\n",
      "0  A00002/B00003/C00006/D28437;A00002/B00003/C000...   \n",
      "1  A00001/B00009/C00031/D29407;A00001/B00009/C000...   \n",
      "2                        A00002/B00001/C00020/D16944   \n",
      "3  A00002/B00004/C00018/D10286;A00002/B00004/C000...   \n",
      "4  A00001/B00001/C00012/D30806;A00001/B00001/C000...   \n",
      "\n",
      "   prod_cat_sub_sub_cat_str_count_enc  \n",
      "0                           -999999.0  \n",
      "1                           -999999.0  \n",
      "2                           -999999.0  \n",
      "3                           -999999.0  \n",
      "4                           -999999.0  \n",
      "\n",
      "[5 rows x 84 columns]\n",
      "session_id                             0\n",
      "startTime                              0\n",
      "endTime                                0\n",
      "ProductList                            0\n",
      "gender                                 0\n",
      "session_duration_sec                   0\n",
      "prod_lst_enc                           0\n",
      "cat_lst_enc                            0\n",
      "sub_cat_lst_enc                        0\n",
      "sub_sub_cat_lst_enc                    0\n",
      "session_start_month_enc                0\n",
      "session_end_month_enc                  0\n",
      "session_start_dow_enc                  0\n",
      "session_end_dow_enc                    0\n",
      "session_start_woy_enc                  0\n",
      "session_end_woy_enc                    0\n",
      "session_start_hod_enc                  0\n",
      "session_end_hod_enc                    0\n",
      "session_start_moh_enc                  0\n",
      "session_end_moh_enc                    0\n",
      "session_start_part_of_day_enc          0\n",
      "session_end_part_of_day_enc            0\n",
      "prod_str_enc                           0\n",
      "cat_str_enc                            0\n",
      "sub_cat_str_enc                        0\n",
      "sub_sub_cat_str_enc                    0\n",
      "prod_cat_lst_enc                       0\n",
      "prod_cat_sub_cat_lst_enc               0\n",
      "prod_cat_sub_sub_cat_lst_enc           0\n",
      "prod_cat_str_enc                       0\n",
      "                                      ..\n",
      "session_end_hod                        0\n",
      "session_end_hod_count_enc              0\n",
      "session_start_moh                      0\n",
      "session_start_moh_count_enc            0\n",
      "session_end_moh                        0\n",
      "session_end_moh_count_enc              0\n",
      "session_start_part_of_day              0\n",
      "session_start_part_of_day_count_enc    0\n",
      "session_end_part_of_day                0\n",
      "session_end_part_of_day_count_enc      0\n",
      "prod_str                               0\n",
      "prod_str_count_enc                     0\n",
      "cat_str                                0\n",
      "cat_str_count_enc                      0\n",
      "sub_cat_str                            0\n",
      "sub_cat_str_count_enc                  0\n",
      "sub_sub_cat_str                        0\n",
      "sub_sub_cat_str_count_enc              0\n",
      "prod_cat_lst                           0\n",
      "prod_cat_lst_count_enc                 0\n",
      "prod_cat_sub_cat_lst                   0\n",
      "prod_cat_sub_cat_lst_count_enc         0\n",
      "prod_cat_sub_sub_cat_lst               0\n",
      "prod_cat_sub_sub_cat_lst_count_enc     0\n",
      "prod_cat_str                           0\n",
      "prod_cat_str_count_enc                 0\n",
      "prod_cat_sub_cat_str                   0\n",
      "prod_cat_sub_cat_str_count_enc         0\n",
      "prod_cat_sub_sub_cat_str               0\n",
      "prod_cat_sub_sub_cat_str_count_enc     0\n",
      "Length: 84, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df1.shape)\n",
    "print(test_df1.shape)\n",
    "print(train_df1.head())\n",
    "print(train_df1.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ratio_feats(numerator, denominator, err_value=0):\n",
    "    if (denominator) and (denominator != 0):\n",
    "        return 1.*numerator/denominator\n",
    "    else:\n",
    "        return err_value\n",
    "    \n",
    "    \n",
    "def mean_encode_mapping(data, group_col, agg_col):\n",
    "    table = pd.DataFrame(data.groupby(group_col)[agg_col].mean())\n",
    "    table.reset_index(inplace=True)\n",
    "    map_dct = dict(zip(table[group_col], table[agg_col]))\n",
    "    return map_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_features(train_data, test_data, id_col=ID_COL, dv_col=DV_COL,\n",
    "                  feat_prefix=FEAT_PREFIX):\n",
    "    \n",
    "    train_df = train_data.copy()\n",
    "    test_df = test_data.copy()\n",
    "    \n",
    "    prod_cols = ['prod_lst', 'cat_lst', 'sub_cat_lst', 'sub_sub_cat_lst']\n",
    "    prod_cols_str = ['prod_str', 'cat_str', 'sub_cat_str',\n",
    "                     'sub_sub_cat_str']\n",
    "    prod_cols_enc = [x+'_enc' for x in prod_cols]\n",
    "    prod_cols_count_enc = [x+'_count_enc' for x in prod_cols]\n",
    "    prod_cat_cols = ['prod_cat_lst', 'prod_cat_sub_cat_lst',\n",
    "                     'prod_cat_sub_sub_cat_lst',\n",
    "                     'prod_cat_str', 'prod_cat_sub_cat_str',\n",
    "                     'prod_cat_sub_sub_cat_str']\n",
    "    prod_cat_cols_enc = ['prod_cat_lst_enc', 'prod_cat_sub_cat_lst_enc',\n",
    "                         'prod_cat_sub_sub_cat_lst_enc']\n",
    "    prod_cat_cols_count_enc = ['prod_cat_lst_count_enc',\n",
    "                               'prod_cat_sub_cat_lst_count_enc',\n",
    "                               'prod_cat_sub_sub_cat_lst_count_enc']\n",
    "    drop_cols = [id_col, dv_col, 'startTime', 'endTime', 'ProductList']\n",
    "    time_cols = ['session_start_month', 'session_end_month',\n",
    "                 'session_start_dow', 'session_end_dow',\n",
    "                 'session_start_woy', 'session_end_woy',\n",
    "                 'session_start_hod', 'session_end_hod',\n",
    "                 'session_start_moh', 'session_end_moh',\n",
    "                 'session_start_part_of_day', 'session_end_part_of_day']\n",
    "    drop_cols = (drop_cols + prod_cols + prod_cols_str + prod_cols_enc +\n",
    "                 prod_cols_count_enc + prod_cat_cols + prod_cat_cols_enc +\n",
    "                 prod_cat_cols_count_enc + time_cols)\n",
    "    \n",
    "    print('count of prod_cols')\n",
    "    for col in prod_cols + ['prod_cat_lst', 'prod_cat_sub_cat_lst']:\n",
    "        train_df['num_'+col] = train_df[col].apply(lambda x: len(set(x)))\n",
    "        test_df['num_'+col] = test_df[col].apply(lambda x: len(set(x)))\n",
    "    \n",
    "    print('interaction between time_cols')\n",
    "    p1, p2, p3 = 'session_start_', 'session_end_', 'session_equal_'\n",
    "    for _name, col1, col2 in [\n",
    "        (p3+'part_of_day', p1+'part_of_day', p2+'part_of_day'),\n",
    "        (p3+'dow', p1+'dow', p2+'dow'),\n",
    "        (p3+'woy', p1+'woy', p2+'woy'),\n",
    "        (p3+'month', p1+'month', p2+'month')]:\n",
    "        train_df[_name] = list(\n",
    "            map(lambda x,y: 1 if x!=y else 0,\n",
    "                train_df[col1],\n",
    "                train_df[col2]))\n",
    "        test_df[_name] = list(\n",
    "            map(lambda x,y: 1 if x!=y else 0,\n",
    "                test_df[col1],\n",
    "                test_df[col2]))\n",
    "    \n",
    "    print('aggregating product encodings')\n",
    "    for col in prod_cols_enc:\n",
    "        print(col)\n",
    "        for _name, _func in [('_min', lambda x: np.nanmin(x)),\n",
    "                             ('_max', lambda x: np.nanmax(x)),\n",
    "                             ('_mean', lambda x: np.nanmean(x)),\n",
    "                             ('_std', lambda x: np.nanstd(x))]:\n",
    "            train_df[col+_name] = train_df[col].apply(lambda x: _func(x))\n",
    "            test_df[col+_name] = test_df[col].apply(lambda x: _func(x))\n",
    "            \n",
    "    for col in prod_cols_count_enc:\n",
    "        print(col)\n",
    "        for _name, _func in [('_min', lambda x: np.nanmin(x)),\n",
    "                             ('_max', lambda x: np.nanmax(x)),\n",
    "                             ('_mean', lambda x: np.nanmean(x)),\n",
    "                             ('_std', lambda x: np.nanstd(x))]:\n",
    "            train_df[col+_name] = train_df[col].apply(lambda x: _func(x))\n",
    "            test_df[col+_name] = test_df[col].apply(lambda x: _func(x))\n",
    "            \n",
    "    for col in prod_cat_cols_enc:\n",
    "        print(col)\n",
    "        for _name, _func in [('_min', lambda x: np.nanmin(x)),\n",
    "                             ('_max', lambda x: np.nanmax(x)),\n",
    "                             ('_mean', lambda x: np.nanmean(x)),\n",
    "                             ('_std', lambda x: np.nanstd(x))]:\n",
    "            train_df[col+_name] = train_df[col].apply(lambda x: _func(x))\n",
    "            test_df[col+_name] = test_df[col].apply(lambda x: _func(x))\n",
    "            \n",
    "    for col in prod_cat_cols_count_enc:\n",
    "        print(col)\n",
    "        for _name, _func in [('_min', lambda x: np.nanmin(x)),\n",
    "                             ('_max', lambda x: np.nanmax(x)),\n",
    "                             ('_mean', lambda x: np.nanmean(x)),\n",
    "                             ('_std', lambda x: np.nanstd(x))]:\n",
    "            train_df[col+_name] = train_df[col].apply(lambda x: _func(x))\n",
    "            test_df[col+_name] = test_df[col].apply(lambda x: _func(x))\n",
    "            \n",
    "    print('Ratio of encodings')\n",
    "    for _name, num, denom in [\n",
    "        ('ratio_prod_lst_enc_mean_min', 'prod_lst_enc_mean',\n",
    "         'prod_lst_enc_min'),\n",
    "        ('ratio_prod_lst_enc_mean_max', 'prod_lst_enc_mean',\n",
    "         'prod_lst_enc_max'),\n",
    "        ('ratio_cat_lst_enc_mean_min', 'cat_lst_enc_mean',\n",
    "         'cat_lst_enc_min'),\n",
    "        ('ratio_cat_lst_enc_mean_max', 'cat_lst_enc_mean',\n",
    "         'cat_lst_enc_max'),\n",
    "        ('ratio_sub_cat_lst_enc_mean_min', 'sub_cat_lst_enc_mean',\n",
    "         'sub_cat_lst_enc_min'),\n",
    "        ('ratio_sub_cat_lst_enc_mean_max', 'sub_cat_lst_enc_mean',\n",
    "         'sub_cat_lst_enc_max'),\n",
    "        ('ratio_sub_sub_cat_lst_enc_mean_min', 'sub_sub_cat_lst_enc_mean',\n",
    "         'sub_sub_cat_lst_enc_min'),\n",
    "        ('ratio_sub_sub_cat_lst_enc_mean_max', 'sub_sub_cat_lst_enc_mean',\n",
    "         'sub_sub_cat_lst_enc_max'),\n",
    "        ('ratio_prod_cat_lst_enc_mean_min', 'prod_cat_lst_enc_mean',\n",
    "         'prod_cat_lst_enc_min'),\n",
    "        ('ratio_prod_cat_lst_enc_mean_max', 'prod_cat_lst_enc_mean',\n",
    "         'prod_cat_lst_enc_max'),\n",
    "        ('ratio_prod_cat_sub_cat_lst_enc_mean_min', 'prod_cat_sub_cat_lst_enc_mean',\n",
    "         'prod_cat_sub_cat_lst_enc_min'),\n",
    "        ('ratio_prod_cat_sub_cat_lst_enc_mean_max', 'prod_cat_sub_cat_lst_enc_mean',\n",
    "         'prod_cat_sub_cat_lst_enc_max')]:\n",
    "        print(_name)\n",
    "        train_df[_name] = list(\n",
    "            map(lambda x, y: calc_ratio_feats(x, y),\n",
    "                train_df[num], train_df[denom]))\n",
    "        test_df[_name] = list(\n",
    "            map(lambda x, y: calc_ratio_feats(x, y),\n",
    "                test_df[num], test_df[denom]))\n",
    "    \n",
    "    print('MI of encodings')\n",
    "    for _name, col1, col2, col3 in [\n",
    "        ('prod_cat_lst_enc_mi', 'prod_cat_lst_enc_mean', 'prod_lst_enc_mean',\n",
    "         'cat_lst_enc_mean'),\n",
    "        ('prod_cat_sub_cat_lst_enc_mi', 'prod_cat_sub_cat_lst_enc_mean',\n",
    "         'prod_cat_lst_enc_mean', 'sub_cat_lst_enc_mean'),\n",
    "        ('prod_cat_sub_sub_cat_lst_enc_mi', 'prod_cat_sub_sub_cat_lst_enc_mean',\n",
    "         'prod_cat_sub_cat_lst_enc_mean', 'sub_sub_cat_lst_enc_mean')]:\n",
    "        print(_name)\n",
    "        train_df[_name] = list(map(lambda x,y,z: 1.*x/(y*z),\n",
    "                                      train_df[col1],\n",
    "                                      train_df[col2],\n",
    "                                      train_df[col3]))\n",
    "        test_df[_name] = list(map(lambda x,y,z: 1.*x/(y*z),\n",
    "                                      test_df[col1],\n",
    "                                      test_df[col2],\n",
    "                                      test_df[col3]))\n",
    "        \n",
    "    print('mean encodings by time_cols')\n",
    "    for col1 in ['session_start_part_of_day', 'session_end_part_of_day']:\n",
    "        for col2 in ['prod_str_enc', 'cat_str_enc', 'sub_cat_str_enc',\n",
    "                     'sub_sub_cat_str_enc', 'prod_cat_lst_enc_mean',\n",
    "                     'prod_cat_sub_cat_lst_enc_mean']:\n",
    "            map_dct = mean_encode_mapping(train_df, col1, col2)\n",
    "            train_df[col1+'_meanenc_'+col2] = train_df[col1].apply(\n",
    "                lambda x: map_dct[x])\n",
    "            \n",
    "            map_dct = mean_encode_mapping(test_df, col1, col2)\n",
    "            test_df[col1+'_meanenc_'+col2] = test_df[col1].apply(\n",
    "                lambda x: map_dct[x])\n",
    "    \n",
    "    \n",
    "    feat_cols = [x for x in list(train_df.columns)\n",
    "                 if x not in drop_cols]\n",
    "    print(feat_cols)\n",
    "    feat_cols_renamed = [feat_prefix+x for x in feat_cols]\n",
    "    rename_dct = dict(zip(feat_cols, feat_cols_renamed))\n",
    "    train_df.rename(columns=rename_dct, inplace=True)\n",
    "    test_df.rename(columns=rename_dct, inplace=True)\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of prod_cols\n",
      "interaction between time_cols\n",
      "aggregating product encodings\n",
      "prod_lst_enc\n",
      "cat_lst_enc\n",
      "sub_cat_lst_enc\n",
      "sub_sub_cat_lst_enc\n",
      "prod_lst_count_enc\n",
      "cat_lst_count_enc\n",
      "sub_cat_lst_count_enc\n",
      "sub_sub_cat_lst_count_enc\n",
      "prod_cat_lst_enc\n",
      "prod_cat_sub_cat_lst_enc\n",
      "prod_cat_sub_sub_cat_lst_enc\n",
      "prod_cat_lst_count_enc\n",
      "prod_cat_sub_cat_lst_count_enc\n",
      "prod_cat_sub_sub_cat_lst_count_enc\n",
      "Ratio of encodings\n",
      "ratio_prod_lst_enc_mean_min\n",
      "ratio_prod_lst_enc_mean_max\n",
      "ratio_cat_lst_enc_mean_min\n",
      "ratio_cat_lst_enc_mean_max\n",
      "ratio_sub_cat_lst_enc_mean_min\n",
      "ratio_sub_cat_lst_enc_mean_max\n",
      "ratio_sub_sub_cat_lst_enc_mean_min\n",
      "ratio_sub_sub_cat_lst_enc_mean_max\n",
      "ratio_prod_cat_lst_enc_mean_min\n",
      "ratio_prod_cat_lst_enc_mean_max\n",
      "ratio_prod_cat_sub_cat_lst_enc_mean_min\n",
      "ratio_prod_cat_sub_cat_lst_enc_mean_max\n",
      "MI of encodings\n",
      "prod_cat_lst_enc_mi\n",
      "prod_cat_sub_cat_lst_enc_mi\n",
      "prod_cat_sub_sub_cat_lst_enc_mi\n",
      "mean encodings by time_cols\n",
      "['session_duration_sec', 'session_start_month_enc', 'session_end_month_enc', 'session_start_dow_enc', 'session_end_dow_enc', 'session_start_woy_enc', 'session_end_woy_enc', 'session_start_hod_enc', 'session_end_hod_enc', 'session_start_moh_enc', 'session_end_moh_enc', 'session_start_part_of_day_enc', 'session_end_part_of_day_enc', 'prod_str_enc', 'cat_str_enc', 'sub_cat_str_enc', 'sub_sub_cat_str_enc', 'prod_cat_str_enc', 'prod_cat_sub_cat_str_enc', 'prod_cat_sub_sub_cat_str_enc', 'session_start_month_count_enc', 'session_end_month_count_enc', 'session_start_dow_count_enc', 'session_end_dow_count_enc', 'session_start_woy_count_enc', 'session_end_woy_count_enc', 'session_start_hod_count_enc', 'session_end_hod_count_enc', 'session_start_moh_count_enc', 'session_end_moh_count_enc', 'session_start_part_of_day_count_enc', 'session_end_part_of_day_count_enc', 'prod_str_count_enc', 'cat_str_count_enc', 'sub_cat_str_count_enc', 'sub_sub_cat_str_count_enc', 'prod_cat_str_count_enc', 'prod_cat_sub_cat_str_count_enc', 'prod_cat_sub_sub_cat_str_count_enc', 'num_prod_lst', 'num_cat_lst', 'num_sub_cat_lst', 'num_sub_sub_cat_lst', 'num_prod_cat_lst', 'num_prod_cat_sub_cat_lst', 'session_equal_part_of_day', 'session_equal_dow', 'session_equal_woy', 'session_equal_month', 'prod_lst_enc_min', 'prod_lst_enc_max', 'prod_lst_enc_mean', 'prod_lst_enc_std', 'cat_lst_enc_min', 'cat_lst_enc_max', 'cat_lst_enc_mean', 'cat_lst_enc_std', 'sub_cat_lst_enc_min', 'sub_cat_lst_enc_max', 'sub_cat_lst_enc_mean', 'sub_cat_lst_enc_std', 'sub_sub_cat_lst_enc_min', 'sub_sub_cat_lst_enc_max', 'sub_sub_cat_lst_enc_mean', 'sub_sub_cat_lst_enc_std', 'prod_lst_count_enc_min', 'prod_lst_count_enc_max', 'prod_lst_count_enc_mean', 'prod_lst_count_enc_std', 'cat_lst_count_enc_min', 'cat_lst_count_enc_max', 'cat_lst_count_enc_mean', 'cat_lst_count_enc_std', 'sub_cat_lst_count_enc_min', 'sub_cat_lst_count_enc_max', 'sub_cat_lst_count_enc_mean', 'sub_cat_lst_count_enc_std', 'sub_sub_cat_lst_count_enc_min', 'sub_sub_cat_lst_count_enc_max', 'sub_sub_cat_lst_count_enc_mean', 'sub_sub_cat_lst_count_enc_std', 'prod_cat_lst_enc_min', 'prod_cat_lst_enc_max', 'prod_cat_lst_enc_mean', 'prod_cat_lst_enc_std', 'prod_cat_sub_cat_lst_enc_min', 'prod_cat_sub_cat_lst_enc_max', 'prod_cat_sub_cat_lst_enc_mean', 'prod_cat_sub_cat_lst_enc_std', 'prod_cat_sub_sub_cat_lst_enc_min', 'prod_cat_sub_sub_cat_lst_enc_max', 'prod_cat_sub_sub_cat_lst_enc_mean', 'prod_cat_sub_sub_cat_lst_enc_std', 'prod_cat_lst_count_enc_min', 'prod_cat_lst_count_enc_max', 'prod_cat_lst_count_enc_mean', 'prod_cat_lst_count_enc_std', 'prod_cat_sub_cat_lst_count_enc_min', 'prod_cat_sub_cat_lst_count_enc_max', 'prod_cat_sub_cat_lst_count_enc_mean', 'prod_cat_sub_cat_lst_count_enc_std', 'prod_cat_sub_sub_cat_lst_count_enc_min', 'prod_cat_sub_sub_cat_lst_count_enc_max', 'prod_cat_sub_sub_cat_lst_count_enc_mean', 'prod_cat_sub_sub_cat_lst_count_enc_std', 'ratio_prod_lst_enc_mean_min', 'ratio_prod_lst_enc_mean_max', 'ratio_cat_lst_enc_mean_min', 'ratio_cat_lst_enc_mean_max', 'ratio_sub_cat_lst_enc_mean_min', 'ratio_sub_cat_lst_enc_mean_max', 'ratio_sub_sub_cat_lst_enc_mean_min', 'ratio_sub_sub_cat_lst_enc_mean_max', 'ratio_prod_cat_lst_enc_mean_min', 'ratio_prod_cat_lst_enc_mean_max', 'ratio_prod_cat_sub_cat_lst_enc_mean_min', 'ratio_prod_cat_sub_cat_lst_enc_mean_max', 'prod_cat_lst_enc_mi', 'prod_cat_sub_cat_lst_enc_mi', 'prod_cat_sub_sub_cat_lst_enc_mi', 'session_start_part_of_day_meanenc_prod_str_enc', 'session_start_part_of_day_meanenc_cat_str_enc', 'session_start_part_of_day_meanenc_sub_cat_str_enc', 'session_start_part_of_day_meanenc_sub_sub_cat_str_enc', 'session_start_part_of_day_meanenc_prod_cat_lst_enc_mean', 'session_start_part_of_day_meanenc_prod_cat_sub_cat_lst_enc_mean', 'session_end_part_of_day_meanenc_prod_str_enc', 'session_end_part_of_day_meanenc_cat_str_enc', 'session_end_part_of_day_meanenc_sub_cat_str_enc', 'session_end_part_of_day_meanenc_sub_sub_cat_str_enc', 'session_end_part_of_day_meanenc_prod_cat_lst_enc_mean', 'session_end_part_of_day_meanenc_prod_cat_sub_cat_lst_enc_mean']\n"
     ]
    }
   ],
   "source": [
    "train_final, test_final = calc_features(train_df1, test_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10500, 177)\n",
      "(4500, 176)\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "print(train_final.shape)\n",
    "print(test_final.shape)\n",
    "feat_cols = [x for x in list(train_final.columns) if x.startswith(FEAT_PREFIX)]\n",
    "print(len(feat_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import operator, joblib, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_feature_map(features, fmap_fn):\n",
    "    outfile = open(fmap_fn, 'w')\n",
    "    for i, feat in enumerate(features):\n",
    "        outfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n",
    "    outfile.close()\n",
    "\n",
    "\n",
    "def runXGB(fmap_fn, imp_fn, train_X, train_y, test_X, test_y=None,\n",
    "           test_X2=None, feature_names=None, seed_val=0, rounds=500, dep=8,\n",
    "           eta=0.05):\n",
    "    # define hyperparameters\n",
    "    params = {}\n",
    "    params[\"objective\"] = \"binary:logistic\"\n",
    "    params['eval_metric'] = 'auc'\n",
    "    params[\"eta\"] = eta\n",
    "    params[\"subsample\"] = 0.7\n",
    "    params[\"min_child_weight\"] = 1\n",
    "    params[\"colsample_bytree\"] = 0.7\n",
    "    params[\"max_depth\"] = dep\n",
    "\n",
    "    params[\"silent\"] = 1\n",
    "    params[\"seed\"] = seed_val\n",
    "    # params[\"max_delta_step\"] = 2\n",
    "    # params[\"gamma\"] = 0.5\n",
    "    num_rounds = rounds\n",
    "\n",
    "    plst = list(params.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [(xgtrain, 'train'), (xgtest, 'test')]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist,\n",
    "                          early_stopping_rounds=100, verbose_eval=20)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    if feature_names is not None:\n",
    "        create_feature_map(feature_names, fmap_fn)\n",
    "        importance = model.get_fscore()\n",
    "        importance = sorted(importance.items(), key=operator.itemgetter(1),\n",
    "                            reverse=True)\n",
    "        imp_df = pd.DataFrame(importance, columns=['feature', 'fscore'])\n",
    "        imp_df['fscore'] = imp_df['fscore'] / imp_df['fscore'].sum()\n",
    "        imp_df.to_csv(imp_fn, index=False, encoding='utf-8')\n",
    "\n",
    "    pred_test_y = model.predict(xgtest, ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "    if test_X2 is not None:\n",
    "        pred_test_y2 = model.predict(xgb.DMatrix(test_X2),\n",
    "                                     ntree_limit=model.best_ntree_limit)\n",
    "    else:\n",
    "        pred_test_y2 = None\n",
    "\n",
    "    loss = 0\n",
    "    if test_y is not None:\n",
    "        loss = metrics.roc_auc_score(test_y, pred_test_y)\n",
    "\n",
    "    return pred_test_y, loss, pred_test_y2, model\n",
    "\n",
    "\n",
    "def runLGB(train_X, train_y, test_X, test_y=None, test_X2=None,\n",
    "           feature_names=None, seed_val=0, rounds=500, dep=8, eta=0.05):\n",
    "    params = {}\n",
    "    params['boosting_type'] = 'gbdt'\n",
    "    params[\"objective\"] = \"binary\"\n",
    "    params['metric'] = 'binary_error'\n",
    "    params[\"max_depth\"] = dep\n",
    "    params[\"min_data_in_leaf\"] = 10\n",
    "    params['subsample'] = 0.8\n",
    "    params['reg_alpha'] = 0.1\n",
    "    params[\"learning_rate\"] = eta\n",
    "    params[\"bagging_fraction\"] = 0.8\n",
    "    params[\"feature_fraction\"] = 0.8\n",
    "    params[\"bagging_freq\"] = 5\n",
    "    params[\"bagging_seed\"] = seed_val\n",
    "    params[\"verbosity\"] = 0\n",
    "    num_rounds = rounds\n",
    "\n",
    "    lgtrain = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        lgtest = lgb.Dataset(test_X, label=test_y)\n",
    "        model = lgb.train(params, lgtrain, num_rounds, valid_sets=[lgtest],\n",
    "                          early_stopping_rounds=300, verbose_eval=20)\n",
    "    else:\n",
    "        lgtest = lgb.DMatrix(test_X)\n",
    "        model = lgb.train(params, lgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "    pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n",
    "    pred_class_y = np.where(pred_test_y > 0.5, 1, 0)\n",
    "    loss = 0\n",
    "    if test_y is not None:\n",
    "        loss = get_accuracy(test_y, pred_class_y)\n",
    "\n",
    "    return pred_test_y, pred_class_y, loss, pred_test_y2, model\n",
    "\n",
    "\n",
    "def runRF(train_X, train_y, test_X, test_y=None, test_X2=None,\n",
    "          dep=20, num_trees=1000, leaf=10, feat=0.2):\n",
    "    model = ensemble.RandomForestClassifier(\n",
    "            n_estimators = num_trees,\n",
    "                    max_depth = dep,\n",
    "                    min_samples_split = 2,\n",
    "                    min_samples_leaf = leaf,\n",
    "                    max_features =  feat,\n",
    "                    n_jobs = 4,\n",
    "                    random_state = 0)\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_train_class_y = model.predict(train_X)\n",
    "    pred_test_class_y = model.predict(test_X)\n",
    "    pred_test_y = model.predict_proba(test_X)[:,1]\n",
    "    pred_test_class_y2 = model.predict(test_X2)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)[:,1]\n",
    "    test_loss = 0\n",
    "    \n",
    "    train_loss = get_accuracy(train_y, pred_train_class_y)\n",
    "    test_loss = get_accuracy(test_y, pred_test_class_y)\n",
    "    print(\"Train and Test accuracy: \", train_loss, test_loss)\n",
    "    return pred_test_y, pred_test_class_y, test_loss, pred_test_y2, pred_test_class_y2, model\n",
    "\n",
    "\n",
    "def xgb_predict(data, model):\n",
    "    xgtest = xgb.DMatrix(data)\n",
    "    probs = list(model.predict(xgtest, ntree_limit=model.best_ntree_limit))\n",
    "    return probs\n",
    "\n",
    "\n",
    "def xgb_persist(model, model_fn):\n",
    "    joblib.dump(model, model_fn)\n",
    "\n",
    "\n",
    "def xgb_load(model_fn):\n",
    "    return joblib.load(model_fn)\n",
    "\n",
    "\n",
    "def get_auc(actual, pred):\n",
    "    auc = roc_auc_score(actual, pred)\n",
    "    return max(auc, 1-auc)\n",
    "\n",
    "\n",
    "def get_accuracy(actual, pred):\n",
    "    return 100.*(actual==pred).mean()\n",
    "\n",
    "\n",
    "def get_prec_rec_f_score(actual, pred, pos_class=1):\n",
    "\n",
    "    if pos_class == 1:\n",
    "        tn, fp, fn, tp = confusion_matrix(actual, pred, labels=[0,1]).ravel()\n",
    "    elif pos_class == 0:\n",
    "        tp, fn, fp, tn = confusion_matrix(actual, pred, labels=[0,1]).ravel()\n",
    "    prec = 100.*(tp/float(tp+fp+0.000000001))\n",
    "    rec = 100.*(tp/float(tp+fn+0.000000001))\n",
    "    f_score = 2.*prec*rec/(prec+rec+0.000000001)\n",
    "    return prec, rec, f_score\n",
    "\n",
    "\n",
    "def calc_metrics(data, dv_col, score_col, pred_class_col, id_col='deal_id'):\n",
    "\n",
    "    df = data.copy()\n",
    "\n",
    "    assert df.shape[0] == df[id_col].nunique()\n",
    "\n",
    "    feats = ['num_deals', 'dv_rate', 'auc', 'accuracy',\n",
    "             'prec_win', 'rec_win', 'f_score_win', 'prec_lost',\n",
    "             'rec_lost', 'f_score_lost']\n",
    "    actual = df[dv_col]\n",
    "    pred = df[pred_class_col]\n",
    "    score = df[score_col]\n",
    "    prec_good, rec_good, f_score_good = get_prec_rec_f_score(actual, pred, 1)\n",
    "    prec_bad, rec_bad, f_score_bad = get_prec_rec_f_score(actual, pred, 0)\n",
    "    feat_values = [df.shape[0], round(df[dv_col].mean(), 4),\n",
    "                   get_auc(actual, score),\n",
    "                   get_accuracy(actual, pred),\n",
    "                   prec_good, rec_good, f_score_good,\n",
    "                   prec_bad, rec_bad, f_score_bad]\n",
    "    return dict(zip(feats, feat_values))\n",
    "\n",
    "\n",
    "def find_optimal_threshold(data, dv_col, score_col, prob_thresholds,\n",
    "                           id_col='deal_id'):\n",
    "\n",
    "    df = data[[id_col, dv_col, score_col]]\n",
    "\n",
    "    accs = []\n",
    "    max_acc = 0\n",
    "    for prob_threshold in prob_thresholds:\n",
    "        print(prob_threshold)\n",
    "        df['pred_class'] = df[score_col].apply(\n",
    "            lambda x: 1 if x >= prob_threshold else 0)\n",
    "        acc = get_accuracy(df[dv_col], df['pred_class'])\n",
    "        accs.append(acc)\n",
    "        if acc > max_acc:\n",
    "            max_acc = acc\n",
    "\n",
    "    max_idx = np.argmax(accs)\n",
    "    return prob_thresholds[max_idx], max_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True False  True False  True  True  True  True  True  True  True\n",
      " False False False False False  True  True False  True False  True False\n",
      "  True False  True  True  True  True False False False False  True  True\n",
      " False False False False False False False False False False False False\n",
      " False  True False False False  True False False False  True  True False\n",
      " False False False  True False False False False False  True False False\n",
      " False  True  True False False False False  True False False  True  True\n",
      " False  True  True False False False False False False  True False False\n",
      " False  True False False False False False False False False False False\n",
      " False False False  True  True False False False False  True  True  True\n",
      " False False False False False False False False False False False False]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'fea_rank'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-778-339f6d30157c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m fea_rank_ = pd.DataFrame({'cols':feat_cols,\n\u001b[1;32m     13\u001b[0m                           'feat_rank':rfe.ranking_})\n\u001b[0;32m---> 14\u001b[0;31m fea_rank_.loc[fea_rank_.fea_rank > 0].sort_values(\n\u001b[0m\u001b[1;32m     15\u001b[0m     by=['feat_rank'], ascending = True)\n",
      "\u001b[0;32m~/.virtualenvs/rasa/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5065\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5066\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5067\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5069\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'fea_rank'"
     ]
    }
   ],
   "source": [
    "# feature selection\n",
    "from sklearn.feature_selection import RFE\n",
    "# create the RFE model and select 10 attributes\n",
    "gbm = lgb.LGBMClassifier()\n",
    "rfe = RFE(gbm, 40)\n",
    "rfe = rfe.fit(train_final[feat_cols], train_final[DV_COL])\n",
    "\n",
    "# summarize the selection of the attributes\n",
    "print(rfe.support_)\n",
    "\n",
    "# summarize the ranking of the attributes\n",
    "fea_rank_ = pd.DataFrame({'cols':feat_cols,\n",
    "                          'feat_rank':rfe.ranking_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "fea_rank_.sort_values(by=['feat_rank'], ascending = True, inplace=True)\n",
    "fea_rank_.reset_index(drop=True, inplace=True)\n",
    "selected_feat_cols = fea_rank_['cols'][:40]\n",
    "print(len(selected_feat_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'lgb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model building..\n",
      "Iteration:  1\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[20]\tvalid_0's binary_error: 0.120952\n",
      "[40]\tvalid_0's binary_error: 0.112381\n",
      "[60]\tvalid_0's binary_error: 0.11\n",
      "[80]\tvalid_0's binary_error: 0.112381\n",
      "[100]\tvalid_0's binary_error: 0.111429\n",
      "[120]\tvalid_0's binary_error: 0.112381\n",
      "[140]\tvalid_0's binary_error: 0.112381\n",
      "[160]\tvalid_0's binary_error: 0.112857\n",
      "[180]\tvalid_0's binary_error: 0.112381\n",
      "[200]\tvalid_0's binary_error: 0.113333\n",
      "[220]\tvalid_0's binary_error: 0.110476\n",
      "[240]\tvalid_0's binary_error: 0.110476\n",
      "[260]\tvalid_0's binary_error: 0.111905\n",
      "[280]\tvalid_0's binary_error: 0.114762\n",
      "[300]\tvalid_0's binary_error: 0.112381\n",
      "[320]\tvalid_0's binary_error: 0.112857\n",
      "[340]\tvalid_0's binary_error: 0.110476\n",
      "Early stopping, best iteration is:\n",
      "[57]\tvalid_0's binary_error: 0.109048\n",
      "[89.09523809523809]\n",
      "mean cv score:  89.09523809523809\n",
      "Iteration:  2\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[20]\tvalid_0's binary_error: 0.131429\n",
      "[40]\tvalid_0's binary_error: 0.128095\n",
      "[60]\tvalid_0's binary_error: 0.12619\n",
      "[80]\tvalid_0's binary_error: 0.127143\n",
      "[100]\tvalid_0's binary_error: 0.127619\n",
      "[120]\tvalid_0's binary_error: 0.12619\n",
      "[140]\tvalid_0's binary_error: 0.124286\n",
      "[160]\tvalid_0's binary_error: 0.125714\n",
      "[180]\tvalid_0's binary_error: 0.125238\n",
      "[200]\tvalid_0's binary_error: 0.125714\n",
      "[220]\tvalid_0's binary_error: 0.12619\n",
      "[240]\tvalid_0's binary_error: 0.124286\n",
      "[260]\tvalid_0's binary_error: 0.123333\n",
      "[280]\tvalid_0's binary_error: 0.121429\n",
      "[300]\tvalid_0's binary_error: 0.121905\n",
      "[320]\tvalid_0's binary_error: 0.121905\n",
      "[340]\tvalid_0's binary_error: 0.121429\n",
      "[360]\tvalid_0's binary_error: 0.122381\n",
      "[380]\tvalid_0's binary_error: 0.121429\n",
      "[400]\tvalid_0's binary_error: 0.121429\n",
      "[420]\tvalid_0's binary_error: 0.121429\n",
      "[440]\tvalid_0's binary_error: 0.121429\n",
      "[460]\tvalid_0's binary_error: 0.122857\n",
      "[480]\tvalid_0's binary_error: 0.122857\n",
      "[500]\tvalid_0's binary_error: 0.121429\n",
      "[520]\tvalid_0's binary_error: 0.121905\n",
      "[540]\tvalid_0's binary_error: 0.122381\n",
      "[560]\tvalid_0's binary_error: 0.122857\n",
      "Early stopping, best iteration is:\n",
      "[276]\tvalid_0's binary_error: 0.120476\n",
      "[89.09523809523809, 87.95238095238095]\n",
      "mean cv score:  88.52380952380952\n",
      "Iteration:  3\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[20]\tvalid_0's binary_error: 0.132381\n",
      "[40]\tvalid_0's binary_error: 0.128571\n",
      "[60]\tvalid_0's binary_error: 0.13\n",
      "[80]\tvalid_0's binary_error: 0.129048\n",
      "[100]\tvalid_0's binary_error: 0.129048\n",
      "[120]\tvalid_0's binary_error: 0.131429\n",
      "[140]\tvalid_0's binary_error: 0.131429\n",
      "[160]\tvalid_0's binary_error: 0.130952\n",
      "[180]\tvalid_0's binary_error: 0.131429\n",
      "[200]\tvalid_0's binary_error: 0.130476\n",
      "[220]\tvalid_0's binary_error: 0.131429\n",
      "[240]\tvalid_0's binary_error: 0.128095\n",
      "[260]\tvalid_0's binary_error: 0.128095\n",
      "[280]\tvalid_0's binary_error: 0.129048\n",
      "[300]\tvalid_0's binary_error: 0.131905\n",
      "[320]\tvalid_0's binary_error: 0.130476\n",
      "[340]\tvalid_0's binary_error: 0.129048\n",
      "[360]\tvalid_0's binary_error: 0.129524\n",
      "[380]\tvalid_0's binary_error: 0.13\n",
      "[400]\tvalid_0's binary_error: 0.128571\n",
      "[420]\tvalid_0's binary_error: 0.128571\n",
      "[440]\tvalid_0's binary_error: 0.129048\n",
      "[460]\tvalid_0's binary_error: 0.128571\n",
      "[480]\tvalid_0's binary_error: 0.129524\n",
      "[500]\tvalid_0's binary_error: 0.128571\n",
      "[520]\tvalid_0's binary_error: 0.129048\n",
      "[540]\tvalid_0's binary_error: 0.128571\n",
      "Early stopping, best iteration is:\n",
      "[256]\tvalid_0's binary_error: 0.126667\n",
      "[89.09523809523809, 87.95238095238095, 87.33333333333333]\n",
      "mean cv score:  88.12698412698411\n",
      "Iteration:  4\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[20]\tvalid_0's binary_error: 0.137143\n",
      "[40]\tvalid_0's binary_error: 0.133333\n",
      "[60]\tvalid_0's binary_error: 0.132857\n",
      "[80]\tvalid_0's binary_error: 0.134286\n",
      "[100]\tvalid_0's binary_error: 0.13381\n",
      "[120]\tvalid_0's binary_error: 0.13381\n",
      "[140]\tvalid_0's binary_error: 0.13381\n",
      "[160]\tvalid_0's binary_error: 0.13381\n",
      "[180]\tvalid_0's binary_error: 0.133333\n",
      "[200]\tvalid_0's binary_error: 0.134286\n",
      "[220]\tvalid_0's binary_error: 0.135238\n",
      "[240]\tvalid_0's binary_error: 0.13381\n",
      "[260]\tvalid_0's binary_error: 0.135714\n",
      "[280]\tvalid_0's binary_error: 0.134762\n",
      "[300]\tvalid_0's binary_error: 0.134286\n",
      "[320]\tvalid_0's binary_error: 0.132857\n",
      "[340]\tvalid_0's binary_error: 0.132381\n",
      "[360]\tvalid_0's binary_error: 0.132857\n",
      "[380]\tvalid_0's binary_error: 0.131905\n",
      "[400]\tvalid_0's binary_error: 0.132381\n",
      "[420]\tvalid_0's binary_error: 0.132857\n",
      "[440]\tvalid_0's binary_error: 0.132381\n",
      "[460]\tvalid_0's binary_error: 0.132857\n",
      "[480]\tvalid_0's binary_error: 0.131905\n",
      "[500]\tvalid_0's binary_error: 0.132381\n",
      "[520]\tvalid_0's binary_error: 0.132381\n",
      "[540]\tvalid_0's binary_error: 0.132857\n",
      "[560]\tvalid_0's binary_error: 0.133333\n",
      "[580]\tvalid_0's binary_error: 0.130952\n",
      "[600]\tvalid_0's binary_error: 0.130952\n",
      "[620]\tvalid_0's binary_error: 0.132857\n",
      "[640]\tvalid_0's binary_error: 0.132857\n",
      "[660]\tvalid_0's binary_error: 0.132381\n",
      "[680]\tvalid_0's binary_error: 0.133333\n",
      "Early stopping, best iteration is:\n",
      "[383]\tvalid_0's binary_error: 0.130952\n",
      "[89.09523809523809, 87.95238095238095, 87.33333333333333, 86.90476190476191]\n",
      "mean cv score:  87.82142857142857\n",
      "Iteration:  5\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[20]\tvalid_0's binary_error: 0.137619\n",
      "[40]\tvalid_0's binary_error: 0.132857\n",
      "[60]\tvalid_0's binary_error: 0.133333\n",
      "[80]\tvalid_0's binary_error: 0.131429\n",
      "[100]\tvalid_0's binary_error: 0.132381\n",
      "[120]\tvalid_0's binary_error: 0.132381\n",
      "[140]\tvalid_0's binary_error: 0.131905\n",
      "[160]\tvalid_0's binary_error: 0.132381\n",
      "[180]\tvalid_0's binary_error: 0.129524\n",
      "[200]\tvalid_0's binary_error: 0.13\n",
      "[220]\tvalid_0's binary_error: 0.13\n",
      "[240]\tvalid_0's binary_error: 0.130476\n",
      "[260]\tvalid_0's binary_error: 0.131429\n",
      "[280]\tvalid_0's binary_error: 0.130952\n",
      "[300]\tvalid_0's binary_error: 0.131905\n",
      "[320]\tvalid_0's binary_error: 0.130952\n",
      "[340]\tvalid_0's binary_error: 0.130952\n",
      "[360]\tvalid_0's binary_error: 0.131429\n",
      "[380]\tvalid_0's binary_error: 0.131905\n",
      "[400]\tvalid_0's binary_error: 0.131905\n",
      "[420]\tvalid_0's binary_error: 0.134762\n",
      "[440]\tvalid_0's binary_error: 0.134286\n",
      "[460]\tvalid_0's binary_error: 0.134286\n",
      "[480]\tvalid_0's binary_error: 0.135238\n",
      "[500]\tvalid_0's binary_error: 0.133333\n",
      "Early stopping, best iteration is:\n",
      "[208]\tvalid_0's binary_error: 0.129048\n",
      "[89.09523809523809, 87.95238095238095, 87.33333333333333, 86.90476190476191, 87.09523809523809]\n",
      "mean cv score:  87.67619047619047\n",
      "87.67619047619047\n"
     ]
    }
   ],
   "source": [
    "print(\"Model building..\")\n",
    "feat_cols = [x for x in list(train_final.columns)\n",
    "             if (x.startswith(FEAT_PREFIX))]\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "cv_scores = []\n",
    "models = []\n",
    "pred_test_full = 0\n",
    "pred_val_full = np.zeros(train_final.shape[0])\n",
    "pred_class_val_full = np.zeros(train_final.shape[0])\n",
    "count = 0\n",
    "for dev_index, val_index in kf.split(train_final):\n",
    "    print('Iteration: ', count+1)\n",
    "    dev_X, val_X = (train_final.loc[dev_index, selected_feat_cols],\n",
    "                    train_final.loc[val_index, selected_feat_cols])\n",
    "    dev_y, val_y = (train_final.loc[dev_index, DV_COL],\n",
    "                    train_final.loc[val_index, DV_COL])\n",
    "\n",
    "    if MODEL_NAME == 'xgb':\n",
    "        pred_val, loss, pred_test, model = runXGB(\n",
    "            fmap_fn=os.path.join(DATA_DIR, 'xgb_feat_map_fname'),\n",
    "            imp_fn=os.path.join(DATA_DIR, 'xgb_feat_imp_fname'),\n",
    "            train_X=dev_X, train_y=dev_y, test_X=val_X, test_y=val_y,\n",
    "            test_X2=test_final[selected_feat_cols], rounds=5000, dep=8,\n",
    "            feature_names=selected_feat_cols)\n",
    "    elif MODEL_NAME == 'lgb':\n",
    "        pred_val, pred_class, loss, pred_test, model = runLGB(\n",
    "            train_X=dev_X, train_y=dev_y, test_X=val_X, test_y=val_y,\n",
    "            test_X2=test_final[selected_feat_cols], rounds=5000, dep=6,\n",
    "            feature_names=selected_feat_cols)\n",
    "    elif MODEL_NAME == 'rf':\n",
    "        pred_val, pred_class, loss, pred_test, pred_test_class, model = runRF(\n",
    "            train_X=dev_X, train_y=dev_y, test_X=val_X, test_y=val_y,\n",
    "            test_X2=test_final[selected_feat_cols], num_trees=1000, dep=20,\n",
    "            feat=0.7)\n",
    "    pred_val_full[val_index] = pred_val\n",
    "    pred_class_val_full[val_index] = pred_class\n",
    "    pred_test_full = pred_test_full + pred_test\n",
    "    cv_scores.append(loss)\n",
    "    models.append(model)\n",
    "    print(cv_scores)\n",
    "    print('mean cv score: ', np.mean(cv_scores))\n",
    "    count += 1\n",
    "\n",
    "pred_test_full /= 5.\n",
    "print(get_accuracy(train_final[DV_COL], pred_class_val_full))\n",
    "train_final['pred_prob'] = pred_val_full\n",
    "train_final['pred_class'] = pred_class_val_full\n",
    "\n",
    "test_final['pred_prob'] = pred_test_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal threshold - optimized for accuracy\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages/ipykernel_launcher.py:202: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "0.02\n",
      "0.03\n",
      "0.04\n",
      "0.05\n",
      "0.06\n",
      "0.07\n",
      "0.08\n",
      "0.09\n",
      "0.1\n",
      "0.11\n",
      "0.12\n",
      "0.13\n",
      "0.14\n",
      "0.15\n",
      "0.16\n",
      "0.17\n",
      "0.18\n",
      "0.19\n",
      "0.2\n",
      "0.21\n",
      "0.22\n",
      "0.23\n",
      "0.24\n",
      "0.25\n",
      "0.26\n",
      "0.27\n",
      "0.28\n",
      "0.29\n",
      "0.3\n",
      "0.31\n",
      "0.32\n",
      "0.33\n",
      "0.34\n",
      "0.35\n",
      "0.36\n",
      "0.37\n",
      "0.38\n",
      "0.39\n",
      "0.4\n",
      "0.41\n",
      "0.42\n",
      "0.43\n",
      "0.44\n",
      "0.45\n",
      "0.46\n",
      "0.47\n",
      "0.48\n",
      "0.49\n",
      "0.5\n",
      "0.51\n",
      "0.52\n",
      "0.53\n",
      "0.54\n",
      "0.55\n",
      "0.56\n",
      "0.57\n",
      "0.58\n",
      "0.59\n",
      "0.6\n",
      "0.61\n",
      "0.62\n",
      "0.63\n",
      "0.64\n",
      "0.65\n",
      "0.66\n",
      "0.67\n",
      "0.68\n",
      "0.69\n",
      "0.7\n"
     ]
    }
   ],
   "source": [
    "print('optimal threshold - optimized for accuracy')\n",
    "PROB_THRESHOLDS = [x/100. for x in range(71)]\n",
    "best_threshold, max_acc = find_optimal_threshold(train_final, dv_col=DV_COL,\n",
    "                                        score_col='pred_prob',\n",
    "                                        prob_thresholds=PROB_THRESHOLDS,\n",
    "                                        id_col=ID_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 87.67619047619047)"
      ]
     },
     "execution_count": 786,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_threshold, max_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final['pred_class'] = test_final['pred_prob'].apply(\n",
    "    lambda x: 1 if x >= best_threshold else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500, 178)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.16577777777777777"
      ]
     },
     "execution_count": 788,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_final.shape)\n",
    "test_final['pred_class'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500, 3)\n",
      "  session_id  pred_class  gender\n",
      "0     u12112           0  female\n",
      "1     u19725           0  female\n",
      "2     u11795           0  female\n",
      "3     u22639           1    male\n",
      "4     u18034           0  female\n",
      "female    3754\n",
      "male       746\n",
      "Name: gender, dtype: int64\n",
      "0    3754\n",
      "1     746\n",
      "Name: pred_class, dtype: int64\n",
      "0.16577777777777777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "out_df = test_final[[ID_COL, 'pred_class']]\n",
    "out_df[DV_COL] = out_df['pred_class'].apply(lambda x: \"male\" if x==1 else\n",
    "                                            \"female\")\n",
    "print(out_df.shape)\n",
    "print(out_df.head())\n",
    "print(out_df[DV_COL].value_counts())\n",
    "print(out_df['pred_class'].value_counts())\n",
    "print(out_df['pred_class'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages/pandas/core/frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "# save\n",
    "out_df.drop('pred_class', axis=1, inplace=True)\n",
    "out_fn = os.path.join(DATA_DIR, \"lgb_Merror_v10.csv\")\n",
    "out_df.to_csv(out_fn, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
