{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys, joblib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "LOCAL_ROOT = '/Users/nathvaru/Documents/personal/AV/janatahack_healthcare_analytics_II/'\n",
    "DATA_DIR = os.path.join(LOCAL_ROOT, 'data')\n",
    "TRAIN_FN = os.path.join(DATA_DIR, 'Train_hMYJ020/train.csv')\n",
    "TEST_FN = os.path.join(DATA_DIR, 'test.csv')\n",
    "SUBMISSION_FN = os.path.join(DATA_DIR, 'sample_submission_lfbv3c3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df_train = pd.read_csv(TRAIN_FN)\n",
    "df_test = pd.read_csv(TEST_FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars = ['Hospital_code', 'Hospital_type_code',\n",
    "            'City_Code_Hospital', 'Hospital_region_code',\n",
    "            'Department', 'Ward_Type', 'Ward_Facility_Code',\n",
    "            'Bed Grade', 'City_Code_Patient',\n",
    "            'Type of Admission', 'Severity of Illness', 'Age']\n",
    "num_vars = ['Available Extra Rooms in Hospital',\n",
    "            'Visitors with Patient', 'Admission_Deposit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values in Bed Grade and City_Code_Patient\n",
    "# with separate category\n",
    "df_train.fillna({'Bed Grade': 'missing', 'City_Code_Patient': 'missing'},\n",
    "                inplace=True)\n",
    "df_test.fillna({'Bed Grade': 'missing', 'City_Code_Patient': 'missing'},\n",
    "                inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCountVar(compute_df, count_df, var_name, count_var):\n",
    "    \"\"\"\n",
    "    compute_df : Data frame for which the count encoding should be done\n",
    "    count_df : Data frame from which the counts should be taken\n",
    "    var_name : categorical variable for count encoding\n",
    "    count_var : some other variable from the dataset (used as dummy variable to get count)\n",
    "    \"\"\"\n",
    "    grouped_df = count_df.groupby(var_name, as_index=False)[count_var].agg('count')\n",
    "    grouped_df.columns = [var_name, \"var_count\"]\n",
    "    merged_df = pd.merge(compute_df, grouped_df, how=\"left\", on=var_name)\n",
    "    merged_df.fillna(-1, inplace=True)\n",
    "    return list(merged_df[\"var_count\"])\n",
    "\n",
    "\n",
    "def getDVEncodeVar(compute_df, target_df, var_name, target_var,\n",
    "                   min_cutoff=1):\n",
    "    if type(var_name) != type([]):\n",
    "        var_name = [var_name]\n",
    "    grouped_df = target_df.groupby(var_name)[target_var].agg([\"mean\"]).reset_index()\n",
    "    grouped_df.columns = var_name + [\"mean_value\"]\n",
    "    merged_df = pd.merge(compute_df, grouped_df, how=\"left\", on=var_name)\n",
    "    merged_df.fillna(-1, inplace=True)\n",
    "    return list(merged_df[\"mean_value\"])\n",
    "\n",
    "\n",
    "def do_target_encode(train_df, test_df, cols_to_encode, target_col,\n",
    "                     encode_type, n_splits=3):\n",
    "        \n",
    "    kf = KFold(n_splits=n_splits, shuffle=True,\n",
    "                               random_state=2020)\n",
    "    for col in cols_to_encode:\n",
    "        train_enc_values = np.zeros(train_df.shape[0])\n",
    "        test_enc_values = 0\n",
    "        for dev_index, val_index in kf.split(train_df):\n",
    "            new_train_df = train_df[[col, target_col]]\n",
    "            dev_X, val_X = new_train_df.iloc[dev_index], new_train_df.iloc[val_index]\n",
    "            \n",
    "            if encode_type == 'dv':\n",
    "                train_enc_values[val_index] =  np.array( \n",
    "                    getDVEncodeVar(val_X[[col]], dev_X, col, target_col))\n",
    "                test_enc_values += np.array( \n",
    "                    getDVEncodeVar(test_df[[col]], dev_X, col, target_col))\n",
    "            elif encode_type == 'count':\n",
    "                train_enc_values[val_index] =  np.array( \n",
    "                    getCountVar(val_X[[col]], dev_X, col, target_col))\n",
    "                test_enc_values += np.array( \n",
    "                    getCountVar(test_df[[col]], dev_X, col, target_col))\n",
    "        \n",
    "        test_enc_values /= n_splits\n",
    "        train_df[col + \"_{}_enc\".format(encode_type)] = train_enc_values\n",
    "        test_df[col + \"_{}_enc\".format(encode_type)] = test_enc_values\n",
    "        \n",
    "        return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hospital_type_code\n",
      "Hospital_region_code\n",
      "Department\n",
      "Ward_Type\n",
      "Ward_Facility_Code\n",
      "Bed Grade\n",
      "City_Code_Patient\n",
      "Type of Admission\n",
      "Severity of Illness\n",
      "Age\n"
     ]
    }
   ],
   "source": [
    "# preprocess cat_vars\n",
    "for var in cat_vars:\n",
    "    if df_train[var].dtypes == object:\n",
    "        print(var)\n",
    "        df_train[var] = df_train[var].apply(\n",
    "            lambda x: str(x).strip().replace(\" \", \"-\").replace(\".\", \"\"))\n",
    "        df_test[var] = df_test[var].apply(\n",
    "            lambda x: str(x).strip().replace(\" \", \"-\").replace(\".\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get dummies\n"
     ]
    }
   ],
   "source": [
    "df_train['sample'] = 'train'\n",
    "df_test['sample'] = 'test'\n",
    "cols = ['case_id', 'sample'] + cat_vars\n",
    "tmp = pd.concat([df_train[cols], df_test[cols]], axis=0)\n",
    "tmp.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print('get dummies')\n",
    "tmp = pd.get_dummies(tmp, prefix=cat_vars, columns=cat_vars,\n",
    "                     prefix_sep='_', drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nathvaru/.virtualenvs/py38/lib/python3.8/site-packages/pandas/core/frame.py:3990: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "mask = tmp['sample'] == 'train'\n",
    "train = tmp.loc[mask, :]\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "train.drop('sample', axis=1, inplace=True)\n",
    "df_train = pd.merge(df_train[['case_id', 'Stay']+num_vars], train,\n",
    "                    on='case_id')\n",
    "del train\n",
    "\n",
    "mask = tmp['sample'] == 'test'\n",
    "test = tmp.loc[mask, :]\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "test.drop('sample', axis=1, inplace=True)\n",
    "df_test = pd.merge(df_test[['case_id']+num_vars], test, on='case_id')\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(318438, 122) (137057, 121)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add prefix to all features\n",
    "FEAT_PREFIX = 'JHA'\n",
    "cols = list(df_test.columns)\n",
    "new_cols = [FEAT_PREFIX + '_'+ col.replace(\" \", \"-\")\n",
    "            if col not in ('case_id', 'Stay') else col for col in cols]\n",
    "rename_dct = dict(zip(cols, new_cols))\n",
    "df_train.rename(columns=rename_dct, inplace=True)\n",
    "df_test.rename(columns=rename_dct, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier treatment and scaling for num_vars\n",
    "from utility import LegacyOutlierScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "PREPROCESS = {\n",
    "    'exoutscaler': LegacyOutlierScaler(),\n",
    "    'stdscaler': StandardScaler()\n",
    "}\n",
    "STEPS = ['exoutscaler', 'stdscaler']\n",
    "\n",
    "\n",
    "def preprocess(train, test, steps, features):\n",
    "    \"\"\"\n",
    "    imputation, outlier treatment and scaling\n",
    "    \"\"\"\n",
    "    train = train.copy()\n",
    "    test = test.copy()\n",
    "    other_cols = list(set(list(test.columns)) - set(features))\n",
    "    classic_steps = steps\n",
    "    steps = list(zip(steps, map(PREPROCESS.get, steps)))\n",
    "    datapipe = Pipeline(steps=steps)\n",
    "\n",
    "    x_dev = train[features].values\n",
    "    \n",
    "    print('fit')\n",
    "    datapipe.fit(x_dev)\n",
    "    \n",
    "    print('transform dataframe using pipeline')\n",
    "    print('train data:')\n",
    "    train1 = datapipe.transform(train[features].values)\n",
    "    train1 = pd.DataFrame(train1, columns=features)\n",
    "    train1 = pd.concat([train1, train[other_cols+['Stay']]], axis=1)\n",
    "    print('test data:')\n",
    "    test1 = datapipe.transform(test[features].values)\n",
    "    test1 = pd.DataFrame(test1, columns=features)\n",
    "    test1 = pd.concat([test1, test[other_cols]], axis=1)\n",
    "    \n",
    "    # Create \"classic\" datapipe and store list of features\n",
    "    classic_pipe = Pipeline([(name, datapipe.named_steps[name])\n",
    "                             for name in classic_steps])\n",
    "    classic_pipe.feature_names = features\n",
    "\n",
    "    return train1, test1, classic_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit\n",
      "transform dataframe using pipeline\n",
      "train data:\n",
      "test data:\n"
     ]
    }
   ],
   "source": [
    "num_vars = [FEAT_PREFIX + '_'+ col.replace(\" \", \"-\") for col in num_vars]\n",
    "df_train_pre, df_test_pre, pipeline = preprocess(\n",
    "    df_train, df_test, STEPS, num_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode target\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(df_train_pre['Stay'].values)\n",
    "\n",
    "df_train_pre['DV'] = le.transform(df_train_pre['Stay'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelling\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "import operator\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "\n",
    "\n",
    "def create_feature_map(features):\n",
    "    outfile = open('../model/xgb.fmap', 'w')\n",
    "    for i, feat in enumerate(features):\n",
    "        outfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n",
    "    outfile.close()\n",
    "\n",
    "\n",
    "def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None,\n",
    "           feature_names=None, seed_val=0, rounds=500, dep=8, eta=0.05):\n",
    "    params = {}\n",
    "    params[\"objective\"] = \"multi:softmax\"\n",
    "    params[\"num_class\"] = 11\n",
    "    params['eval_metric'] = \"merror\"\n",
    "    params[\"eta\"] = eta\n",
    "    params[\"subsample\"] = 0.7\n",
    "    params[\"min_child_weight\"] = 1\n",
    "    params[\"colsample_bytree\"] = 0.7\n",
    "    params[\"max_depth\"] = dep\n",
    "\n",
    "    params[\"silent\"] = 1\n",
    "    params[\"seed\"] = seed_val\n",
    "    # params[\"max_delta_step\"] = 2\n",
    "    # params[\"gamma\"] = 0.5\n",
    "    num_rounds = rounds\n",
    "\n",
    "    plst = list(params.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [(xgtrain, 'train'), (xgtest, 'test')]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist,\n",
    "                          early_stopping_rounds=100, verbose_eval=20)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    if feature_names is not None:\n",
    "        create_feature_map(feature_names)\n",
    "        model.dump_model('../model/xgbmodel.txt', '../model/xgb.fmap',\n",
    "                         with_stats=True)\n",
    "        importance = model.get_fscore(fmap='../model/xgb.fmap')\n",
    "        importance = sorted(importance.items(), key=operator.itemgetter(1),\n",
    "                            reverse=True)\n",
    "        imp_df = pd.DataFrame(importance, columns=['feature', 'fscore'])\n",
    "        imp_df['fscore'] = imp_df['fscore'] / imp_df['fscore'].sum()\n",
    "        imp_df.to_csv(\"imp_feat.txt\", index=False)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest,\n",
    "                                ntree_limit=model.best_ntree_limit)\n",
    "    if test_X2 is not None:\n",
    "        pred_test_y2 = model.predict(xgb.DMatrix(test_X2),\n",
    "                                     ntree_limit=model.best_ntree_limit)\n",
    "    else:\n",
    "        pred_test_y2 = None\n",
    "\n",
    "    loss = 0\n",
    "    if test_y is not None:\n",
    "        loss = metrics.accuracy_score(test_y, pred_test_y)\n",
    "\n",
    "    return pred_test_y, loss, pred_test_y2\n",
    "\n",
    "\n",
    "def runLGB(train_X, train_y, test_X, test_y=None, test_X2=None,\n",
    "           feature_names=None, seed_val=0, rounds=500, dep=8, eta=0.05):\n",
    "    params = {}\n",
    "    params[\"objective\"] = \"multiclass\"\n",
    "    params[\"num_class\"] = 11\n",
    "    params['metric'] = \"multi_error\"\n",
    "    params['seed'] = seed_val\n",
    "    params[\"max_depth\"] = dep\n",
    "    params[\"num_leaves\"] = 70\n",
    "    params[\"min_data_in_leaf\"] = 20\n",
    "    params[\"learning_rate\"] = eta\n",
    "    params[\"bagging_fraction\"] = 0.7\n",
    "    params[\"feature_fraction\"] = 0.7\n",
    "    params[\"bagging_freq\"] = 5\n",
    "    params[\"bagging_seed\"] = seed_val\n",
    "    params[\"verbosity\"] = 0\n",
    "    num_rounds = rounds\n",
    "\n",
    "    lgtrain = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        lgtest = lgb.Dataset(test_X, label=test_y)\n",
    "        model = lgb.train(params, lgtrain, num_rounds, valid_sets=[lgtest],\n",
    "                          early_stopping_rounds=100, verbose_eval=20)\n",
    "    else:\n",
    "        lgtest = lgb.DMatrix(test_X)\n",
    "        model = lgb.train(params, lgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(test_X,\n",
    "                                num_iteration=model.best_iteration)\n",
    "    pred_test_y = pred_test_y.argmax(axis=1)\n",
    "    \n",
    "    if test_X2 is not None:\n",
    "        pred_test_y2 = model.predict(test_X2,\n",
    "                                     num_iteration=model.best_iteration)\n",
    "        pred_test_y2 = pred_test_y2.argmax(axis=1)\n",
    "    else:\n",
    "        pred_test_y2 = None\n",
    "        \n",
    "    loss = 0\n",
    "    if test_y is not None:\n",
    "        loss = metrics.accuracy_score(test_y, pred_test_y)\n",
    "\n",
    "    return pred_test_y, loss, pred_test_y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building\n",
    "\n",
    "def trainModel(train_X, train_y, test_X, n_splits, model_name, feats, \n",
    "               **params):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=2020)\n",
    "    cv_scores = []\n",
    "    pred_test_full = []\n",
    "    pred_val_full = np.zeros(train_X.shape[0])\n",
    "    for dev_index, val_index in kf.split(train_X):\n",
    "        dev_X, val_X = train_X.iloc[dev_index, :], train_X.iloc[val_index, :]\n",
    "        dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "\n",
    "        if model_name == \"XGB\":\n",
    "            pred_val, acc, pred_test = runXGB(\n",
    "             dev_X, dev_y, val_X, val_y, test_X, rounds=params['rounds'],\n",
    "             dep=params['depth'], eta=params['eta'], feature_names=feats)\n",
    "        elif model_name == \"LGB\":\n",
    "            pred_val, acc, pred_test = runLGB(\n",
    "             dev_X, dev_y, val_X, val_y, test_X, rounds=params['rounds'],\n",
    "             dep=params['depth'], eta=params['eta'])\n",
    "        \n",
    "        cv_scores.append(acc)\n",
    "        pred_val_full[val_index] = pred_val\n",
    "        if pred_test is not None:\n",
    "            pred_test_full.append(pred_test)\n",
    "\n",
    "    #pred_test_full = pred_test_full/n_splits\n",
    "    acc = metrics.accuracy_score(train_y, pred_val_full)\n",
    "    return pred_val_full, acc, pred_test_full, cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cols = [x for x in list(df_train_pre.columns)\n",
    "             if x.startswith(FEAT_PREFIX)]\n",
    "x_train = df_train_pre[feat_cols]\n",
    "y_train = df_train_pre['DV']\n",
    "x_test = df_test_pre[feat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:13:12] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:516: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-merror:0.61706\ttest-merror:0.61803\n",
      "Multiple eval metrics have been passed: 'test-merror' will be used for early stopping.\n",
      "\n",
      "Will train until test-merror hasn't improved in 100 rounds.\n",
      "[20]\ttrain-merror:0.59233\ttest-merror:0.59528\n",
      "[40]\ttrain-merror:0.58279\ttest-merror:0.58754\n",
      "[60]\ttrain-merror:0.57837\ttest-merror:0.58446\n",
      "[80]\ttrain-merror:0.57450\ttest-merror:0.58212\n",
      "[100]\ttrain-merror:0.57157\ttest-merror:0.58045\n",
      "[120]\ttrain-merror:0.56923\ttest-merror:0.57933\n",
      "[140]\ttrain-merror:0.56680\ttest-merror:0.57813\n",
      "[160]\ttrain-merror:0.56462\ttest-merror:0.57743\n",
      "[180]\ttrain-merror:0.56275\ttest-merror:0.57700\n",
      "[200]\ttrain-merror:0.56097\ttest-merror:0.57650\n",
      "[220]\ttrain-merror:0.55915\ttest-merror:0.57630\n",
      "[240]\ttrain-merror:0.55729\ttest-merror:0.57607\n",
      "[260]\ttrain-merror:0.55556\ttest-merror:0.57573\n",
      "[280]\ttrain-merror:0.55397\ttest-merror:0.57544\n",
      "[300]\ttrain-merror:0.55212\ttest-merror:0.57486\n",
      "[320]\ttrain-merror:0.55045\ttest-merror:0.57490\n",
      "[340]\ttrain-merror:0.54882\ttest-merror:0.57487\n",
      "[360]\ttrain-merror:0.54713\ttest-merror:0.57456\n",
      "[380]\ttrain-merror:0.54573\ttest-merror:0.57428\n",
      "[400]\ttrain-merror:0.54410\ttest-merror:0.57408\n",
      "[420]\ttrain-merror:0.54273\ttest-merror:0.57372\n",
      "[440]\ttrain-merror:0.54135\ttest-merror:0.57347\n",
      "[460]\ttrain-merror:0.54010\ttest-merror:0.57331\n",
      "[480]\ttrain-merror:0.53846\ttest-merror:0.57323\n",
      "[500]\ttrain-merror:0.53734\ttest-merror:0.57340\n",
      "[520]\ttrain-merror:0.53630\ttest-merror:0.57326\n",
      "[540]\ttrain-merror:0.53469\ttest-merror:0.57359\n",
      "[560]\ttrain-merror:0.53323\ttest-merror:0.57334\n",
      "[580]\ttrain-merror:0.53193\ttest-merror:0.57345\n",
      "[599]\ttrain-merror:0.53075\ttest-merror:0.57313\n",
      "[18:45:17] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:516: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-merror:0.61686\ttest-merror:0.61772\n",
      "Multiple eval metrics have been passed: 'test-merror' will be used for early stopping.\n",
      "\n",
      "Will train until test-merror hasn't improved in 100 rounds.\n",
      "[20]\ttrain-merror:0.59049\ttest-merror:0.59397\n",
      "[40]\ttrain-merror:0.58212\ttest-merror:0.58827\n",
      "[60]\ttrain-merror:0.57782\ttest-merror:0.58547\n",
      "[80]\ttrain-merror:0.57475\ttest-merror:0.58389\n",
      "[100]\ttrain-merror:0.57108\ttest-merror:0.58268\n",
      "[120]\ttrain-merror:0.56846\ttest-merror:0.58253\n",
      "[140]\ttrain-merror:0.56623\ttest-merror:0.58094\n",
      "[160]\ttrain-merror:0.56406\ttest-merror:0.58026\n",
      "[180]\ttrain-merror:0.56230\ttest-merror:0.58010\n",
      "[200]\ttrain-merror:0.55992\ttest-merror:0.57930\n",
      "[220]\ttrain-merror:0.55800\ttest-merror:0.57866\n",
      "[240]\ttrain-merror:0.55620\ttest-merror:0.57824\n",
      "[260]\ttrain-merror:0.55421\ttest-merror:0.57795\n",
      "[280]\ttrain-merror:0.55203\ttest-merror:0.57781\n",
      "[300]\ttrain-merror:0.55053\ttest-merror:0.57773\n",
      "[320]\ttrain-merror:0.54896\ttest-merror:0.57773\n",
      "[340]\ttrain-merror:0.54765\ttest-merror:0.57717\n",
      "[360]\ttrain-merror:0.54617\ttest-merror:0.57714\n",
      "[380]\ttrain-merror:0.54481\ttest-merror:0.57710\n",
      "[400]\ttrain-merror:0.54309\ttest-merror:0.57685\n",
      "[420]\ttrain-merror:0.54204\ttest-merror:0.57700\n",
      "[440]\ttrain-merror:0.54067\ttest-merror:0.57673\n",
      "[460]\ttrain-merror:0.53915\ttest-merror:0.57670\n",
      "[480]\ttrain-merror:0.53755\ttest-merror:0.57666\n",
      "[500]\ttrain-merror:0.53629\ttest-merror:0.57585\n",
      "[520]\ttrain-merror:0.53506\ttest-merror:0.57572\n",
      "[540]\ttrain-merror:0.53338\ttest-merror:0.57557\n",
      "[560]\ttrain-merror:0.53212\ttest-merror:0.57511\n",
      "[580]\ttrain-merror:0.53060\ttest-merror:0.57537\n",
      "[599]\ttrain-merror:0.52933\ttest-merror:0.57540\n",
      "[19:23:20] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:516: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-merror:0.61265\ttest-merror:0.61754\n",
      "Multiple eval metrics have been passed: 'test-merror' will be used for early stopping.\n",
      "\n",
      "Will train until test-merror hasn't improved in 100 rounds.\n",
      "[20]\ttrain-merror:0.59132\ttest-merror:0.59623\n",
      "[40]\ttrain-merror:0.58185\ttest-merror:0.58850\n",
      "[60]\ttrain-merror:0.57757\ttest-merror:0.58609\n",
      "[80]\ttrain-merror:0.57389\ttest-merror:0.58403\n",
      "[100]\ttrain-merror:0.57106\ttest-merror:0.58242\n",
      "[120]\ttrain-merror:0.56840\ttest-merror:0.58138\n",
      "[140]\ttrain-merror:0.56563\ttest-merror:0.58090\n",
      "[160]\ttrain-merror:0.56372\ttest-merror:0.58011\n",
      "[180]\ttrain-merror:0.56166\ttest-merror:0.57943\n",
      "[200]\ttrain-merror:0.55968\ttest-merror:0.57949\n",
      "[220]\ttrain-merror:0.55770\ttest-merror:0.57871\n",
      "[240]\ttrain-merror:0.55565\ttest-merror:0.57843\n",
      "[260]\ttrain-merror:0.55436\ttest-merror:0.57838\n",
      "[280]\ttrain-merror:0.55265\ttest-merror:0.57808\n",
      "[300]\ttrain-merror:0.55092\ttest-merror:0.57797\n",
      "[320]\ttrain-merror:0.54909\ttest-merror:0.57754\n",
      "[340]\ttrain-merror:0.54741\ttest-merror:0.57737\n",
      "[360]\ttrain-merror:0.54592\ttest-merror:0.57709\n",
      "[380]\ttrain-merror:0.54454\ttest-merror:0.57667\n",
      "[400]\ttrain-merror:0.54271\ttest-merror:0.57667\n",
      "[420]\ttrain-merror:0.54142\ttest-merror:0.57654\n",
      "[440]\ttrain-merror:0.53967\ttest-merror:0.57644\n",
      "[460]\ttrain-merror:0.53820\ttest-merror:0.57667\n",
      "[480]\ttrain-merror:0.53669\ttest-merror:0.57631\n",
      "[500]\ttrain-merror:0.53551\ttest-merror:0.57635\n",
      "[520]\ttrain-merror:0.53406\ttest-merror:0.57596\n",
      "[540]\ttrain-merror:0.53255\ttest-merror:0.57633\n",
      "[560]\ttrain-merror:0.53123\ttest-merror:0.57633\n",
      "[580]\ttrain-merror:0.52996\ttest-merror:0.57627\n",
      "[599]\ttrain-merror:0.52887\ttest-merror:0.57638\n",
      "CPU times: user 12h 35min 27s, sys: 3min 32s, total: 12h 38min 59s\n",
      "Wall time: 1h 44min 27s\n"
     ]
    }
   ],
   "source": [
    "# XGB\n",
    "params = {'rounds': 600, 'depth': 6, 'eta': 0.05}\n",
    "%time pred_val_full, acc, pred_test_full, cv_scores = trainModel(x_train, y_train, x_test, 3, \"XGB\", feat_cols, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy:  0.4252884391938148\n"
     ]
    }
   ],
   "source": [
    "print('CV accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max voting with preference for the first model when predictions are equally frequent\n",
    "from scipy.stats import mode\n",
    "\n",
    "pred_test_full1 = np.array(pred_test_full).T\n",
    "pred_test_full1 = mode(pred_test_full1, 1)[0]\n",
    "pred_test_full1 = pred_test_full1.reshape((pred_test_full1.shape[0], ))\n",
    "print(pred_test_full1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform pred_test_full into original labels\n",
    "out_df = pd.DataFrame({\"case_id\": df_test_pre[\"case_id\"].values})\n",
    "out_df[\"Stay\"] = le.inverse_transform(pred_test_full1)\n",
    "out_df.to_csv(\"../model/Mbaseline_out.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[20]\tvalid_0's multi_error: 0.592787\n",
      "[40]\tvalid_0's multi_error: 0.581586\n",
      "[60]\tvalid_0's multi_error: 0.578496\n",
      "[80]\tvalid_0's multi_error: 0.577799\n",
      "[100]\tvalid_0's multi_error: 0.576659\n",
      "[120]\tvalid_0's multi_error: 0.575782\n",
      "[140]\tvalid_0's multi_error: 0.575358\n",
      "[160]\tvalid_0's multi_error: 0.575123\n",
      "[180]\tvalid_0's multi_error: 0.575189\n",
      "[200]\tvalid_0's multi_error: 0.574803\n",
      "[220]\tvalid_0's multi_error: 0.57484\n",
      "[240]\tvalid_0's multi_error: 0.574511\n",
      "[260]\tvalid_0's multi_error: 0.574708\n",
      "[280]\tvalid_0's multi_error: 0.574341\n",
      "[300]\tvalid_0's multi_error: 0.574501\n",
      "[320]\tvalid_0's multi_error: 0.574011\n",
      "[340]\tvalid_0's multi_error: 0.573992\n",
      "[360]\tvalid_0's multi_error: 0.57387\n",
      "[380]\tvalid_0's multi_error: 0.57371\n",
      "[400]\tvalid_0's multi_error: 0.573531\n",
      "[420]\tvalid_0's multi_error: 0.573324\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[440]\tvalid_0's multi_error: 0.573418\n",
      "[460]\tvalid_0's multi_error: 0.573097\n",
      "[480]\tvalid_0's multi_error: 0.57371\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[500]\tvalid_0's multi_error: 0.574058\n",
      "[520]\tvalid_0's multi_error: 0.574049\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[540]\tvalid_0's multi_error: 0.573926\n",
      "[560]\tvalid_0's multi_error: 0.574096\n",
      "Early stopping, best iteration is:\n",
      "[460]\tvalid_0's multi_error: 0.573097\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040180 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[20]\tvalid_0's multi_error: 0.59323\n",
      "[40]\tvalid_0's multi_error: 0.584563\n",
      "[60]\tvalid_0's multi_error: 0.581313\n",
      "[80]\tvalid_0's multi_error: 0.580295\n",
      "[100]\tvalid_0's multi_error: 0.579099\n",
      "[120]\tvalid_0's multi_error: 0.578072\n",
      "[140]\tvalid_0's multi_error: 0.577554\n",
      "[160]\tvalid_0's multi_error: 0.577384\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[180]\tvalid_0's multi_error: 0.576885\n",
      "[200]\tvalid_0's multi_error: 0.576451\n",
      "[220]\tvalid_0's multi_error: 0.576235\n",
      "[240]\tvalid_0's multi_error: 0.576084\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[260]\tvalid_0's multi_error: 0.575641\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[280]\tvalid_0's multi_error: 0.575406\n",
      "[300]\tvalid_0's multi_error: 0.575585\n",
      "[320]\tvalid_0's multi_error: 0.575679\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[340]\tvalid_0's multi_error: 0.575537\n",
      "[360]\tvalid_0's multi_error: 0.575547\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[380]\tvalid_0's multi_error: 0.57534\n",
      "[400]\tvalid_0's multi_error: 0.575368\n",
      "[420]\tvalid_0's multi_error: 0.575227\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[440]\tvalid_0's multi_error: 0.575396\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[460]\tvalid_0's multi_error: 0.575519\n",
      "Early stopping, best iteration is:\n",
      "[376]\tvalid_0's multi_error: 0.575104\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017397 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[20]\tvalid_0's multi_error: 0.593899\n",
      "[40]\tvalid_0's multi_error: 0.585043\n",
      "[60]\tvalid_0's multi_error: 0.583564\n",
      "[80]\tvalid_0's multi_error: 0.581793\n",
      "[100]\tvalid_0's multi_error: 0.580191\n",
      "[120]\tvalid_0's multi_error: 0.579494\n",
      "[140]\tvalid_0's multi_error: 0.578251\n",
      "[160]\tvalid_0's multi_error: 0.577742\n",
      "[180]\tvalid_0's multi_error: 0.577412\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[200]\tvalid_0's multi_error: 0.57728\n",
      "[220]\tvalid_0's multi_error: 0.576998\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[240]\tvalid_0's multi_error: 0.576649\n",
      "[260]\tvalid_0's multi_error: 0.576809\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[280]\tvalid_0's multi_error: 0.576659\n",
      "[300]\tvalid_0's multi_error: 0.576197\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[320]\tvalid_0's multi_error: 0.576348\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[340]\tvalid_0's multi_error: 0.575933\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[360]\tvalid_0's multi_error: 0.575848\n",
      "[380]\tvalid_0's multi_error: 0.576037\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[400]\tvalid_0's multi_error: 0.575858\n",
      "[420]\tvalid_0's multi_error: 0.576065\n",
      "[440]\tvalid_0's multi_error: 0.57631\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[460]\tvalid_0's multi_error: 0.576385\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[480]\tvalid_0's multi_error: 0.576338\n",
      "Early stopping, best iteration is:\n",
      "[384]\tvalid_0's multi_error: 0.575622\n",
      "CPU times: user 15min 55s, sys: 2.98 s, total: 15min 58s\n",
      "Wall time: 15min 59s\n"
     ]
    }
   ],
   "source": [
    "# LGB\n",
    "params = {'rounds': 600, 'depth': 7, 'eta': 0.05}\n",
    "%time pred_val_full, acc, pred_test_full, cv_scores = trainModel(x_train, y_train, x_test, 3, \"LGB\", feat_cols, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.425392070041892,\n",
       " [0.42690256816083505, 0.42489589810261336, 0.4243777438622275])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 5 2 ... 2 1 3]\n"
     ]
    }
   ],
   "source": [
    "# max voting with preference for the first model when predictions are equally frequent\n",
    "from scipy.stats import mode\n",
    "\n",
    "pred_test_full1 = np.array(pred_test_full).T\n",
    "pred_test_full1 = mode(pred_test_full1, 1)[0]\n",
    "pred_test_full1 = pred_test_full1.reshape((pred_test_full1.shape[0], ))\n",
    "print(pred_test_full1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform pred_test_full into original labels\n",
    "out_df = pd.DataFrame({\"case_id\": df_test_pre[\"case_id\"].values})\n",
    "out_df[\"Stay\"] = le.inverse_transform(pred_test_full1)\n",
    "out_df.to_csv(\"../model/Mbaseline_LGB_out.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
