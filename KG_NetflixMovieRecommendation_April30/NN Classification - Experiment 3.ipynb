{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "import copy\n",
    "import pickle\n",
    "import zipfile\n",
    "from textwrap import wrap\n",
    "from pathlib import Path\n",
    "from itertools import zip_longest\n",
    "from collections import defaultdict\n",
    "from urllib.error import URLError\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import tensor\n",
    "from torch.nn import functional as F \n",
    "from torch.optim.lr_scheduler import _LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"src/\")\n",
    "from constants import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4 - NN Classification with baseline features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import IterableDataset\n",
    "from itertools import chain, islice\n",
    "\n",
    "\n",
    "class InteractionsStream(IterableDataset):\n",
    "\n",
    "    def __init__(self, prep_data_dir=PREPARED_DATA_DIR, file_num=None,\n",
    "                 sample='train', user_col='User', item_col='Movie',\n",
    "                 dv_col='Rating',\n",
    "                 end_token='.h5', start_token='user_{}_data_',\n",
    "                 baseline_feats=False, model_type='regression',\n",
    "                 chunksize=10):\n",
    "\n",
    "        if file_num is None:\n",
    "            self.files = [os.path.join(prep_data_dir, x) for x in\n",
    "                          _find_files(prep_data_dir,\n",
    "                                      start_token.format(sample),\n",
    "                                      end_token)]\n",
    "        else:\n",
    "            self.files = [\n",
    "                os.path.join(prep_data_dir,\n",
    "                             start_token.format(sample)+str(file_num)+\n",
    "                             end_token)]\n",
    "        print(self.files)\n",
    "        self.user_col = user_col\n",
    "        self.item_col = item_col\n",
    "        self.baseline_feats = baseline_feats\n",
    "        self.sample = sample\n",
    "        self.chunksize = chunksize\n",
    "        self.model_type = model_type\n",
    "        self.dv_col = dv_col\n",
    "        self.cat_cols = [self.user_col, self.item_col]\n",
    "        \n",
    "        if baseline_feats:\n",
    "            self.numeric_cols = [\n",
    "                'days_since_first_user_rating',\n",
    "                'sqrt_days_since_first_user_rating',\n",
    "                'rating_age_days_user', 'rating_age_weeks_user',\n",
    "                'rating_age_months_user', 'mean_ratings_user',\n",
    "                'num_ratings_user', 'days_since_first_item_rating',\n",
    "                'sqrt_days_since_first_item_rating',\n",
    "                'rating_age_days_item', 'rating_age_weeks_item',\n",
    "                'rating_age_months_item', 'mean_ratings_movie',\n",
    "                'weighted_mean_ratings_movie', 'num_ratings_movie']\n",
    "        else:\n",
    "            self.numeric_cols = []            \n",
    "\n",
    "    def read_file(self, fn):\n",
    "        \n",
    "        if self.sample == 'train':\n",
    "            df = pd.read_hdf(fn, key='stage', iterator=True,\n",
    "                             chunksize=self.chunksize)\n",
    "        else:\n",
    "            df = pd.read_hdf(fn, key='stage')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_dv_for_classification(self, dv_lst):\n",
    "        \n",
    "        if self.model_type == 'classification':\n",
    "            return [int(x-1) for x in dv_lst]\n",
    "        else:\n",
    "            return [int(x) for x in dv_lst]\n",
    "\n",
    "    def process_data(self, fn):\n",
    "\n",
    "        print('read data')\n",
    "        data = self.read_file(fn)\n",
    "\n",
    "        print('create an iterable')\n",
    "        if self.sample == 'train':\n",
    "            if self.baseline_feats:\n",
    "                for row in data:\n",
    "                    x1 = row[self.cat_cols].values.tolist()\n",
    "                    x2 = row[self.numeric_cols].values.tolist()\n",
    "                    y = self.get_dv_for_classification(\n",
    "                            row[self.dv_col].tolist())\n",
    "                    yield (x1, x2, y)\n",
    "            else:\n",
    "                for row in data:\n",
    "                    user = row[self.user_col].tolist()\n",
    "                    item = row[self.item_col].tolist()\n",
    "                    y = self.get_dv_for_classification(\n",
    "                            row[self.dv_col].tolist())\n",
    "                    yield (user, item), y\n",
    "        else:\n",
    "            if self.baseline_feats:\n",
    "                for i, row in data.iterrows():\n",
    "                    y = int(row[self.dv_col]-1) if self.model_type == 'classification' else row[self.dv_col]\n",
    "                    yield (row[self.cat_cols].tolist(),\n",
    "                           row[self.numeric_cols].tolist(), \n",
    "                           y)\n",
    "            else:\n",
    "                for i, row in data.iterrows():\n",
    "                    y = int(row[self.dv_col]-1) if self.model_type == 'classification' else row[self.dv_col]\n",
    "                    yield (row[self.user_col],\n",
    "                           row[self.item_col]), y\n",
    "\n",
    "    def get_stream(self, files):\n",
    "        return chain.from_iterable(map(self.process_data, files))\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.get_stream(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class TabularModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines the neural network for tabular data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_sizes, n_cont, n_classes=5):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList(\n",
    "            [nn.Embedding(categories, size) for\n",
    "             categories, size in embedding_sizes])\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeddings)\n",
    "        self.n_emb, self.n_cont, self.n_classes = n_emb, n_cont, n_classes\n",
    "        self.lin1 = nn.Linear(self.n_emb + self.n_cont, 200)\n",
    "        self.lin2 = nn.Linear(200, 70)\n",
    "        self.lin3 = nn.Linear(70, self.n_classes)\n",
    "        self.bn1 = nn.BatchNorm1d(self.n_cont)\n",
    "        self.bn2 = nn.BatchNorm1d(200)\n",
    "        self.bn3 = nn.BatchNorm1d(70)\n",
    "        self.emb_drop = nn.Dropout(0.6)\n",
    "        self.drops = nn.Dropout(0.3)\n",
    "\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        x = [e(x_cat[:, i]) for i, e in enumerate(self.embeddings)]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.emb_drop(x)\n",
    "        x2 = self.bn1(x_cont)\n",
    "        x = torch.cat([x, x2], 1)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.lin3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as torch_optim\n",
    "import torch.nn.functional as F\n",
    "from torch import tensor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def choose_embedding_size(cat_cols, cat_num_values, min_emb_dim=100):\n",
    "    \"\"\"\n",
    "    cat_cols: list of categorical columns\n",
    "    cat_num_values: list of number of unique values for each categorical column\n",
    "    \"\"\"\n",
    "    embedded_cols = dict(zip(cat_cols, cat_num_values))\n",
    "    embedding_sizes = [(n_categories, min(min_emb_dim, (n_categories+1)//2))\n",
    "                       for _, n_categories in embedded_cols.items()]\n",
    "    return embedding_sizes\n",
    "\n",
    "\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "\n",
    "\n",
    "def get_optimizer(model, lr = 0.001, wd = 0.0):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optim = torch_optim.Adam(parameters, lr=lr, weight_decay=wd)\n",
    "    return optim\n",
    "\n",
    "\n",
    "def construct_tensor(a):\n",
    "    final = []\n",
    "    for i in a:\n",
    "        out = []\n",
    "        for j in i:\n",
    "            out.append(j.tolist())\n",
    "        out1 = []\n",
    "        for item in zip(*out):\n",
    "            out1.append(list(item))\n",
    "        final += out1\n",
    "    return tensor(final)\n",
    "\n",
    "\n",
    "def construct_tensor_test(a):\n",
    "    out = []\n",
    "    for i in a:\n",
    "        out.append(i.tolist())\n",
    "        out1 = []\n",
    "        for item in zip(*out):\n",
    "            out1.append(list(item))\n",
    "    return tensor(out1)\n",
    "\n",
    "\n",
    "def construct_tensor_y(a):\n",
    "    out = []\n",
    "    for i in a:\n",
    "        out += i.tolist()\n",
    "    return tensor(out)\n",
    "\n",
    "\n",
    "def train_model(model, optim, train_dl, train_size, chunksize, batch_size,\n",
    "                device, loss_fn=F.cross_entropy):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    with tqdm(total=train_size // (batch_size * chunksize)) as pbar:\n",
    "        for x1, x2, y in train_dl:\n",
    "            x1, x2, y = (construct_tensor(x1), construct_tensor(x2),\n",
    "                         construct_tensor_y(y))\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            y = y.to(device)\n",
    "            batch = y.size()[0]\n",
    "            output = model(x1, x2)\n",
    "            loss = loss_fn(output, y)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            total += batch\n",
    "            sum_loss += loss.item()\n",
    "            pbar.update(1)\n",
    "    return sum_loss/total\n",
    "\n",
    "\n",
    "def val_loss(model, valid_dl, test_size, batch_size,\n",
    "             device, loss_fn=F.cross_entropy):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    correct = 0\n",
    "    sum_auc_macro = 0\n",
    "    sum_auc_micro = 0\n",
    "    num_aucs = 0\n",
    "    with tqdm(total=test_size // (batch_size)) as pbar:\n",
    "        for x1, x2, y in valid_dl:\n",
    "            x1, x2 = construct_tensor_test(x1), construct_tensor_test(x2)\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            y = y.to(device)\n",
    "            current_batch_size = y.size()[0]\n",
    "            out = model(x1, x2)\n",
    "            loss = loss_fn(out, y)\n",
    "            sum_loss += loss.item()\n",
    "            total += current_batch_size\n",
    "            pred = torch.max(out, 1)[1]\n",
    "            pred_prob = F.softmax(out, dim=1)\n",
    "            y_onehot = F.one_hot(y)\n",
    "            correct += (pred == y).float().sum().item()\n",
    "            pred_prob = pred_prob.cpu().detach().numpy()\n",
    "            y_onehot = y_onehot.cpu().detach().numpy()\n",
    "            try:\n",
    "                sum_auc_macro += roc_auc_score(y_onehot, pred_prob,\n",
    "                                               average='macro')\n",
    "                sum_auc_micro += roc_auc_score(y_onehot, pred_prob,\n",
    "                                               average='micro')\n",
    "                num_aucs += current_batch_size\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "            pbar.update(1)\n",
    "    print(\"valid loss %.3f, accuracy %.3f, macro auc %.3f and micro auc %.3f\" % (\n",
    "        sum_loss/total, correct/total, sum_auc_macro/num_aucs, sum_auc_micro/num_aucs))\n",
    "    return sum_loss/total, correct/total, sum_auc_macro/num_aucs, sum_auc_micro/num_aucs\n",
    "\n",
    "\n",
    "def train_loop(model, train_dl, valid_dl, epochs, train_size,\n",
    "               test_size, chunksize, batch_size, device, lr=0.01,\n",
    "               wd=0.0, loss_fn=F.cross_entropy):\n",
    "    optim = get_optimizer(model, lr = lr, wd = wd)\n",
    "    start = time.time()\n",
    "    losses = []\n",
    "    for i in range(epochs):\n",
    "        stats = {'epoch': i+1}\n",
    "        train_loss = train_model(model, optim, train_dl, train_size,\n",
    "                                 chunksize, batch_size, device,\n",
    "                                 loss_fn)\n",
    "        print(\"training loss: \", train_loss)\n",
    "        stats['train_loss'] = train_loss\n",
    "        loss, acc, auc_macro, auc_micro = val_loss(\n",
    "            model, valid_dl, test_size, batch_size, device, loss_fn)\n",
    "        print('time taken: %0.2f' % (time.time() - start))\n",
    "        stats['test_loss'] = loss\n",
    "        stats['test_acc'] = acc\n",
    "        stats['test_auc_macro'] = auc_macro\n",
    "        stats['test_auc_micro'] = auc_micro\n",
    "        losses.append(stats)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "FILE_NUM = 1\n",
    "N_USERS = 480189\n",
    "N_ITEMS = 17770\n",
    "BATCH_SIZE = 50\n",
    "CHUNKSIZE = 100\n",
    "TRAIN_SIZE = 22851074 # corresponds to FILE_NUM\n",
    "VAL_SIZE = 962152     # corresponds to FILE_NUM\n",
    "TEST_SIZE = 240538    # corresponds to FILE_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose embedding size\n",
    "\n",
    "cat_cols = ['User', 'Movie']\n",
    "cat_num_values = [N_USERS, N_ITEMS]\n",
    "embedding_sizes = choose_embedding_size(cat_cols, cat_num_values, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(480189, 100), (17770, 100)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/data/kaggle/netflix-prize-data/prepared_data_for_NN_modelling/user_train_data_1.h5']\n",
      "['/data/kaggle/netflix-prize-data/prepared_data_for_NN_modelling/user_test_data_1.h5']\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = InteractionsStream(\n",
    "    file_num=FILE_NUM, baseline_feats=True, model_type='classification',\n",
    "    sample='train', chunksize=CHUNKSIZE)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                          shuffle=False)\n",
    "\n",
    "test_dataset = InteractionsStream(file_num=FILE_NUM, baseline_feats=True,\n",
    "                                  model_type='classification',\n",
    "                                  sample='test')\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of numeric vars:  15\n"
     ]
    }
   ],
   "source": [
    "n_cont = len(train_loader.dataset.numeric_cols)\n",
    "print('number of numeric vars: ', n_cont)\n",
    "\n",
    "net = TabularModel(embedding_sizes, n_cont, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabularModel(\n",
       "  (embeddings): ModuleList(\n",
       "    (0): Embedding(480189, 100)\n",
       "    (1): Embedding(17770, 100)\n",
       "  )\n",
       "  (lin1): Linear(in_features=215, out_features=200, bias=True)\n",
       "  (lin2): Linear(in_features=200, out_features=70, bias=True)\n",
       "  (lin3): Linear(in_features=70, out_features=5, bias=True)\n",
       "  (bn1): BatchNorm1d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn3): BatchNorm1d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (emb_drop): Dropout(p=0.6, inplace=False)\n",
       "  (drops): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_device(net, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4570 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n",
      "create an iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4571it [30:18,  2.51it/s]                          \n",
      "  0%|          | 0/4810 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  0.0002537636473082755\n",
      "read data\n",
      "create an iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 4352/4810 [06:53<00:43, 10.53it/s]\n",
      "  0%|          | 0/4570 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss 0.025, accuracy 0.441, macro auc 0.014 and micro auc 0.016\n",
      "time taken: 2231.54\n",
      "read data\n",
      "create an iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4571it [30:43,  2.48it/s]                          \n",
      "  0%|          | 0/4810 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  0.000253764094867131\n",
      "read data\n",
      "create an iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 4352/4810 [06:53<00:43, 10.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss 0.025, accuracy 0.440, macro auc 0.014 and micro auc 0.016\n",
      "time taken: 4488.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "losses = train_loop(model=net, train_dl=train_loader,\n",
    "                    valid_dl=test_loader, epochs=2,\n",
    "                    train_size=TRAIN_SIZE, test_size=TEST_SIZE,\n",
    "                    chunksize=CHUNKSIZE, batch_size=BATCH_SIZE,\n",
    "                    device=device, lr=0.05, wd=0.00001,\n",
    "                    loss_fn=F.cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1,\n",
       "  'train_loss': 0.0002537636473082755,\n",
       "  'test_loss': 0.025352873244562046,\n",
       "  'test_acc': 0.44095319658432347,\n",
       "  'test_auc_macro': 0.014386494047214069,\n",
       "  'test_auc_micro': 0.01558661101017521},\n",
       " {'epoch': 2,\n",
       "  'train_loss': 0.000253764094867131,\n",
       "  'test_loss': 0.025394514972358027,\n",
       "  'test_acc': 0.4396062160656528,\n",
       "  'test_auc_macro': 0.014356277362117713,\n",
       "  'test_auc_micro': 0.015562970227170028}]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabularModel(\n",
       "  (embeddings): ModuleList(\n",
       "    (0): Embedding(480189, 100)\n",
       "    (1): Embedding(17770, 100)\n",
       "  )\n",
       "  (lin1): Linear(in_features=215, out_features=200, bias=True)\n",
       "  (lin2): Linear(in_features=200, out_features=70, bias=True)\n",
       "  (lin3): Linear(in_features=70, out_features=5, bias=True)\n",
       "  (bn1): BatchNorm1d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn3): BatchNorm1d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (emb_drop): Dropout(p=0.6, inplace=False)\n",
       "  (drops): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n",
      "create an iterable\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "pred_probs = []\n",
    "actuals = []\n",
    "actuals_onehot = []\n",
    "with torch.no_grad():\n",
    "    for x1, x2, y in test_loader:\n",
    "        x1, x2 = construct_tensor_test(x1), construct_tensor_test(x2)\n",
    "        x1 = x1.to(device)\n",
    "        x2 = x2.to(device)\n",
    "        y = y.to(device)\n",
    "        out = net(x1, x2)\n",
    "        pred = torch.max(out, 1)[1]\n",
    "        pred_prob = F.softmax(out, dim=1)\n",
    "        y_onehot = F.one_hot(y)\n",
    "        preds.append(pred)\n",
    "        pred_probs.append(pred_prob)\n",
    "        actuals.append(y)\n",
    "        actuals_onehot.append(y_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4811, 4811, 4811, 4811)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds), len(pred_probs), len(actuals), len(actuals_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = [item for sublist in preds for\n",
    "               item in sublist.cpu().detach().tolist()]\n",
    "final_pred_probs = [item for sublist in pred_probs for\n",
    "                    item in sublist.cpu().detach().numpy()]\n",
    "final_actuals = [item for sublist in actuals for\n",
    "                 item in sublist.cpu().detach().tolist()]\n",
    "final_actuals_onehot = [item for sublist in actuals_onehotonehot for\n",
    "                        item in sublist.cpu().detach().numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240538, 240538, 240538, 240538)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_preds), len(final_pred_probs), len(final_actuals), len(final_actuals_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred_probs = np.array(final_pred_probs)\n",
    "final_actuals_onehot = np.array(final_actuals_onehot)\n",
    "final_actuals = np.array(final_actuals)\n",
    "final_preds = np.array(final_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((240538, 5),\n",
       " (240538, 5),\n",
       " array([0.03358189, 0.10595833, 0.48191768, 0.32772338, 0.0508187 ],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred_probs.shape, final_actuals_onehot.shape, final_pred_probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.440\n",
      "Test Macro AUC: 0.718\n",
      "Test Micro AUC: 0.781\n",
      "Test RMSE: 1.038\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "\n",
    "acc = (final_actuals == final_preds).mean()\n",
    "auc_macro = roc_auc_score(y_true=final_actuals_onehot,\n",
    "                          y_score=final_pred_probs, average='macro')\n",
    "auc_micro = roc_auc_score(y_true=final_actuals_onehot,\n",
    "                          y_score=final_pred_probs, average='micro')\n",
    "rmse = np.sqrt(mean_squared_error(y_true=final_actuals,\n",
    "                                  y_pred=final_preds))\n",
    "\n",
    "print('Test Accuracy: %0.3f' % (acc))\n",
    "print('Test Macro AUC: %0.3f' % (auc_macro))\n",
    "print('Test Micro AUC: %0.3f' % (auc_micro))\n",
    "print('Test RMSE: %0.3f' % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.virtualenvs/py36/lib/python3.6/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type TabularModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "model_fn = os.path.join(MODEL_DIR, 'model_NN_DeepMF_withBaseline_classification_{}_E2.pt'.format(\n",
    "    FILE_NUM))\n",
    "torch.save(net.state_dict, model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
