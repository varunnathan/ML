{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "import copy\n",
    "import pickle\n",
    "import zipfile\n",
    "from textwrap import wrap\n",
    "from pathlib import Path\n",
    "from itertools import zip_longest\n",
    "from collections import defaultdict\n",
    "from urllib.error import URLError\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import tensor\n",
    "from torch.nn import functional as F \n",
    "from torch.optim.lr_scheduler import _LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"src/\")\n",
    "from constants import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize baseline features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22851074, 20)\n",
      "     User  Rating       Date  Movie  Rating_class  \\\n",
      "0  161459     4.0 2004-07-17   2138             0   \n",
      "1   87375     2.0 2004-03-14   3253             0   \n",
      "2  191296     2.0 2005-12-23   1154             0   \n",
      "3   27266     5.0 2004-09-26   1201             1   \n",
      "4  175666     3.0 2004-08-03   4377             0   \n",
      "\n",
      "   days_since_first_user_rating  sqrt_days_since_first_user_rating  \\\n",
      "0                            23                           4.795832   \n",
      "1                            13                           3.605551   \n",
      "2                           453                          21.283797   \n",
      "3                            15                           3.872983   \n",
      "4                           446                          21.118712   \n",
      "\n",
      "   rating_age_days_user  rating_age_weeks_user  rating_age_months_user  \\\n",
      "0                   251              35.857143                8.366667   \n",
      "1                   617              88.142857               20.566667   \n",
      "2                   455              65.000000               15.166667   \n",
      "3                   429              61.285714               14.300000   \n",
      "4                   835             119.285714               27.833333   \n",
      "\n",
      "   mean_ratings_user  num_ratings_user  days_since_first_item_rating  \\\n",
      "0           3.396365                28                          1611   \n",
      "1           4.333700               163                           395   \n",
      "2           3.955031               108                           507   \n",
      "3           3.757806               124                          1754   \n",
      "4           3.280928                51                           565   \n",
      "\n",
      "   sqrt_days_since_first_item_rating  rating_age_days_item  \\\n",
      "0                          40.137264                  2143   \n",
      "1                          19.874607                  1052   \n",
      "2                          22.516660                   514   \n",
      "3                          41.880783                  2215   \n",
      "4                          23.769729                  1080   \n",
      "\n",
      "   rating_age_weeks_item  rating_age_months_item  mean_ratings_movie  \\\n",
      "0             306.142857               71.433333            3.526814   \n",
      "1             150.285714               35.066667            2.977046   \n",
      "2              73.428571               17.133333            3.818879   \n",
      "3             316.428571               73.833333            3.771652   \n",
      "4             154.285714               36.000000            3.488060   \n",
      "\n",
      "   weighted_mean_ratings_movie  num_ratings_movie  \n",
      "0                     3.527663              21220  \n",
      "1                     2.979649              59554  \n",
      "2                     3.790705               1695  \n",
      "3                     3.771080              74899  \n",
      "4                     3.518392                670  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(240538, 20)\n",
      "            User  Rating       Date  Movie  Rating_class  \\\n",
      "23813226  294990     3.0 2005-07-11    952             0   \n",
      "23813227  372871     3.0 2005-05-05   1306             0   \n",
      "23813228  207396     4.0 2005-09-17   3924             0   \n",
      "23813229  330647     2.0 2005-02-09    357             0   \n",
      "23813230  169970     3.0 2004-12-14    311             0   \n",
      "\n",
      "          days_since_first_user_rating  sqrt_days_since_first_user_rating  \\\n",
      "23813226                           686                          26.191602   \n",
      "23813227                             0                           0.000000   \n",
      "23813228                             0                           0.000000   \n",
      "23813229                            50                           7.071068   \n",
      "23813230                           861                          29.342802   \n",
      "\n",
      "          rating_age_days_user  rating_age_weeks_user  rating_age_months_user  \\\n",
      "23813226                   818             116.857143               27.266667   \n",
      "23813227                    99              14.142857                3.300000   \n",
      "23813228                    96              13.714286                3.200000   \n",
      "23813229                   280              40.000000                9.333333   \n",
      "23813230                  1162             166.000000               38.733333   \n",
      "\n",
      "          mean_ratings_user  num_ratings_user  days_since_first_item_rating  \\\n",
      "23813226           3.298116             203.0                          2020   \n",
      "23813227           4.068520              55.0                           664   \n",
      "23813228           2.783074              81.0                           891   \n",
      "23813229           4.072074              29.0                           918   \n",
      "23813230           3.087920              93.0                          1704   \n",
      "\n",
      "          sqrt_days_since_first_item_rating  rating_age_days_item  \\\n",
      "23813226                          44.944410                  2193   \n",
      "23813227                          25.768197                   904   \n",
      "23813228                          29.849623                   996   \n",
      "23813229                          30.298515                  1243   \n",
      "23813230                          41.279535                  2086   \n",
      "\n",
      "          rating_age_weeks_item  rating_age_months_item  mean_ratings_movie  \\\n",
      "23813226             313.285714               73.100000            3.414623   \n",
      "23813227             129.142857               30.133333            3.310373   \n",
      "23813228             142.285714               33.200000            3.514406   \n",
      "23813229             177.571429               41.433333            3.537308   \n",
      "23813230             298.000000               69.533333            3.709892   \n",
      "\n",
      "          weighted_mean_ratings_movie  num_ratings_movie  \n",
      "23813226                     3.421331               6647  \n",
      "23813227                     3.310999             115345  \n",
      "23813228                     3.514616             101449  \n",
      "23813229                     3.542309               2868  \n",
      "23813230                     3.709451              62225  \n"
     ]
    }
   ],
   "source": [
    "TRAIN_FN = os.path.join(PREPARED_DATA_DIR, 'user_train_data_1.h5')\n",
    "TEST_FN = os.path.join(PREPARED_DATA_DIR, 'user_test_data_1.h5')\n",
    "\n",
    "train_df = pd.read_hdf(TRAIN_FN, key='stage')\n",
    "test_df = pd.read_hdf(TEST_FN, key='stage')\n",
    "\n",
    "print(train_df.shape)\n",
    "print(train_df.head())\n",
    "print('\\n\\n\\n')\n",
    "\n",
    "print(test_df.shape)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['days_since_first_user_rating',\n",
    "                'sqrt_days_since_first_user_rating',\n",
    "                'rating_age_days_user', 'rating_age_weeks_user',\n",
    "                'rating_age_months_user', 'mean_ratings_user',\n",
    "                'num_ratings_user', 'days_since_first_item_rating',\n",
    "                'sqrt_days_since_first_item_rating',\n",
    "                'rating_age_days_item', 'rating_age_weeks_item',\n",
    "                'rating_age_months_item', 'mean_ratings_movie',\n",
    "                'weighted_mean_ratings_movie', 'num_ratings_movie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.1 s, sys: 26.2 s, total: 39.3 s\n",
      "Wall time: 46.8 s\n",
      "checkout the calculated parameters\n",
      "\n",
      "[2.74709960e+02 1.30171303e+01 6.01375438e+02 8.59107768e+01\n",
      " 2.00458479e+01 3.60770401e+00 1.38134583e+02 1.01209640e+03\n",
      " 2.99323996e+01 1.45806966e+03 2.08295666e+02 4.86023221e+01\n",
      " 3.59968341e+00 3.60876007e+00 5.15705185e+04]\n",
      "\n",
      "\n",
      "\n",
      "[1.25198956e+05 1.05264278e+02 2.34023739e+05 4.77599468e+03\n",
      " 2.60026377e+02 1.94270819e-01 3.21530138e+04 4.03047266e+05\n",
      " 1.16147858e+02 3.92335018e+05 8.00683711e+03 4.35927798e+02\n",
      " 1.53311077e-01 1.35894022e-01 2.08705953e+09]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "print('fit\\n')\n",
    "%time scaler.fit(train_df[numeric_cols])\n",
    "\n",
    "print('checkout the calculated parameters\\n')\n",
    "print(scaler.mean_)\n",
    "print('\\n\\n')\n",
    "print(scaler.var_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages/ipykernel_launcher.py:1: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.77 s, sys: 13.6 s, total: 20.4 s\n",
      "Wall time: 26.9 s\n",
      "CPU times: user 85.6 ms, sys: 133 ms, total: 219 ms\n",
      "Wall time: 276 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages/ipykernel_launcher.py:1: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "%time train_df_trans = scaler.transform(train_df[numeric_cols]) \n",
    "%time test_df_trans = scaler.transform(test_df[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.71137737 -0.80130883 -0.72427534 -0.72427534 -0.72427534 -0.47948529\n",
      " -0.61420432  0.9433632   0.94689463  1.09349768  1.09349768  1.09349768\n",
      " -0.18610436 -0.21999151 -0.66435269]\n",
      "days_since_first_user_rating              23\n",
      "sqrt_days_since_first_user_rating    4.79583\n",
      "rating_age_days_user                     251\n",
      "rating_age_weeks_user                35.8571\n",
      "rating_age_months_user               8.36667\n",
      "mean_ratings_user                    3.39637\n",
      "num_ratings_user                          28\n",
      "days_since_first_item_rating            1611\n",
      "sqrt_days_since_first_item_rating    40.1373\n",
      "rating_age_days_item                    2143\n",
      "rating_age_weeks_item                306.143\n",
      "rating_age_months_item               71.4333\n",
      "mean_ratings_movie                   3.52681\n",
      "weighted_mean_ratings_movie          3.52766\n",
      "num_ratings_movie                      21220\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_df_trans[0, :])\n",
    "print(train_df.loc[0, numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8013089747646966"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(4.79583 - scaler.mean_[1])/np.sqrt(scaler.var_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'days_since_first_user_rating': {'mean': 274.7099601095336, 'std': 353.83464511490894}, 'sqrt_days_since_first_user_rating': {'mean': 13.017130344209974, 'std': 10.259838093819116}, 'rating_age_days_user': {'mean': 601.3754375396097, 'std': 483.76000175538024}, 'rating_age_weeks_user': {'mean': 85.91077679137302, 'std': 69.10857167933996}, 'rating_age_months_user': {'mean': 20.045847917987032, 'std': 16.125333391846}, 'mean_ratings_user': {'mean': 3.6077040142672616, 'std': 0.4407616348363394}, 'num_ratings_user': {'mean': 138.1345834335839, 'std': 179.3126146705238}, 'days_since_first_item_rating': {'mean': 1012.0964039589561, 'std': 634.8600365564057}, 'sqrt_days_since_first_item_rating': {'mean': 29.93239959797679, 'std': 10.777191576008672}, 'rating_age_days_item': {'mean': 1458.0696618460909, 'std': 626.3665206343762}, 'rating_age_weeks_item': {'mean': 208.2956659780133, 'std': 89.48093151919663}, 'rating_age_months_item': {'mean': 48.602322061536434, 'std': 20.8788840211459}, 'mean_ratings_movie': {'mean': 3.5996834109416445, 'std': 0.391549583881406}, 'weighted_mean_ratings_movie': {'mean': 3.6087600696882904, 'std': 0.3686380639211172}, 'num_ratings_movie': {'mean': 51570.518516810196, 'std': 45684.34666970711}}\n"
     ]
    }
   ],
   "source": [
    "# store the params for later use\n",
    "means = scaler.mean_\n",
    "stds = [np.sqrt(x) for x in scaler.var_]\n",
    "numeric_params_dct = {}\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    numeric_params_dct[col] = {'mean': means[i], 'std': stds[i]}\n",
    "\n",
    "print(numeric_params_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist\n",
    "out_fn = os.path.join(METADATA_DIR, 'numeric_feats_params_dct.json')\n",
    "json.dump(numeric_params_dct, open(out_fn, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def construct_tensor(a):\n",
    "    final = []\n",
    "    for i in a:\n",
    "        out = []\n",
    "        for j in i:\n",
    "            out.append(j.tolist())\n",
    "        out1 = []\n",
    "        for item in zip(*out):\n",
    "            out1.append(list(item))\n",
    "        final += out1\n",
    "    return tensor(final)\n",
    "\n",
    "\n",
    "def construct_tensor_test(a):\n",
    "    out = []\n",
    "    for i in a:\n",
    "        out.append(i.tolist())\n",
    "        out1 = []\n",
    "        for item in zip(*out):\n",
    "            out1.append(list(item))\n",
    "    return tensor(out1)\n",
    "\n",
    "\n",
    "def construct_tensor_y(a):\n",
    "    out = []\n",
    "    for i in a:\n",
    "        out += i.tolist()\n",
    "    return tensor(out)\n",
    "\n",
    "\n",
    "def transform_numeric_cols(numeric_params_dct, numeric_cols, x):\n",
    "    x_new = []\n",
    "    count = 0\n",
    "    for item in x:\n",
    "        if isinstance(item, list):\n",
    "            x_new_item = [] \n",
    "            for i, value in enumerate(item): \n",
    "                d = numeric_params_dct[numeric_cols[i]] \n",
    "                x_new_item.append((value - d['mean'])/d['std']) \n",
    "            x_new.append(x_new_item)\n",
    "        else:\n",
    "            d = numeric_params_dct[numeric_cols[count]]\n",
    "            x_new.append((item - d['mean'])/d['std'])\n",
    "            count += 1\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3.1 - NN Regression with baseline features (normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import IterableDataset\n",
    "from itertools import chain, islice\n",
    "\n",
    "\n",
    "class InteractionsStream(IterableDataset):\n",
    "\n",
    "    def __init__(self, prep_data_dir=PREPARED_DATA_DIR, file_num=None,\n",
    "                 sample='train', user_col='User', item_col='Movie',\n",
    "                 end_token='.h5', start_token='user_{}_data_',\n",
    "                 baseline_feats=False, model_type='regression',\n",
    "                 chunksize=10, normalize=False,\n",
    "                 numeric_params_fn=NUMERIC_FEATS_PARAMS_DCT_FN):\n",
    "\n",
    "        if file_num is None:\n",
    "            self.files = [os.path.join(prep_data_dir, x) for x in\n",
    "                          _find_files(prep_data_dir,\n",
    "                                      start_token.format(sample),\n",
    "                                      end_token)]\n",
    "        else:\n",
    "            self.files = [\n",
    "                os.path.join(prep_data_dir,\n",
    "                             start_token.format(sample)+str(file_num)+\n",
    "                             end_token)]\n",
    "        print(self.files)\n",
    "        self.user_col = user_col\n",
    "        self.item_col = item_col\n",
    "        self.baseline_feats = baseline_feats\n",
    "        self.sample = sample\n",
    "        self.chunksize = chunksize\n",
    "        if model_type == 'regression':\n",
    "            self.dv_col = 'Rating'\n",
    "        elif model_type == 'classification':\n",
    "            self.dv_col = 'Rating_class'\n",
    "        self.cat_cols = [self.user_col, self.item_col]\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        if self.normalize:\n",
    "            self.numeric_params_dct = json.load(open(numeric_params_fn))\n",
    "        \n",
    "        if baseline_feats:\n",
    "            self.numeric_cols = [\n",
    "                'days_since_first_user_rating',\n",
    "                'sqrt_days_since_first_user_rating',\n",
    "                'rating_age_days_user', 'rating_age_weeks_user',\n",
    "                'rating_age_months_user', 'mean_ratings_user',\n",
    "                'num_ratings_user', 'days_since_first_item_rating',\n",
    "                'sqrt_days_since_first_item_rating',\n",
    "                'rating_age_days_item', 'rating_age_weeks_item',\n",
    "                'rating_age_months_item', 'mean_ratings_movie',\n",
    "                'weighted_mean_ratings_movie', 'num_ratings_movie']\n",
    "        else:\n",
    "            self.numeric_cols = []            \n",
    "\n",
    "    def read_file(self, fn):\n",
    "        \n",
    "        if self.sample == 'train':\n",
    "            df = pd.read_hdf(fn, key='stage', iterator=True,\n",
    "                             chunksize=self.chunksize)\n",
    "        else:\n",
    "            df = pd.read_hdf(fn, key='stage')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def transform_numeric_cols(self, numeric_params_dct, numeric_cols,\n",
    "                                x):\n",
    "        x_new = []\n",
    "        count = 0\n",
    "        for item in x:\n",
    "            if isinstance(item, list):\n",
    "                x_new_item = [] \n",
    "                for i, value in enumerate(item): \n",
    "                    d = numeric_params_dct[numeric_cols[i]] \n",
    "                    x_new_item.append((value - d['mean'])/d['std']) \n",
    "                x_new.append(x_new_item)\n",
    "            else:\n",
    "                d = numeric_params_dct[numeric_cols[count]]\n",
    "                x_new.append((item - d['mean'])/d['std'])\n",
    "                count += 1\n",
    "        return x_new\n",
    "\n",
    "    def process_data(self, fn):\n",
    "\n",
    "        print('read data')\n",
    "        data = self.read_file(fn)\n",
    "\n",
    "        print('create an iterable')\n",
    "        if self.sample == 'train':\n",
    "            if self.baseline_feats:\n",
    "                for row in data:\n",
    "                    x1 = row[self.cat_cols].values.tolist()\n",
    "                    x2 = row[self.numeric_cols].values.tolist()\n",
    "                    if self.normalize:\n",
    "                        x2 = self.transform_numeric_cols(\n",
    "                            self.numeric_params_dct, self.numeric_cols,\n",
    "                            x2)\n",
    "                    y = row[self.dv_col].tolist()\n",
    "                    yield (x1, x2, y)\n",
    "            else:\n",
    "                for row in data:\n",
    "                    user = row[self.user_col].tolist()\n",
    "                    item = row[self.item_col].tolist()\n",
    "                    y = row[self.dv_col].tolist()\n",
    "                    yield (user, item), y\n",
    "        else:\n",
    "            if self.baseline_feats:\n",
    "                for i, row in data.iterrows():\n",
    "                    x1 = row[self.cat_cols].tolist()\n",
    "                    x2 = row[self.numeric_cols].tolist()\n",
    "                    if self.normalize:\n",
    "                        x2 = self.transform_numeric_cols(\n",
    "                            self.numeric_params_dct, self.numeric_cols,\n",
    "                            x2)\n",
    "                    y = row[self.dv_col]\n",
    "                    yield (x1, x2, y)\n",
    "            else:\n",
    "                for i, row in data.iterrows():\n",
    "                    yield (row[self.user_col],\n",
    "                           row[self.item_col]), row[self.dv_col]\n",
    "\n",
    "    def get_stream(self, files):\n",
    "        return chain.from_iterable(map(self.process_data, files))\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.get_stream(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines the neural network for tabular data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_sizes, n_cont):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList(\n",
    "            [nn.Embedding(categories, size) for\n",
    "             categories, size in embedding_sizes])\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeddings)\n",
    "        self.n_emb, self.n_cont = n_emb, n_cont\n",
    "        self.lin1 = nn.Linear(self.n_emb + self.n_cont, 200)\n",
    "        self.lin2 = nn.Linear(200, 70)\n",
    "        self.lin3 = nn.Linear(70, 1)\n",
    "        #self.bn1 = nn.BatchNorm1d(self.n_cont)\n",
    "        self.bn1 = nn.BatchNorm1d(200)\n",
    "        self.bn2 = nn.BatchNorm1d(70)\n",
    "        self.emb_drop = nn.Dropout(0.6)\n",
    "        self.drops = nn.Dropout(0.3)\n",
    "\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        x = [e(x_cat[:, i]) for i, e in enumerate(self.embeddings)]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.emb_drop(x)\n",
    "        #x2 = self.bn1(x_cont)\n",
    "        x = torch.cat([x, x_cont], 1)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.lin3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time\n",
    "import torch.optim as torch_optim\n",
    "import torch.nn.functional as F\n",
    "from torch import tensor\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "class Train(object):\n",
    "    \n",
    "    def __init__(self, loss_fn=nn.MSELoss(reduction='sum'), file_num=1,\n",
    "                 n_users=480189, n_items=17770, n_cont=15,\n",
    "                 min_emb_dim=100, cat_cols=['User', 'Movie'],\n",
    "                 lr=0.02, wd=0.00001):\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "                       else torch.device('cpu'))\n",
    "        self.file_num = file_num\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.n_cont = n_cont\n",
    "        self.cat_cols = cat_cols\n",
    "        self.min_emb_dim = min_emb_dim\n",
    "        self.embedding_sizes = self.choose_embedding_size(\n",
    "            self.cat_cols, [self.n_users, self.n_items],\n",
    "            self.min_emb_dim)\n",
    "        self.model = TabularModel(self.embedding_sizes, self.n_cont)\n",
    "        self.model.to(self.device)\n",
    "        self.lr = lr\n",
    "        self.wd = wd\n",
    "        self.optimizer = self.get_optimizer(self.model, lr=self.lr,\n",
    "                                            wd=self.wd)\n",
    "        \n",
    "    def choose_embedding_size(self, cat_cols, cat_num_values,\n",
    "                              min_emb_dim=100):\n",
    "        \"\"\"\n",
    "        cat_cols: list of categorical columns\n",
    "        cat_num_values: list of number of unique values for each\n",
    "        categorical column\n",
    "        \"\"\"\n",
    "        embedded_cols = dict(zip(cat_cols, cat_num_values))\n",
    "        embedding_sizes = [\n",
    "            (n_categories, min(min_emb_dim, (n_categories+1)//2))\n",
    "             for _, n_categories in embedded_cols.items()]\n",
    "        return embedding_sizes\n",
    "    \n",
    "    def get_optimizer(self, model, lr = 0.001, wd = 0.0):\n",
    "        parameters = filter(lambda p: p.requires_grad,\n",
    "                            model.parameters())\n",
    "        optim = torch_optim.Adam(parameters, lr=lr, weight_decay=wd)\n",
    "        return optim\n",
    "    \n",
    "    def construct_tensor(self, a):\n",
    "        final = []\n",
    "        for i in a:\n",
    "            out = []\n",
    "            for j in i:\n",
    "                out.append(j.tolist())\n",
    "            out1 = []\n",
    "            for item in zip(*out):\n",
    "                out1.append(list(item))\n",
    "            final += out1\n",
    "        return tensor(final)\n",
    "\n",
    "\n",
    "    def construct_tensor_test(self, a):\n",
    "        out = []\n",
    "        for i in a:\n",
    "            out.append(i.tolist())\n",
    "            out1 = []\n",
    "            for item in zip(*out):\n",
    "                out1.append(list(item))\n",
    "        return tensor(out1)\n",
    "\n",
    "\n",
    "    def construct_tensor_y(self, a):\n",
    "        out = []\n",
    "        for i in a:\n",
    "            out += i.tolist()\n",
    "        return tensor(out)\n",
    "    \n",
    "    def train(self, train_dl, train_size, chunksize, batch_size):\n",
    "        self.model.train()\n",
    "        total = 0\n",
    "        sum_loss = 0\n",
    "        with tqdm(total=train_size // (batch_size * chunksize)) as pbar:\n",
    "            for x1, x2, y in train_dl:\n",
    "                x1, x2, y = (self.construct_tensor(x1),\n",
    "                             self.construct_tensor(x2),\n",
    "                             self.construct_tensor_y(y))\n",
    "                x1 = x1.to(self.device)\n",
    "                x2 = x2.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                batch = y.size()[0]\n",
    "                y = y.reshape((y.size()[0], 1))\n",
    "                output = self.model(x1, x2)\n",
    "                loss = self.loss_fn(output, y)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total += batch\n",
    "                sum_loss += loss.item()\n",
    "                pbar.update(1)\n",
    "        return sum_loss/total\n",
    "    \n",
    "    def evaluate(self, valid_dl, test_size, batch_size):\n",
    "        self.model.eval()\n",
    "        total = 0\n",
    "        sum_loss = 0\n",
    "        with tqdm(total=test_size // (batch_size)) as pbar:\n",
    "            for x1, x2, y in valid_dl:\n",
    "                x1, x2 = (self.construct_tensor_test(x1),\n",
    "                          self.construct_tensor_test(x2))\n",
    "                x1 = x1.to(self.device)\n",
    "                x2 = x2.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                current_batch_size = y.size()[0]\n",
    "                y = y.reshape((y.size()[0], 1))\n",
    "                y = y.float()\n",
    "                out = self.model(x1, x2)\n",
    "                loss = self.loss_fn(out, y)\n",
    "                sum_loss += loss.item()\n",
    "                total += current_batch_size\n",
    "                pbar.update(1)\n",
    "        print(\"valid loss %.3f\" % (sum_loss/total))\n",
    "\n",
    "        return sum_loss/total\n",
    "    \n",
    "    def batch_fit(self, train_dl, valid_dl, epochs, train_size,\n",
    "                  test_size, chunksize, batch_size):\n",
    "        start = time.time()\n",
    "        losses = []\n",
    "        for i in range(epochs):\n",
    "            stats = {'epoch': i+1}\n",
    "            train_loss = self.train(train_dl, train_size, chunksize,\n",
    "                                    batch_size)\n",
    "            print(\"training loss: \", train_loss)\n",
    "            stats['train_loss'] = train_loss\n",
    "            test_loss = self.evaluate(valid_dl, test_size, batch_size)\n",
    "            print('time taken: %0.2f' % (time.time() - start))\n",
    "            stats['test_loss'] = test_loss\n",
    "            losses.append(stats)\n",
    "        return losses\n",
    "    \n",
    "    def predict(self, test_dl):\n",
    "        preds = []\n",
    "        actuals = []\n",
    "        with torch.no_grad():\n",
    "            for x1, x2, y in test_dl:\n",
    "                x1, x2 = (self.construct_tensor_test(x1),\n",
    "                          self.construct_tensor_test(x2))\n",
    "                x1 = x1.to(self.device)\n",
    "                x2 = x2.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                y = y.reshape((y.size()[0], 1))\n",
    "                pred = self.model(x1, x2)\n",
    "                preds.append(pred.tolist())\n",
    "                actuals.append(y.tolist())\n",
    "        final_preds = [item for sublist in preds for item in sublist]\n",
    "        final_actuals = [item for sublist in actuals for item in sublist]\n",
    "        rmse = np.sqrt(mean_squared_error(y_true=final_actuals,\n",
    "                                          y_pred=final_preds))\n",
    "        return final_actuals, final_preds, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "FILE_NUM = 1\n",
    "N_USERS = 480189\n",
    "N_ITEMS = 17770\n",
    "N_CONT = 15\n",
    "BATCH_SIZE = 50\n",
    "CHUNKSIZE = 100\n",
    "TRAIN_SIZE = 22851074\n",
    "VAL_SIZE = 962152 \n",
    "TEST_SIZE = 240538"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/varunn/Documents/kaggle/netflix-prize-data/prepared_data_for_NN_modelling/user_train_data_1.h5']\n",
      "['/Users/varunn/Documents/kaggle/netflix-prize-data/prepared_data_for_NN_modelling/user_test_data_1.h5']\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = InteractionsStream(\n",
    "    file_num=FILE_NUM, baseline_feats=True, model_type='regression',\n",
    "    sample='train', chunksize=CHUNKSIZE, normalize=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                          shuffle=False)\n",
    "\n",
    "test_dataset = InteractionsStream(file_num=FILE_NUM, baseline_feats=True,\n",
    "                                  model_type='regression', sample='test',\n",
    "                                  normalize=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n",
      "create an iterable\n",
      "tensor([[161459,   2138],\n",
      "        [191296,   1154],\n",
      "        [ 87375,   3253],\n",
      "        [ 27266,   1201]])\n",
      "\n",
      "\n",
      "tensor([[-0.7114, -0.8013, -0.7243, -0.7243, -0.7243, -0.4795, -0.6142,  0.9434,\n",
      "          0.9469,  1.0935,  1.0935,  1.0935, -0.1861, -0.2200, -0.6644],\n",
      "        [ 0.5039,  0.8057, -0.3026, -0.3026, -0.3026,  0.7880, -0.1681, -0.7956,\n",
      "         -0.6881, -1.5072, -1.5072, -1.5072,  0.5598,  0.4936, -1.0917],\n",
      "        [-0.7396, -0.9173,  0.0323,  0.0323,  0.0323,  1.6471,  0.1387, -0.9720,\n",
      "         -0.9332, -0.6483, -0.6483, -0.6483, -1.5902, -1.7066,  0.1748],\n",
      "        [-0.7340, -0.8913, -0.3563, -0.3563, -0.3563,  0.3406, -0.0788,  1.1686,\n",
      "          1.1087,  1.2084,  1.2084,  1.2084,  0.4392,  0.4403,  0.5106]])\n",
      "\n",
      "\n",
      "tensor([[4.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [5.]])\n",
      "torch.Size([4, 15])\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "for x1, x2, y in islice(train_loader, 1):\n",
    "    x1, x2, y = (construct_tensor(x1), construct_tensor(x2),\n",
    "                 construct_tensor_y(y))\n",
    "    y = y.reshape((y.size()[0], 1))\n",
    "    print(x1)\n",
    "    print('\\n')\n",
    "    print(x2)\n",
    "    print('\\n')\n",
    "    print(y)\n",
    "    print(x2.shape)\n",
    "    print('\\n\\n\\n')\n",
    "    #x2_new = transform_numeric_cols(numeric_params_dct, numeric_cols, x2)\n",
    "    #print(x2_new)\n",
    "    #print(x2_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor x1, x2, y in islice(test_loader, 1):\\n    x1, x2 = construct_tensor_test(x1), construct_tensor_test(x2)\\n    y = y.reshape((y.size()[0], 1))\\n    y = y.float()\\n    print(x1)\\n    print('\\n')\\n    print(x2)\\n    print('\\n')\\n    print(y)\\n    print(x2.shape)\\n    print(y.shape)\\n    out = model.model(x1, x2)\\n    print(out)\\n    loss = torch.nn.MSELoss(reduction='sum')(out, y)\\n    print(loss)\\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for x1, x2, y in islice(test_loader, 1):\n",
    "    x1, x2 = construct_tensor_test(x1), construct_tensor_test(x2)\n",
    "    y = y.reshape((y.size()[0], 1))\n",
    "    y = y.float()\n",
    "    print(x1)\n",
    "    print('\\n')\n",
    "    print(x2)\n",
    "    print('\\n')\n",
    "    print(y)\n",
    "    print(x2.shape)\n",
    "    print(y.shape)\n",
    "    out = model.model(x1, x2)\n",
    "    print(out)\n",
    "    loss = torch.nn.MSELoss(reduction='sum')(out, y)\n",
    "    print(loss)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate train class\n",
    "\n",
    "model = Train(file_num=FILE_NUM, n_users=N_USERS, n_items=N_ITEMS,\n",
    "              n_cont=N_CONT, lr=0.02, wd=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabularModel(\n",
       "  (embeddings): ModuleList(\n",
       "    (0): Embedding(480189, 100)\n",
       "    (1): Embedding(17770, 100)\n",
       "  )\n",
       "  (lin1): Linear(in_features=215, out_features=200, bias=True)\n",
       "  (lin2): Linear(in_features=200, out_features=70, bias=True)\n",
       "  (lin3): Linear(in_features=70, out_features=1, bias=True)\n",
       "  (bn1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm1d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (emb_drop): Dropout(p=0.6, inplace=False)\n",
       "  (drops): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4570 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n",
      "create an iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4571it [2:00:27,  1.42s/it]                            \n",
      "  0%|          | 0/4810 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  0.8844703704705356\n",
      "read data\n",
      "create an iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4811it [06:00, 13.36it/s]                          \n",
      "  0%|          | 0/4570 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss 0.804\n",
      "time taken: 7587.23\n",
      "read data\n",
      "create an iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4571it [1:57:31,  1.32s/it]                            \n",
      "  0%|          | 0/4810 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  0.8080374877252133\n",
      "read data\n",
      "create an iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4811it [05:59, 13.37it/s]                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss 0.791\n",
      "time taken: 14998.71\n",
      "time taken: 14998.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "model.batch_fit(train_dl=train_loader, valid_dl=test_loader, epochs=2,\n",
    "                train_size=TRAIN_SIZE, test_size=TEST_SIZE,\n",
    "                chunksize=CHUNKSIZE, batch_size=BATCH_SIZE)\n",
    "\n",
    "print('time taken: %0.2f' % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8893818077743664"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(0.791)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
