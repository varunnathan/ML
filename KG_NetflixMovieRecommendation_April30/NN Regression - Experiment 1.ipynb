{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "1. MF using NN - features based on users and movies\n",
    "2. NN regression - features based on users, movies & baseline features\n",
    "3. NN regression - features based on users, movies, movie titles & baseline features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "import copy\n",
    "import pickle\n",
    "import zipfile\n",
    "from textwrap import wrap\n",
    "from pathlib import Path\n",
    "from itertools import zip_longest\n",
    "from collections import defaultdict\n",
    "from urllib.error import URLError\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import tensor\n",
    "from torch.nn import functional as F \n",
    "from torch.optim.lr_scheduler import _LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"src/\")\n",
    "from constants import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the number of unique users and movies and subsequently create a user2index and item2index map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_files(_dir, start_token, end_token):\n",
    "    return [x for x in os.listdir(_dir) if (x.startswith(start_token))\n",
    "            and (x.endswith(end_token))]\n",
    "\n",
    "\n",
    "def _get_entity_index_map(entity_lst):\n",
    "\n",
    "    user2idx = {user: i for i, user in enumerate(entity_lst)}\n",
    "    idx2user = {i: user for i, user in enumerate(entity_lst)}\n",
    "\n",
    "    return user2idx, idx2user\n",
    "\n",
    "\n",
    "def get_metadata(data_dir=OUT_DIR, start_token='user_data_',\n",
    "                 end_token='.h5', user_col='User', item_col='Movie'):\n",
    "    \n",
    "    print('get all the files to be read')\n",
    "    files = _find_files(data_dir, start_token, end_token)\n",
    "    print(files)\n",
    "    print('num files: %d' % (len(files)))\n",
    "    \n",
    "    user_ids, item_ids = set(), set()\n",
    "    \n",
    "    for i, file in enumerate(files):\n",
    "        print('num files completed: %d' % (i))\n",
    "        print('\\n')\n",
    "\n",
    "        fn = os.path.join(data_dir, file)\n",
    "        print('reading file %s' % (fn))\n",
    "        df = pd.read_hdf(fn, key='stage')\n",
    "        print('shape: ', df.shape)\n",
    "        \n",
    "        print('update number of unique categories for discrete variables')\n",
    "        user_lst = df[user_col].unique().tolist()\n",
    "        item_lst = df[item_col].unique().tolist()\n",
    "\n",
    "        user_ids = user_ids.union(set(user_lst))\n",
    "        item_ids = item_ids.union(set(item_lst))\n",
    "    \n",
    "    print('\\n\\n\\n')\n",
    "    print('num users: %d' % (len(user_ids)))\n",
    "    print('num items: %d' % (len(item_ids)))\n",
    "    \n",
    "    print('create entity to index mapping')\n",
    "    user2idx, idx2user = _get_entity_index_map(user_ids)\n",
    "    item2idx, idx2item = _get_entity_index_map(item_ids)\n",
    "    del user_ids, item_ids\n",
    "    print('\\n\\n')\n",
    "    \n",
    "    print('save artifacts \\n')\n",
    "\n",
    "    print('user2idx \\n')\n",
    "    json.dump(user2idx, open(USER2IDX_FN, 'w'))\n",
    "\n",
    "    print('idx2user \\n')\n",
    "    json.dump(idx2user, open(IDX2USER_FN, 'w'))\n",
    "\n",
    "    print('item2idx \\n')\n",
    "    json.dump(item2idx, open(ITEM2IDX_FN, 'w'))\n",
    "\n",
    "    print('idx2item \\n')\n",
    "    json.dump(idx2item, open(IDX2ITEM_FN, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get all the files to be read\n",
      "['user_data_1.h5', 'user_data_2.h5', 'user_data_3.h5', 'user_data_4.h5']\n",
      "num files: 4\n",
      "num files completed: 0\n",
      "\n",
      "\n",
      "reading file /Users/varunn/Documents/kaggle/netflix-prize-data/interim/user_data_1.h5\n",
      "shape:  (24053764, 4)\n",
      "update number of unique categories for discrete variables\n",
      "num files completed: 1\n",
      "\n",
      "\n",
      "reading file /Users/varunn/Documents/kaggle/netflix-prize-data/interim/user_data_2.h5\n",
      "shape:  (26977591, 4)\n",
      "update number of unique categories for discrete variables\n",
      "num files completed: 2\n",
      "\n",
      "\n",
      "reading file /Users/varunn/Documents/kaggle/netflix-prize-data/interim/user_data_3.h5\n",
      "shape:  (22601629, 4)\n",
      "update number of unique categories for discrete variables\n",
      "num files completed: 3\n",
      "\n",
      "\n",
      "reading file /Users/varunn/Documents/kaggle/netflix-prize-data/interim/user_data_4.h5\n",
      "shape:  (26847523, 4)\n",
      "update number of unique categories for discrete variables\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "num users: 480189\n",
      "num items: 17770\n",
      "create entity to index mapping\n",
      "\n",
      "\n",
      "\n",
      "save artifacts \n",
      "\n",
      "user2idx \n",
      "\n",
      "idx2user \n",
      "\n",
      "item2idx \n",
      "\n",
      "idx2item \n",
      "\n",
      "CPU times: user 34.7 s, sys: 8.39 s, total: 43 s\n",
      "Wall time: 43.2 s\n"
     ]
    }
   ],
   "source": [
    "%time get_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for the modelling experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareData(object):\n",
    "\n",
    "    def __init__(self, sample, interim_data_dir=OUT_DIR,\n",
    "                 prep_data_dir=PREPARED_DATA_DIR,\n",
    "                 user_col='User', item_col='Movie', end_token='.h5',\n",
    "                 date_col='Date', dv_col='Rating', good_ratings=[5],\n",
    "                 start_token='user_{}_data_', user2idx_fn=USER2IDX_FN,\n",
    "                 item2idx_fn=ITEM2IDX_FN, baseline_feats=False,\n",
    "                 erd_user_fn=EARLIEST_RATING_DATE_USER_FN,\n",
    "                 erd_movie_fn=EARLIEST_RATING_DATE_MOVIE_FN,\n",
    "                 lrd_user_fn=LATEST_RATING_DATE_USER_FN,\n",
    "                 lrd_movie_fn=LATEST_RATING_DATE_MOVIE_FN,\n",
    "                 mr_user_fn=MEAN_RATINGS_USER_DCT_FN,\n",
    "                 mr_movie_fn=MEAN_RATINGS_MOVIE_DCT_FN,\n",
    "                 wmr_movie_fn=WEIGHTED_MEAN_RATINGS_MOVIE_DCT_FN,\n",
    "                 nr_user_fn=NUM_RATINGS_USER_DCT_FN,\n",
    "                 nr_movie_fn=NUM_RATINGS_MOVIE_DCT_FN):\n",
    "\n",
    "        files = _find_files(interim_data_dir, start_token.format(sample),\n",
    "                            end_token)\n",
    "        self.files = [os.path.join(interim_data_dir, x) for x in files]\n",
    "        self.out_files = [os.path.join(prep_data_dir, x) for x in files]\n",
    "        print(self.files)\n",
    "        print(self.out_files)\n",
    "        self.date_col = date_col\n",
    "        self.dv_col = dv_col\n",
    "        self.good_ratings = good_ratings\n",
    "        self.dv_col_class = dv_col + '_class'\n",
    "        self.user_col = user_col\n",
    "        self.item_col = item_col\n",
    "        self.user2idx = json.load(open(user2idx_fn))\n",
    "        self.item2idx = json.load(open(item2idx_fn))\n",
    "        self.baseline_feats = baseline_feats\n",
    "        \n",
    "        if self.baseline_feats:\n",
    "            \n",
    "            self.erd_user = self.convert_to_datetime(json.load(open(\n",
    "                erd_user_fn)))\n",
    "            d = self.calc_median(self.erd_user, self.user2idx,\n",
    "                                 'earliest')\n",
    "            if d is not None:\n",
    "                self.erd_user.update(d)\n",
    "            \n",
    "            self.erd_movie = self.convert_to_datetime(json.load(open(\n",
    "                erd_movie_fn)))\n",
    "            d = self.calc_median(self.erd_movie, self.item2idx,\n",
    "                                 'earliest')\n",
    "            if d is not None:\n",
    "                self.erd_movie.update(d)\n",
    "            \n",
    "            self.lrd_user = self.convert_to_datetime(json.load(open(\n",
    "                lrd_user_fn)))\n",
    "            d = self.calc_median(self.lrd_user, self.user2idx, 'latest')\n",
    "            if d is not None:\n",
    "                self.lrd_user.update(d)\n",
    "            \n",
    "            self.lrd_movie = self.convert_to_datetime(json.load(open(\n",
    "                lrd_movie_fn)))\n",
    "            d = self.calc_median(self.lrd_movie, self.item2idx, 'latest')\n",
    "            if d is not None:\n",
    "                self.lrd_movie.update(d)\n",
    "            \n",
    "            self.mr_user = json.load(open(mr_user_fn))\n",
    "            d = self.calc_median(self.mr_user, self.user2idx, None)\n",
    "            if d is not None:\n",
    "                self.mr_user.update(d)\n",
    "            \n",
    "            self.mr_movie = json.load(open(mr_movie_fn))\n",
    "            d = self.calc_median(self.mr_movie, self.item2idx, None)\n",
    "            if d is not None:\n",
    "                self.mr_movie.update(d)\n",
    "            \n",
    "            self.wmr_movie = json.load(open(wmr_movie_fn))\n",
    "            d = self.calc_median(self.wmr_movie, self.item2idx, None)\n",
    "            if d is not None:\n",
    "                self.wmr_movie.update(d)\n",
    "            \n",
    "            self.nr_user = json.load(open(nr_user_fn))\n",
    "            d = self.calc_median(self.nr_user, self.user2idx, None)\n",
    "            if d is not None:\n",
    "                self.nr_user.update(d)\n",
    "            \n",
    "            self.nr_movie = json.load(open(nr_movie_fn))\n",
    "            d = self.calc_median(self.nr_movie, self.item2idx, None)\n",
    "            if d is not None:\n",
    "                self.nr_movie.update(d)\n",
    "            \n",
    "\n",
    "    def read_file(self, fn):\n",
    "        \n",
    "        df = pd.read_hdf(fn, key='stage')\n",
    "        return df\n",
    "    \n",
    "    def convert_to_datetime(self, feat_dct):\n",
    "        return {k: pd.to_datetime(v, format='%Y-%m-%d')\n",
    "                for k, v in feat_dct.items()}\n",
    "    \n",
    "    def calc_median(self, feat_dct, ent2idx, date):\n",
    "        \n",
    "        if len(ent2idx) > len(feat_dct):\n",
    "            print('there are missing users or movies')\n",
    "            missing_users = set(ent2idx.keys())\n",
    "            missing_users = missing_users - set(feat_dct.keys())\n",
    "            if date is None:\n",
    "                median = np.nanmedian(list(feat_dct.values()))\n",
    "            elif date == 'earliest':\n",
    "                median = np.nanmin(list(feat_dct.values()))\n",
    "            elif date == 'latest':\n",
    "                median = np.nanmax(list(feat_dct.values()))\n",
    "            d = dict(zip(missing_users, [median]*len(missing_users)))\n",
    "            return d\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def calc_date_features(self, data, erd_feat_dct, lrd_feat_dct,\n",
    "                           feat_type):\n",
    "        \n",
    "        if feat_type == 'user':\n",
    "            ent_col = self.user_col\n",
    "        elif feat_type == 'item':\n",
    "            ent_col = self.item_col\n",
    "        \n",
    "        parent_feature = 'days_since_first_{}_rating'.format(feat_type)\n",
    "        print(parent_feature)\n",
    "        data[parent_feature] = list(\n",
    "            map(lambda user, date: (date -\n",
    "                                    erd_feat_dct[str(user)]).days\n",
    "                if str(user) in erd_feat_dct else None,\n",
    "                data[ent_col], data[self.date_col]))\n",
    "\n",
    "        feature = 'sqrt_days_since_first_{}_rating'.format(feat_type)\n",
    "        print(feature)\n",
    "        data[feature] = data[parent_feature].apply(\n",
    "            lambda x: np.sqrt(x) if (x is not None) and (x>=0) else None)\n",
    "        mask = data[parent_feature] < 0\n",
    "        data.loc[mask, feature] = -1\n",
    "        data.loc[mask, parent_feature] = -1\n",
    "\n",
    "        feature = 'rating_age_days_{}'.format(feat_type)\n",
    "        print(feature)\n",
    "        data[feature] = list(\n",
    "            map(lambda user: (lrd_feat_dct[str(user)] -\n",
    "                              erd_feat_dct[str(user)]).days\n",
    "                if (str(user) in erd_feat_dct) and\n",
    "                (str(user) in lrd_feat_dct) else None, data[ent_col]))\n",
    "\n",
    "        feature = 'rating_age_weeks_{}'.format(feat_type)\n",
    "        parent_feature = 'rating_age_days_{}'.format(feat_type)\n",
    "        print(feature)\n",
    "        data[feature] = data[parent_feature].apply(\n",
    "            lambda x: x/7. if x is not None else None)\n",
    "\n",
    "        feature = 'rating_age_months_{}'.format(feat_type)\n",
    "        parent_feature = 'rating_age_days_{}'.format(feat_type)\n",
    "        print(feature)\n",
    "        data[feature] = data[parent_feature].apply(\n",
    "            lambda x: x/30. if x is not None else None)\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def preprocess(self, data):\n",
    "\n",
    "        print('convert %s to datetime format' % (self.date_col))\n",
    "        data[self.date_col] = pd.to_datetime(data[self.date_col],\n",
    "                                             format='%Y-%m-%d')\n",
    "\n",
    "        print('encoding for DV - only for classification experiments')\n",
    "        mask = data[self.dv_col].isin(self.good_ratings)\n",
    "        data[self.dv_col_class] = 0\n",
    "        data.loc[mask, self.dv_col_class] = 1\n",
    "\n",
    "        if self.baseline_feats:\n",
    "            print('adding user and item baseline features\\n')\n",
    "\n",
    "            print('Define ordered tuple of feature columns\\n')\n",
    "            user_feats = [\n",
    "                ('mean_ratings_user', self.mr_user),\n",
    "                ('num_ratings_user', self.nr_user)]\n",
    "            \n",
    "            movie_feats = [\n",
    "                ('mean_ratings_movie', self.mr_movie),\n",
    "                ('weighted_mean_ratings_movie', self.wmr_movie),\n",
    "                ('num_ratings_movie', self.nr_movie)]\n",
    "\n",
    "            print('User Features\\n')\n",
    "            \n",
    "            print('date features\\n')\n",
    "            data = self.calc_date_features(data, self.erd_user,\n",
    "                                           self.lrd_user, 'user')\n",
    "            print('other features\\n')\n",
    "            for feat_name, feat_dct in user_feats:\n",
    "                print('Feature: ', feat_name)\n",
    "                data[feat_name] = data[self.user_col].apply(\n",
    "                    lambda x: feat_dct.get(str(x), None))\n",
    "\n",
    "            print('Item Features\\n')\n",
    "            \n",
    "            print('date features\\n')\n",
    "            data = self.calc_date_features(data, self.erd_movie,\n",
    "                                           self.lrd_movie, 'item')\n",
    "            print('other features\\n')\n",
    "            for feat_name, feat_dct in movie_feats:\n",
    "                print('Feature: ', feat_name)\n",
    "                data[feat_name] = data[self.item_col].apply(\n",
    "                    lambda x: feat_dct.get(str(x), None))     \n",
    "\n",
    "        print('encoding for categorical variables')\n",
    "        data[self.user_col] = data[self.user_col].apply(\n",
    "            lambda x: self.user2idx[str(x)])\n",
    "        data[self.item_col] = data[self.item_col].apply(\n",
    "            lambda x: self.item2idx[str(x)])\n",
    "        \n",
    "        print('drop unwanted columns')\n",
    "        data.drop(['num_rating_user', 'num_rating_user_bins',\n",
    "                   'num_rating_movie', 'num_rating_movie_bins'],\n",
    "                  axis=1, inplace=True)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \n",
    "        for i, fn in enumerate(self.files):\n",
    "            if os.path.isfile(self.out_files[i]):\n",
    "                print('file %s already exists in the disk.' % (\n",
    "                    self.out_files[i]))\n",
    "                continue\n",
    "            print('num completed files: %d' % (i))\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            print('read data')\n",
    "            data = self.read_file(fn)\n",
    "            print('time taken: %0.2f' % (time.time() - start))\n",
    "            print('\\n\\n\\n')\n",
    "\n",
    "            print('preprocess data')\n",
    "            data = self.preprocess(data)\n",
    "            print('time taken: %0.2f' % (time.time() - start))\n",
    "            print('\\n\\n\\n')\n",
    "\n",
    "            print('save')\n",
    "            out_fn = self.out_files[i]\n",
    "            print('out file: %s' % (out_fn))\n",
    "            data.to_hdf(out_fn, key='stage', mode='w')\n",
    "            del data\n",
    "            print('time taken: %0.2f' % (time.time() - start))\n",
    "            print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/varunn/Documents/kaggle/netflix-prize-data/interim/user_train_data_1.h5', '/Users/varunn/Documents/kaggle/netflix-prize-data/interim/user_train_data_2.h5', '/Users/varunn/Documents/kaggle/netflix-prize-data/interim/user_train_data_3.h5', '/Users/varunn/Documents/kaggle/netflix-prize-data/interim/user_train_data_4.h5']\n",
      "['/Users/varunn/Documents/kaggle/netflix-prize-data/prepared_data_for_NN_modelling/user_train_data_1.h5', '/Users/varunn/Documents/kaggle/netflix-prize-data/prepared_data_for_NN_modelling/user_train_data_2.h5', '/Users/varunn/Documents/kaggle/netflix-prize-data/prepared_data_for_NN_modelling/user_train_data_3.h5', '/Users/varunn/Documents/kaggle/netflix-prize-data/prepared_data_for_NN_modelling/user_train_data_4.h5']\n",
      "there are missing users or movies\n",
      "there are missing users or movies\n",
      "there are missing users or movies\n",
      "there are missing users or movies\n",
      "CPU times: user 2min 35s, sys: 1.25 s, total: 2min 36s\n",
      "Wall time: 2min 38s\n"
     ]
    }
   ],
   "source": [
    "%time prep_data = PrepareData(sample='train', baseline_feats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480189\n",
      "17770\n",
      "480189\n",
      "17770\n",
      "480189\n",
      "17770\n",
      "17770\n",
      "480189\n",
      "17770\n"
     ]
    }
   ],
   "source": [
    "print(len(prep_data.erd_user))\n",
    "print(len(prep_data.erd_movie))\n",
    "print(len(prep_data.lrd_user))\n",
    "print(len(prep_data.lrd_movie))\n",
    "print(len(prep_data.mr_user))\n",
    "print(len(prep_data.mr_movie))\n",
    "print(len(prep_data.wmr_movie))\n",
    "print(len(prep_data.nr_user))\n",
    "print(len(prep_data.nr_movie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num completed files: 0\n",
      "read data\n",
      "time taken: 8.43\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "preprocess data\n",
      "convert Date to datetime format\n",
      "encoding for DV - only for classification experiments\n",
      "adding user and item baseline features\n",
      "\n",
      "Define ordered tuple of feature columns\n",
      "\n",
      "User Features\n",
      "\n",
      "date features\n",
      "\n",
      "days_since_first_user_rating\n",
      "sqrt_days_since_first_user_rating\n",
      "rating_age_days_user\n",
      "rating_age_weeks_user\n",
      "rating_age_months_user\n",
      "other features\n",
      "\n",
      "Feature:  mean_ratings_user\n",
      "Feature:  num_ratings_user\n",
      "Item Features\n",
      "\n",
      "date features\n",
      "\n",
      "days_since_first_item_rating\n",
      "sqrt_days_since_first_item_rating\n",
      "rating_age_days_item\n",
      "rating_age_weeks_item\n",
      "rating_age_months_item\n",
      "other features\n",
      "\n",
      "Feature:  mean_ratings_movie\n",
      "Feature:  weighted_mean_ratings_movie\n",
      "Feature:  num_ratings_movie\n",
      "encoding for categorical variables\n",
      "drop unwanted columns\n",
      "time taken: 1957.49\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "save\n",
      "out file: /Users/varunn/Documents/kaggle/netflix-prize-data/prepared_data_for_NN_modelling/user_train_data_1.h5\n",
      "time taken: 1970.24\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "num completed files: 1\n",
      "read data\n",
      "time taken: 9.08\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "preprocess data\n",
      "convert Date to datetime format\n",
      "encoding for DV - only for classification experiments\n",
      "adding user and item baseline features\n",
      "\n",
      "Define ordered tuple of feature columns\n",
      "\n",
      "User Features\n",
      "\n",
      "date features\n",
      "\n",
      "days_since_first_user_rating\n",
      "sqrt_days_since_first_user_rating\n",
      "rating_age_days_user\n",
      "rating_age_weeks_user\n",
      "rating_age_months_user\n",
      "other features\n",
      "\n",
      "Feature:  mean_ratings_user\n",
      "Feature:  num_ratings_user\n",
      "Item Features\n",
      "\n",
      "date features\n",
      "\n",
      "days_since_first_item_rating\n",
      "sqrt_days_since_first_item_rating\n",
      "rating_age_days_item\n",
      "rating_age_weeks_item\n",
      "rating_age_months_item\n",
      "other features\n",
      "\n",
      "Feature:  mean_ratings_movie\n",
      "Feature:  weighted_mean_ratings_movie\n",
      "Feature:  num_ratings_movie\n",
      "encoding for categorical variables\n",
      "drop unwanted columns\n",
      "time taken: 2154.71\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "save\n",
      "out file: /Users/varunn/Documents/kaggle/netflix-prize-data/prepared_data_for_NN_modelling/user_train_data_2.h5\n",
      "time taken: 2179.78\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "num completed files: 2\n",
      "read data\n",
      "time taken: 8.18\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "preprocess data\n",
      "convert Date to datetime format\n",
      "encoding for DV - only for classification experiments\n",
      "adding user and item baseline features\n",
      "\n",
      "Define ordered tuple of feature columns\n",
      "\n",
      "User Features\n",
      "\n",
      "date features\n",
      "\n",
      "days_since_first_user_rating\n",
      "sqrt_days_since_first_user_rating\n",
      "rating_age_days_user\n",
      "rating_age_weeks_user\n",
      "rating_age_months_user\n",
      "other features\n",
      "\n",
      "Feature:  mean_ratings_user\n",
      "Feature:  num_ratings_user\n",
      "Item Features\n",
      "\n",
      "date features\n",
      "\n",
      "days_since_first_item_rating\n",
      "sqrt_days_since_first_item_rating\n",
      "rating_age_days_item\n",
      "rating_age_weeks_item\n",
      "rating_age_months_item\n",
      "other features\n",
      "\n",
      "Feature:  mean_ratings_movie\n",
      "Feature:  weighted_mean_ratings_movie\n",
      "Feature:  num_ratings_movie\n",
      "encoding for categorical variables\n",
      "drop unwanted columns\n",
      "time taken: 1675.59\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "save\n",
      "out file: /Users/varunn/Documents/kaggle/netflix-prize-data/prepared_data_for_NN_modelling/user_train_data_3.h5\n",
      "time taken: 1699.99\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "file /Users/varunn/Documents/kaggle/netflix-prize-data/prepared_data_for_NN_modelling/user_train_data_4.h5 already exists in the disk.\n",
      "CPU times: user 1h 34min 7s, sys: 2min 29s, total: 1h 36min 36s\n",
      "Wall time: 1h 37min 30s\n"
     ]
    }
   ],
   "source": [
    "%time prep_data.prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1 - MF with NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network and model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCF(nn.Module):\n",
    "    def __init__(self, n_users: int, n_items: int, factors: int = 16,\n",
    "                 user_embeddings: torch.tensor = None,\n",
    "                 freeze_users: bool = False,\n",
    "                 item_embeddings: torch.tensor = None,\n",
    "                 freeze_items: bool = False,\n",
    "                 init: torch.nn.init = torch.nn.init.normal_,\n",
    "                 binary: bool =False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.binary = binary\n",
    "\n",
    "        self.user_embeddings = self._create_embedding(\n",
    "            n_users, factors, user_embeddings, freeze_users,\n",
    "            init, **kwargs)\n",
    "        self.item_embeddings = self._create_embedding(\n",
    "            n_items, factors, item_embeddings, freeze_items,\n",
    "            init, **kwargs)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, u: torch.tensor, i: torch.tensor) -> torch.tensor:\n",
    "        user_embedding = self.user_embeddings(u)\n",
    "        user_embedding = user_embedding[:, None, :]\n",
    "        item_embedding = self.item_embeddings(i)\n",
    "        item_embedding = item_embedding[:, None, :]\n",
    "        rating = torch.matmul(user_embedding, item_embedding.transpose(\n",
    "            1, 2))\n",
    "        if self.binary:\n",
    "            return self.sigmoid(rating)\n",
    "        return rating\n",
    "\n",
    "    def _create_embedding(self, n_items, factors, weights, freeze,\n",
    "                          init, **kwargs):\n",
    "        embedding = nn.Embedding(n_items, factors)\n",
    "        init(embedding.weight.data, **kwargs)\n",
    "\n",
    "        if weights is not None:\n",
    "            embedding.load_state_dict({'weight': weights})\n",
    "        if freeze:\n",
    "            embedding.weight.requires_grad = False\n",
    "\n",
    "        return embedding\n",
    "    \n",
    "    \n",
    "class BaseModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Base module for explicit matrix factorization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 n_users,\n",
    "                 n_items,\n",
    "                 n_factors=40,\n",
    "                 dropout_p=0,\n",
    "                 sparse=False,\n",
    "                 user_embeddings: torch.tensor = None,\n",
    "                 user_biases: torch.tensor = None,\n",
    "                 freeze_users: bool = False,\n",
    "                 item_embeddings: torch.tensor = None,\n",
    "                 item_biases: torch.tensor = None,\n",
    "                 freeze_items: bool = False,\n",
    "                 init: torch.nn.init = torch.nn.init.normal_,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_users : int\n",
    "            Number of users\n",
    "        n_items : int\n",
    "            Number of items\n",
    "        n_factors : int\n",
    "            Number of latent factors (or embeddings or whatever you want to\n",
    "            call it).\n",
    "        dropout_p : float\n",
    "            p in nn.Dropout module. Probability of dropout.\n",
    "        sparse : bool\n",
    "            Whether or not to treat embeddings as sparse. NOTE: cannot use\n",
    "            weight decay on the optimizer if sparse=True. Also, can only use\n",
    "            Adagrad.\n",
    "        \"\"\"\n",
    "        super(BaseModule, self).__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.n_factors = n_factors\n",
    "        self.user_embeddings, self.user_biases = self._create_embedding(\n",
    "            n_users, n_factors, user_embeddings, user_biases,\n",
    "            freeze_users, init, sparse, **kwargs)\n",
    "        self.item_embeddings, self.item_biases = self._create_embedding(\n",
    "            n_items, n_factors, item_embeddings, item_biases,\n",
    "            freeze_items, init, sparse, **kwargs)\n",
    "        \n",
    "        self.dropout_p = dropout_p\n",
    "        self.dropout = nn.Dropout(p=self.dropout_p)\n",
    "\n",
    "        self.sparse = sparse\n",
    "        \n",
    "    def forward(self, users, items):\n",
    "        \"\"\"\n",
    "        Forward pass through the model. For a single user and item, this\n",
    "        looks like:\n",
    "        user_bias + item_bias + user_embeddings.dot(item_embeddings)\n",
    "        Parameters\n",
    "        ----------\n",
    "        users : np.ndarray\n",
    "            Array of user indices\n",
    "        items : np.ndarray\n",
    "            Array of item indices\n",
    "        Returns\n",
    "        -------\n",
    "        preds : np.ndarray\n",
    "            Predicted ratings.\n",
    "        \"\"\"\n",
    "        ues = self.user_embeddings(users)\n",
    "        uis = self.item_embeddings(items)\n",
    "\n",
    "        preds = self.user_biases(users)\n",
    "        preds += self.item_biases(items)\n",
    "        preds += (self.dropout(ues) * self.dropout(uis)).sum(\n",
    "            dim=1, keepdim=True)\n",
    "\n",
    "        return preds.squeeze()\n",
    "    \n",
    "    def __call__(self, *args):\n",
    "        return self.forward(*args)\n",
    "\n",
    "    def predict(self, users, items):\n",
    "        return self.forward(users, items)\n",
    "    \n",
    "    def _create_embedding(self, n_items, n_factors, pre_weights,\n",
    "                          pre_biases, freeze, init, sparse, **kwargs):\n",
    "        \n",
    "        bias = nn.Embedding(n_items, 1, sparse=sparse)\n",
    "        embedding = nn.Embedding(n_items, n_factors, sparse=sparse)\n",
    "        init(bias.weight.data, **kwargs)\n",
    "        init(embedding.weight.data, **kwargs)\n",
    "\n",
    "        if pre_weights is not None:\n",
    "            embedding.load_state_dict({'weight': pre_weights})\n",
    "            \n",
    "        if pre_biases is not None:\n",
    "            bias.load_state_dict({'weight': pre_biases})\n",
    "        \n",
    "        if freeze:\n",
    "            embedding.weight.requires_grad = False\n",
    "            bias.weight.requires_grad = False\n",
    "\n",
    "        return embedding, bias\n",
    "\n",
    "\n",
    "def bpr_loss(preds, vals):\n",
    "    sig = nn.Sigmoid()\n",
    "    return (1.0 - sig(preds)).pow(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class Interactions(Dataset):\n",
    "    \"\"\"\n",
    "    Hold data in the form of an interactions matrix.\n",
    "    Typical use-case is like a ratings matrix:\n",
    "    - Users are the rows\n",
    "    - Items are the columns\n",
    "    - Elements of the matrix are the ratings given by a user for an item.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mat):\n",
    "        self.mat = mat.astype(np.float32).tocoo()\n",
    "        self.n_users = self.mat.shape[0]\n",
    "        self.n_items = self.mat.shape[1]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.mat.row[index]\n",
    "        col = self.mat.col[index]\n",
    "        val = self.mat.data[index]\n",
    "        return (row, col), val\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.mat.nnz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create interactions matrix\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def get_interaction_matrix(df, n_users, n_movies, user2index,\n",
    "                           item2index):\n",
    "    interactions = np.zeros((n_users, n_movies))\n",
    "    for row in df.itertuples():\n",
    "        interactions[user2index[row[1]], item2index[row[2]]] = row[3]\n",
    "    \n",
    "    return sp.coo_matrix(interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset\n",
    "from itertools import chain, islice\n",
    "\n",
    "\n",
    "class InteractionsStream(IterableDataset):\n",
    "\n",
    "    def __init__(self, prep_data_dir=PREPARED_DATA_DIR, file_num=None,\n",
    "                 sample='train', user_col='User', item_col='Movie',\n",
    "                 end_token='.h5', start_token='user_{}_data_',\n",
    "                 baseline_feats=False, model_type='regression',\n",
    "                 chunksize=10):\n",
    "\n",
    "        if file_num is None:\n",
    "            self.files = [os.path.join(prep_data_dir, x) for x in\n",
    "                          _find_files(prep_data_dir,\n",
    "                                      start_token.format(sample),\n",
    "                                      end_token)]\n",
    "        else:\n",
    "            self.files = [\n",
    "                os.path.join(prep_data_dir,\n",
    "                             start_token.format(sample)+str(file_num)+\n",
    "                             end_token)]\n",
    "        print(self.files)\n",
    "        self.user_col = user_col\n",
    "        self.item_col = item_col\n",
    "        self.baseline_feats = baseline_feats\n",
    "        self.sample = sample\n",
    "        self.chunksize = chunksize\n",
    "        if model_type == 'regression':\n",
    "            self.dv_col = 'Rating'\n",
    "        elif model_type == 'classification':\n",
    "            self.dv_col = 'Rating_class'\n",
    "        self.cat_cols = [self.user_col, self.item_col]\n",
    "        \n",
    "        if baseline_feats:\n",
    "            self.numeric_cols = [\n",
    "                'days_since_first_user_rating',\n",
    "                'sqrt_days_since_first_user_rating',\n",
    "                'rating_age_days_user', 'rating_age_weeks_user',\n",
    "                'rating_age_months_user', 'mean_ratings_user',\n",
    "                'num_ratings_user', 'days_since_first_item_rating',\n",
    "                'sqrt_days_since_first_item_rating',\n",
    "                'rating_age_days_item', 'rating_age_weeks_item',\n",
    "                'rating_age_months_item', 'mean_ratings_movie',\n",
    "                'weighted_mean_ratings_movie', 'num_ratings_movie']\n",
    "        else:\n",
    "            self.numeric_cols = []            \n",
    "\n",
    "    def read_file(self, fn):\n",
    "        \n",
    "        if self.sample == 'train':\n",
    "            df = pd.read_hdf(fn, key='stage', iterator=True,\n",
    "                             chunksize=self.chunksize)\n",
    "        else:\n",
    "            df = pd.read_hdf(fn, key='stage')\n",
    "        \n",
    "        return df       \n",
    "\n",
    "    def process_data(self, fn):\n",
    "\n",
    "        print('read data')\n",
    "        data = self.read_file(fn)\n",
    "\n",
    "        if self.sample == 'train':\n",
    "            for row in data:\n",
    "                user = row[self.user_col].tolist()\n",
    "                item = row[self.item_col].tolist()\n",
    "                y = row[self.dv_col].tolist()\n",
    "                yield (user, item), y\n",
    "        else:\n",
    "            for i, row in data.iterrows():\n",
    "                yield (row[self.user_col],\n",
    "                       row[self.item_col]), row[self.dv_col]\n",
    "\n",
    "    def get_stream(self, files):\n",
    "        return chain.from_iterable(map(self.process_data, files))\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.get_stream(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, tensor\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "from abc import ABCMeta\n",
    "from abc import abstractmethod\n",
    "from typing import Callable\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class StepBase:\n",
    "    \"\"\"Defines the interface that all step models here expose.\"\"\"\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    @abstractmethod\n",
    "    def batch_fit(self, data_loader: torch.utils.data.DataLoader, epochs: int):\n",
    "        \"\"\"Trains the model on a batch of user-item interactions.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self, user: torch.tensor, item: torch.tensor,\n",
    "             rating: torch.tensor, preference: torch.tensor):\n",
    "        \"\"\"Trains the model incrementally.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, user: torch.tensor, k: int):\n",
    "        \"\"\"Recommends the top-k items to a specific user.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save(self, path: str):\n",
    "        \"\"\"Saves the model parameters to the given path.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load(self, path: str):\n",
    "        \"\"\"Loads the model parameters from a given path.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class Step(StepBase):\n",
    "    \"\"\"Incremental and batch training of recommender systems.\"\"\"\n",
    "    def __init__(self, model: torch.nn.Module,\n",
    "                 loss_function=torch.nn.MSELoss(reduction='sum'),\n",
    "                 optimizer = torch.optim.Adam,\n",
    "                 lr = 0.01, weight_decay = 0., batch_size=512,\n",
    "                 chunksize=10):\n",
    "        self.model = model\n",
    "        self.loss_function = loss_function\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.chunksize = chunksize\n",
    "        self.weight_decay = weight_decay\n",
    "        self.optimizer = optimizer(self.model.parameters(),\n",
    "                                   lr=self.lr,\n",
    "                                   weight_decay=self.weight_decay)\n",
    "        self.losses = []\n",
    "\n",
    "        # check if the user has provided user and item embeddings\n",
    "        assert self.model.user_embeddings, 'User embedding matrix could not be found.'\n",
    "        assert self.model.item_embeddings, 'Item embedding matrix could not be found.'\n",
    "\n",
    "    @property\n",
    "    def user_embeddings(self):\n",
    "        return self.model.user_embeddings\n",
    "\n",
    "    @property\n",
    "    def item_embeddings(self):\n",
    "        return self.model.item_embeddings\n",
    "    \n",
    "    @property\n",
    "    def user_biases(self):\n",
    "        return self.model.user_biases\n",
    "    \n",
    "    @property\n",
    "    def item_biases(self):\n",
    "        return self.model.item_biases\n",
    "    \n",
    "    def construct_tensor(self, a):\n",
    "        out = [] \n",
    "        for i in a: \n",
    "            out += i.tolist() \n",
    "        return tensor(out)\n",
    "\n",
    "    def batch_fit(self, data_loader: torch.utils.data.DataLoader,\n",
    "                  data_size: int, epochs: int = 1):\n",
    "        \"\"\"Trains the model on a batch of user-item interactions.\"\"\"\n",
    "        \n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = torch.Tensor([0])\n",
    "            with tqdm(total=data_size//(self.batch_size * self.chunksize)) as pbar:\n",
    "                for _, ((row, col), val) in enumerate(data_loader):\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "                    row = self.construct_tensor(row).long()\n",
    "                    col = self.construct_tensor(col).long()\n",
    "                    val = self.construct_tensor(val).float()\n",
    "                    \n",
    "                    #print(row.size(), '\\t', col.size(), '\\t', val.size())\n",
    "\n",
    "                    preds = self.model(row, col)\n",
    "                    loss = self.loss_function(preds, val)\n",
    "                    loss.backward()\n",
    "\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    total_loss += loss.item()\n",
    "                    batch_loss = loss.item() / row.size()[0]\n",
    "\n",
    "                    pbar.update(1)\n",
    "                \n",
    "            total_loss /= data_size\n",
    "            self.losses.append(total_loss)\n",
    "            \n",
    "    def _validation_loss(self, data_loader: torch.utils.data.DataLoader,\n",
    "                         data_size: int):\n",
    "        self.model.eval()\n",
    "        total_loss = torch.Tensor([0])\n",
    "        for _, ((row, col), val) in enumerate(data_loader):\n",
    "            row = row.long()\n",
    "            if isinstance(col, list):\n",
    "                col = tuple(c.long() for c in col)\n",
    "            else:\n",
    "                col = col.long()\n",
    "            val = val.float()\n",
    "\n",
    "            preds = self.model(row, col)\n",
    "            loss = self.loss_function(preds, val)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        total_loss /= data_size\n",
    "        return total_loss[0]\n",
    "\n",
    "    def step(self, user: torch.tensor, item: torch.tensor,\n",
    "             rating: torch.tensor = None):\n",
    "        \"\"\"Trains the model incrementally.\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        pred = self.model(user, item)\n",
    "        loss = self.loss_function(pred, rating)\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "        return batch_loss\n",
    "\n",
    "    def recommend(self, user: torch.tensor, k:int = 10) -> torch.tensor:\n",
    "        \"\"\"Recommends the top-k items to a specific user.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        u_embed_one = self.user_embeddings(user)\n",
    "        u_embed_one_reshaped = u_embed_one.reshape((\n",
    "            1, u_embed_one.shape[0]))\n",
    "        m_embed = self.item_embeddings.weight\n",
    "        u_bias_one = self.user_biases(user)\n",
    "        u_bias_one_reshaped = u_bias_one.reshape((\n",
    "            1, u_bias_one.shape[0]))\n",
    "        m_bias = self.item_biases.weight\n",
    "        \n",
    "        bias_sum = u_bias_one_reshaped + m_bias\n",
    "        bias_sum = bias_sum.reshape((bias_sum.shape[1],\n",
    "                                     bias_sum.shape[0]))\n",
    "\n",
    "        preds = torch.matmul(u_embed_one_reshaped, m_embed.t())+bias_sum\n",
    "\n",
    "        return preds.squeeze().argsort()[-k:]\n",
    "\n",
    "    def save(self, path: str):\n",
    "        \"\"\"Saves the model parameters to the given path.\"\"\"\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "    def load(self, path: str):\n",
    "        \"\"\"Loads the model parameters from a given path.\"\"\"\n",
    "        self.model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try training with file 1. If it works, then create a custom data loader that continuously streams batches of data from all the 4 part files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "FILE_NUM = 1\n",
    "N_USERS = 480189\n",
    "N_ITEMS = 17770\n",
    "BATCH_SIZE = 500\n",
    "TRAIN_SIZE = 22851074 # corresponds to FILE_NUM\n",
    "VAL_SIZE = 962152     # corresponds to FILE_NUM\n",
    "TEST_SIZE = 240538    # corresponds to FILE_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/varunn/Documents/kaggle/netflix-prize-data/prepared_data_for_NN_modelling/user_train_data_1.h5']\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = InteractionsStream(file_num=FILE_NUM, baseline_feats=False,\n",
    "                             model_type='regression', sample='train',\n",
    "                             chunksize=2)\n",
    "train_loader = DataLoader(dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/varunn/Documents/kaggle/netflix-prize-data/prepared_data_for_NN_modelling/user_test_data_1.h5']\n"
     ]
    }
   ],
   "source": [
    "test_dataset = InteractionsStream(file_num=FILE_NUM, baseline_feats=False,\n",
    "                                  model_type='regression', sample='test')\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/varunn/Documents/kaggle/netflix-prize-data/prepared_data_for_NN_modelling/user_val_data_1.h5']\n"
     ]
    }
   ],
   "source": [
    "val_dataset = InteractionsStream(file_num=FILE_NUM, baseline_feats=False,\n",
    "                                 model_type='regression', sample='val')\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_tensor(a):\n",
    "    out = [] \n",
    "    for i in a: \n",
    "        out += i.tolist() \n",
    "    return tensor(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n",
      "0 \t tensor([161459, 191296,  87375,  27266]) \t tensor([2138, 1154, 3253, 1201]) \t tensor([4., 2., 2., 5.]) torch.Size([4])\n",
      "1 \t tensor([175666, 252679, 141629, 134412]) \t tensor([4377,  289, 1405, 4340]) \t tensor([3., 3., 3., 3.]) torch.Size([4])\n",
      "2 \t tensor([130415, 398255, 296886, 122198]) \t tensor([2339, 4487, 1026, 1641]) \t tensor([3., 3., 4., 3.]) torch.Size([4])\n",
      "3 \t tensor([ 86100, 405639, 187857, 109577]) \t tensor([3417, 3127, 1901,  995]) \t tensor([3., 3., 3., 5.]) torch.Size([4])\n",
      "4 \t tensor([ 55618, 392816, 383780, 392221]) \t tensor([2560,  329, 3902, 1832]) \t tensor([3., 3., 4., 1.]) torch.Size([4])\n",
      "5 \t tensor([360000, 331687, 177187, 327369]) \t tensor([2574, 1876, 4170, 2620]) \t tensor([5., 5., 4., 5.]) torch.Size([4])\n",
      "6 \t tensor([ 97259, 371177, 106697,  32546]) \t tensor([2327, 3824, 3167, 1434]) \t tensor([1., 3., 5., 3.]) torch.Size([4])\n",
      "7 \t tensor([202018, 141240, 331242, 460100]) \t tensor([ 108, 1011, 2250, 1865]) \t tensor([4., 5., 2., 4.]) torch.Size([4])\n",
      "8 \t tensor([ 11693, 140841, 222181,  45378]) \t tensor([1961, 1179, 1434, 1219]) \t tensor([5., 4., 5., 3.]) torch.Size([4])\n",
      "9 \t tensor([229159, 232382, 418302, 314350]) \t tensor([1143, 3925,  482,  961]) \t tensor([5., 3., 1., 3.]) torch.Size([4])\n",
      "time taken: 0.28\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for i, ((row, col), val) in enumerate(islice(train_loader, 10)):\n",
    "    \n",
    "    print(i, '\\t', construct_tensor(row).long(), '\\t',\n",
    "          construct_tensor(col).long(),\n",
    "          '\\t', construct_tensor(val).float(), construct_tensor(row).size())\n",
    "    \n",
    "print('time taken: %0.2f' % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n",
      "0 \t tensor([294990, 372871]) \t tensor([ 952, 1306]) \t tensor([3., 3.])\n",
      "1 \t tensor([207396, 330647]) \t tensor([3924,  357]) \t tensor([4., 2.])\n",
      "2 \t tensor([169970,  67519]) \t tensor([ 311, 2991]) \t tensor([3., 3.])\n",
      "3 \t tensor([332836,  59424]) \t tensor([2881,  570]) \t tensor([4., 5.])\n",
      "4 \t tensor([  5795, 193554]) \t tensor([ 456, 3863]) \t tensor([2., 5.])\n",
      "5 \t tensor([293805, 261844]) \t tensor([1143, 4079]) \t tensor([5., 5.])\n",
      "6 \t tensor([65289, 24001]) \t tensor([2847, 3797]) \t tensor([5., 5.])\n",
      "7 \t tensor([ 24053, 246913]) \t tensor([1859, 1743]) \t tensor([4., 4.])\n",
      "time taken: 1.41\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for i, ((row, col), val) in enumerate(islice(test_loader, 8)):\n",
    "    print(i, '\\t', row.long(), '\\t', col.long(), '\\t', val.float())\n",
    "    \n",
    "print('time taken: %0.2f' % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n",
      "0 \t [[tensor([ 95805, 392278]), tensor([3714, 1641])], tensor([5., 5.], dtype=torch.float64)]\n",
      "1 \t [[tensor([231167, 228667]), tensor([3623,  894])], tensor([4., 4.], dtype=torch.float64)]\n",
      "2 \t [[tensor([183932, 275970]), tensor([1179, 2605])], tensor([3., 4.], dtype=torch.float64)]\n",
      "3 \t [[tensor([402484, 418136]), tensor([3221, 1809])], tensor([3., 4.], dtype=torch.float64)]\n",
      "4 \t [[tensor([371283, 336578]), tensor([1797, 3152])], tensor([5., 5.], dtype=torch.float64)]\n",
      "5 \t [[tensor([156182, 407293]), tensor([4431, 1702])], tensor([2., 4.], dtype=torch.float64)]\n",
      "6 \t [[tensor([221310, 210155]), tensor([ 788, 1651])], tensor([5., 5.], dtype=torch.float64)]\n",
      "7 \t [[tensor([244322, 263423]), tensor([ 988, 4355])], tensor([4., 3.], dtype=torch.float64)]\n",
      "time taken: 5.52\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for i, batch in enumerate(islice(val_loader, 8)):\n",
    "    print(i, '\\t', batch)\n",
    "    \n",
    "print('time taken: %0.2f' % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on 1 part file owing to data size and resource constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/varunn/Documents/kaggle/netflix-prize-data/prepared_data_for_NN_modelling/user_train_data_1.h5']\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_dataset = InteractionsStream(\n",
    "    file_num=FILE_NUM, baseline_feats=False, model_type='regression',\n",
    "    sample='train', chunksize=10)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/varunn/Documents/kaggle/netflix-prize-data/prepared_data_for_NN_modelling/user_test_data_1.h5']\n",
      "['/Users/varunn/Documents/kaggle/netflix-prize-data/prepared_data_for_NN_modelling/user_val_data_1.h5']\n"
     ]
    }
   ],
   "source": [
    "test_dataset = InteractionsStream(file_num=FILE_NUM, baseline_feats=False,\n",
    "                                  model_type='regression', sample='test')\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                         shuffle=False)\n",
    "\n",
    "val_dataset = InteractionsStream(file_num=FILE_NUM, baseline_feats=False,\n",
    "                                 model_type='regression', sample='val')\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 0.82\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "net = BaseModule(n_users=N_USERS, n_items=N_ITEMS, n_factors=100,\n",
    "                 dropout_p=0.02)\n",
    "model = Step(net, lr=0.02, weight_decay=0.1, batch_size=BATCH_SIZE,\n",
    "             chunksize=10)\n",
    "print('time taken: %0.2f' % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModule(\n",
       "  (user_embeddings): Embedding(480189, 100)\n",
       "  (user_biases): Embedding(480189, 1)\n",
       "  (item_embeddings): Embedding(17770, 100)\n",
       "  (item_biases): Embedding(17770, 1)\n",
       "  (dropout): Dropout(p=0.02, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4570 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4571it [3:26:25,  2.23s/it]                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 12385.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "model.batch_fit(train_loader, TRAIN_SIZE, epochs=1)\n",
    "\n",
    "print('time taken: %0.2f' % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([2.7790]), tensor([1.0267]), tensor([1.0279])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n",
      "loss of model on test:  tensor(1.0241)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print('loss of model on test: ',\n",
    "      model._validation_loss(test_loader, TEST_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = os.path.join(MODEL_DIR, 'model_NN_MF_{}_E2.pt'.format(\n",
    "    FILE_NUM))\n",
    "model.save(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n"
     ]
    }
   ],
   "source": [
    "# get prediction for test set\n",
    "\n",
    "preds = []\n",
    "actuals = []\n",
    "with torch.no_grad():\n",
    "    for _, ((row, col), val) in enumerate(test_loader):\n",
    "        row = row.long()\n",
    "        if isinstance(col, list):\n",
    "            col = tuple(c.long() for c in col)\n",
    "        else:\n",
    "            col = col.long()\n",
    "        val = val.float()\n",
    "        #print(row, col, val)\n",
    "\n",
    "        pred = model.model(row, col)\n",
    "        #print(pred)\n",
    "        preds.append(pred.tolist())\n",
    "        actuals.append(val.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(482, 482)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(actuals), len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = [item for sublist in preds for item in sublist]\n",
    "final_actuals = [item for sublist in actuals for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240538,\n",
       " 240538,\n",
       " [3.1168930530548096,\n",
       "  3.129922866821289,\n",
       "  3.370454788208008,\n",
       "  3.2390098571777344,\n",
       "  3.769129514694214,\n",
       "  3.7638726234436035,\n",
       "  3.4293339252471924,\n",
       "  4.000853538513184,\n",
       "  3.80224347114563,\n",
       "  4.212867736816406],\n",
       " [3.0, 3.0, 4.0, 2.0, 3.0, 3.0, 4.0, 5.0, 2.0, 5.0])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_preds), len(final_actuals), final_preds[:10], final_actuals[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 1.0120\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "rmse = np.sqrt(mean_squared_error(y_true=final_actuals,\n",
    "                                  y_pred=final_preds))\n",
    "print('Test RMSE: %0.4f' % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3 - NN Regression with baseline features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import IterableDataset\n",
    "from itertools import chain, islice\n",
    "\n",
    "\n",
    "class InteractionsStream(IterableDataset):\n",
    "\n",
    "    def __init__(self, prep_data_dir=PREPARED_DATA_DIR, file_num=None,\n",
    "                 sample='train', user_col='User', item_col='Movie',\n",
    "                 end_token='.h5', start_token='user_{}_data_',\n",
    "                 baseline_feats=False, model_type='regression',\n",
    "                 chunksize=10):\n",
    "\n",
    "        if file_num is None:\n",
    "            self.files = [os.path.join(prep_data_dir, x) for x in\n",
    "                          _find_files(prep_data_dir,\n",
    "                                      start_token.format(sample),\n",
    "                                      end_token)]\n",
    "        else:\n",
    "            self.files = [\n",
    "                os.path.join(prep_data_dir,\n",
    "                             start_token.format(sample)+str(file_num)+\n",
    "                             end_token)]\n",
    "        print(self.files)\n",
    "        self.user_col = user_col\n",
    "        self.item_col = item_col\n",
    "        self.baseline_feats = baseline_feats\n",
    "        self.sample = sample\n",
    "        self.chunksize = chunksize\n",
    "        if model_type == 'regression':\n",
    "            self.dv_col = 'Rating'\n",
    "        elif model_type == 'classification':\n",
    "            self.dv_col = 'Rating_class'\n",
    "        self.cat_cols = [self.user_col, self.item_col]\n",
    "        \n",
    "        if baseline_feats:\n",
    "            self.numeric_cols = [\n",
    "                'days_since_first_user_rating',\n",
    "                'sqrt_days_since_first_user_rating',\n",
    "                'rating_age_days_user', 'rating_age_weeks_user',\n",
    "                'rating_age_months_user', 'mean_ratings_user',\n",
    "                'num_ratings_user', 'days_since_first_item_rating',\n",
    "                'sqrt_days_since_first_item_rating',\n",
    "                'rating_age_days_item', 'rating_age_weeks_item',\n",
    "                'rating_age_months_item', 'mean_ratings_movie',\n",
    "                'weighted_mean_ratings_movie', 'num_ratings_movie']\n",
    "        else:\n",
    "            self.numeric_cols = []            \n",
    "\n",
    "    def read_file(self, fn):\n",
    "        \n",
    "        if self.sample == 'train':\n",
    "            df = pd.read_hdf(fn, key='stage', iterator=True,\n",
    "                             chunksize=self.chunksize)\n",
    "        else:\n",
    "            df = pd.read_hdf(fn, key='stage')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def iter_data(self, data):\n",
    "        \n",
    "        if self.sample == 'train':\n",
    "            if self.baseline_feats:\n",
    "                for row in data:\n",
    "                    yield (row[self.cat_cols].values.tolist(),\n",
    "                           row[self.numeric_cols].values.tolist(),\n",
    "                           row[self.dv_col].tolist())\n",
    "            else:\n",
    "                for row in data:\n",
    "                    user = row[self.user_col].tolist()\n",
    "                    item = row[self.item_col].tolist()\n",
    "                    y = row[self.dv_col].tolist()\n",
    "                    yield (user, item), y\n",
    "        else:\n",
    "            if self.baseline_feats:\n",
    "                for i, row in data.iterrows():\n",
    "                    yield (row[self.cat_cols].values,\n",
    "                           row[self.numeric_cols].values, \n",
    "                           row[self.dv_col])\n",
    "            else:\n",
    "                for i, row in data.iterrows():\n",
    "                    yield (row[self.user_col],\n",
    "                           row[self.item_col]), row[self.dv_col]\n",
    "\n",
    "    def process_data(self, fn):\n",
    "\n",
    "        print('read data')\n",
    "        data = self.read_file(fn)\n",
    "\n",
    "        print('create an iterable')\n",
    "        self.iter_data(data)\n",
    "\n",
    "    def get_stream(self, files):\n",
    "        return chain.from_iterable(map(self.process_data, files))\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.get_stream(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines the neural network for product recommendation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_sizes, n_cont, n_classes=3):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(categories, size) for\n",
    "                                         categories, size in embedding_sizes])\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeddings)\n",
    "        self.n_emb, self.n_cont, self.n_classes = n_emb, n_cont, n_classes\n",
    "        self.lin1 = nn.Linear(self.n_emb + self.n_cont, 200)\n",
    "        self.lin2 = nn.Linear(200, 70)\n",
    "        self.lin3 = nn.Linear(70, self.n_classes)\n",
    "        self.bn1 = nn.BatchNorm1d(self.n_cont)\n",
    "        self.bn2 = nn.BatchNorm1d(200)\n",
    "        self.bn3 = nn.BatchNorm1d(70)\n",
    "        self.emb_drop = nn.Dropout(0.6)\n",
    "        self.drops = nn.Dropout(0.3)\n",
    "\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        x = [e(x_cat[:, i]) for i, e in enumerate(self.embeddings)]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.emb_drop(x)\n",
    "        x2 = self.bn1(x_cont)\n",
    "        x = torch.cat([x, x2], 1)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.lin3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, <function torch.nn.parallel.DistributedDataParallelCPU(*args, **kwargs)>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.get_num_threads(), nn.parallel.DistributedDataParallelCPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.999994398512737"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(160 * 4570214)/22851074"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
