{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys, time, joblib, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"src/\")\n",
    "from constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read movie titles\n",
    "movie_titles = pd.read_csv(os.path.join(DATA_DIR, 'movie_titles.csv'),\n",
    "                           encoding = 'ISO-8859-1', \n",
    "                           header = None, \n",
    "                           names = ['Id', 'Year', 'Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17770, 3)\n",
      "   Id    Year                          Name\n",
      "0   1  2003.0               Dinosaur Planet\n",
      "1   2  2004.0    Isle of Man TT 2004 Review\n",
      "2   3  1997.0                     Character\n",
      "3   4  1994.0  Paula Abdul's Get Up & Dance\n",
      "4   5  2004.0      The Rise and Fall of ECW\n"
     ]
    }
   ],
   "source": [
    "print(movie_titles.shape)\n",
    "print(movie_titles.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17770 ['Dinosaur Planet', 'Isle of Man TT 2004 Review', 'Character']\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary\n",
    "names = movie_titles['Name'].tolist()\n",
    "movie_ids = movie_titles['Id'].tolist()\n",
    "print(len(names), names[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove everything except words and numbers\n",
    "import re, nltk\n",
    "\n",
    "def regex_cleaning(text, only_alphanumeric=True, only_alpha=True):\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\x01', ' ')\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+',' ', text)\n",
    "    text = re.sub(r\"http[s]?\\S+\", \" \", text)\n",
    "    text = re.sub(r\"xx+\", \" \", text)\n",
    "    text = re.sub(r\"x{2,}\", \"\", text)\n",
    "    text = re.sub(r\"`\", \"\", text)\n",
    "    if only_alpha:\n",
    "        text = re.sub('[0-9]+', '', text)\n",
    "    text = text.replace('\"', '')\n",
    "    text = text.replace(\"'\", \"\")\n",
    "    if only_alphanumeric:\n",
    "        text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_cleaned = [regex_cleaning(x) for x in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17770,\n",
       " ['dinosaur planet',\n",
       "  'isle of man tt review',\n",
       "  'character',\n",
       "  'paula abduls get up dance',\n",
       "  'the rise and fall of ecw'],\n",
       " ['paula', 'abduls', 'get', 'up', 'dance'],\n",
       " ['paula', 'abduls', 'get', 'up', 'dance'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(names_cleaned), names_cleaned[:5], names_cleaned[3].split(),\n",
    " nltk.word_tokenize(names_cleaned[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary size\n",
    "from collections import Counter\n",
    "\n",
    "words = [w for item in names_cleaned for w in nltk.word_tokenize(item)]\n",
    "\n",
    "counter = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 6102),\n",
       " ('of', 2149),\n",
       " ('a', 764),\n",
       " ('and', 742),\n",
       " ('in', 742),\n",
       " ('season', 742),\n",
       " ('to', 422),\n",
       " ('live', 296),\n",
       " ('vol', 292),\n",
       " ('on', 256)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(counter.keys()))\n",
    "counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/varunn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords + keep only words and numbers\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "words = [w for item in names_cleaned for w in nltk.word_tokenize(item)\n",
    "         if w not in stop]\n",
    "\n",
    "counter = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('season', 742),\n",
       " ('live', 296),\n",
       " ('vol', 292),\n",
       " ('series', 218),\n",
       " ('man', 216),\n",
       " ('bonus', 216),\n",
       " ('material', 216),\n",
       " ('love', 201),\n",
       " ('best', 161),\n",
       " ('world', 158)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(counter.keys()))\n",
    "counter.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify dataset class to accommodate text embedding based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "from torch import tensor\n",
    "def construct_tensor(a):\n",
    "    final = []\n",
    "    for i in a:\n",
    "        out = []\n",
    "        for j in i:\n",
    "            out.append(j.tolist())\n",
    "        out1 = []\n",
    "        for item in zip(*out):\n",
    "            out1.append(list(item))\n",
    "        final += out1\n",
    "    return tensor(final)\n",
    "\n",
    "\n",
    "def construct_tensor_test(a):\n",
    "    out = []\n",
    "    for i in a:\n",
    "        out.append(i.tolist())\n",
    "        out1 = []\n",
    "        for item in zip(*out):\n",
    "            out1.append(list(item))\n",
    "    return tensor(out1)\n",
    "\n",
    "\n",
    "def construct_tensor_y(a):\n",
    "    out = []\n",
    "    for i in a:\n",
    "        out += i.tolist()\n",
    "    return tensor(out)\n",
    "\n",
    "\n",
    "def transform_numeric_cols(numeric_params_dct, numeric_cols, x):\n",
    "    x_new = []\n",
    "    count = 0\n",
    "    for item in x:\n",
    "        if isinstance(item, list):\n",
    "            x_new_item = [] \n",
    "            for i, value in enumerate(item): \n",
    "                d = numeric_params_dct[numeric_cols[i]] \n",
    "                x_new_item.append((value - d['mean'])/d['std']) \n",
    "            x_new.append(x_new_item)\n",
    "        else:\n",
    "            d = numeric_params_dct[numeric_cols[count]]\n",
    "            x_new.append((item - d['mean'])/d['std'])\n",
    "            count += 1\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import IterableDataset\n",
    "from itertools import chain, islice\n",
    "\n",
    "\n",
    "class InteractionsStream(IterableDataset):\n",
    "\n",
    "    def __init__(self, prep_data_dir=PREPARED_DATA_DIR, file_num=None,\n",
    "                 sample='train', user_col='User', item_col='Movie',\n",
    "                 end_token='.h5', start_token='user_{}_data_',\n",
    "                 baseline_feats=False, model_type='regression',\n",
    "                 chunksize=10, normalize=False, title_features=False,\n",
    "                 numeric_params_fn=NUMERIC_FEATS_PARAMS_DCT_FN,\n",
    "                 title_feats_fn=MOVIE_TITLES_TFIDF_COMPS_FN):\n",
    "\n",
    "        if file_num is None:\n",
    "            self.files = [os.path.join(prep_data_dir, x) for x in\n",
    "                          _find_files(prep_data_dir,\n",
    "                                      start_token.format(sample),\n",
    "                                      end_token)]\n",
    "        else:\n",
    "            self.files = [\n",
    "                os.path.join(prep_data_dir,\n",
    "                             start_token.format(sample)+str(file_num)+\n",
    "                             end_token)]\n",
    "        print(self.files)\n",
    "        self.user_col = user_col\n",
    "        self.item_col = item_col\n",
    "        self.baseline_feats = baseline_feats\n",
    "        self.sample = sample\n",
    "        self.chunksize = chunksize\n",
    "        if model_type == 'regression':\n",
    "            self.dv_col = 'Rating'\n",
    "        elif model_type == 'classification':\n",
    "            self.dv_col = 'Rating_class'\n",
    "        self.cat_cols = [self.user_col, self.item_col]\n",
    "        self.normalize = normalize\n",
    "        self.title_features = title_features\n",
    "        \n",
    "        if self.normalize:\n",
    "            self.numeric_params_dct = json.load(open(numeric_params_fn))\n",
    "        \n",
    "        if self.title_features:\n",
    "            self.title_feats_dct = json.load(open(title_feats_fn))\n",
    "        \n",
    "        if baseline_feats:\n",
    "            self.numeric_cols = [\n",
    "                'days_since_first_user_rating',\n",
    "                'sqrt_days_since_first_user_rating',\n",
    "                'rating_age_days_user', 'rating_age_weeks_user',\n",
    "                'rating_age_months_user', 'mean_ratings_user',\n",
    "                'num_ratings_user', 'days_since_first_item_rating',\n",
    "                'sqrt_days_since_first_item_rating',\n",
    "                'rating_age_days_item', 'rating_age_weeks_item',\n",
    "                'rating_age_months_item', 'mean_ratings_movie',\n",
    "                'weighted_mean_ratings_movie', 'num_ratings_movie']\n",
    "        else:\n",
    "            self.numeric_cols = []            \n",
    "\n",
    "    def read_file(self, fn):\n",
    "        \n",
    "        if self.sample == 'train':\n",
    "            df = pd.read_hdf(fn, key='stage', iterator=True,\n",
    "                             chunksize=self.chunksize)\n",
    "        else:\n",
    "            df = pd.read_hdf(fn, key='stage')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def transform_numeric_cols(self, numeric_params_dct, numeric_cols,\n",
    "                                x):\n",
    "        x_new = []\n",
    "        count = 0\n",
    "        for item in x:\n",
    "            if isinstance(item, list):\n",
    "                x_new_item = [] \n",
    "                for i, value in enumerate(item): \n",
    "                    d = numeric_params_dct[numeric_cols[i]] \n",
    "                    x_new_item.append((value - d['mean'])/d['std']) \n",
    "                x_new.append(x_new_item)\n",
    "            else:\n",
    "                d = numeric_params_dct[numeric_cols[count]]\n",
    "                x_new.append((item - d['mean'])/d['std'])\n",
    "                count += 1\n",
    "        return x_new\n",
    "\n",
    "    def process_data(self, fn):\n",
    "\n",
    "        print('read data')\n",
    "        data = self.read_file(fn)\n",
    "\n",
    "        print('create an iterable')\n",
    "        if self.sample == 'train':\n",
    "            if self.baseline_feats:\n",
    "                for row in data:\n",
    "                    x1 = row[self.cat_cols].values.tolist()\n",
    "                    x2 = row[self.numeric_cols].values.tolist()\n",
    "                    if self.normalize:\n",
    "                        x2 = self.transform_numeric_cols(\n",
    "                            self.numeric_params_dct, self.numeric_cols,\n",
    "                            x2)\n",
    "                    if self.title_features:\n",
    "                        item_cols = row[self.item_col].tolist()\n",
    "                        x2_new = []\n",
    "                        for i, item in enumerate(item_cols):\n",
    "                            x2_new.append(\n",
    "                                x2[i] + self.title_feats_dct[str(item)])\n",
    "                        x2 = x2_new\n",
    "                    y = row[self.dv_col].tolist()\n",
    "                    yield (x1, x2, y)\n",
    "            else:\n",
    "                for row in data:\n",
    "                    user = row[self.user_col].tolist()\n",
    "                    item = row[self.item_col].tolist()\n",
    "                    y = row[self.dv_col].tolist()\n",
    "                    yield (user, item), y\n",
    "        else:\n",
    "            if self.baseline_feats:\n",
    "                for i, row in data.iterrows():\n",
    "                    x1 = row[self.cat_cols].tolist()\n",
    "                    x2 = row[self.numeric_cols].tolist()\n",
    "                    if self.normalize:\n",
    "                        x2 = self.transform_numeric_cols(\n",
    "                            self.numeric_params_dct, self.numeric_cols,\n",
    "                            x2)\n",
    "                    if self.title_features:\n",
    "                        item = row[self.item_col]\n",
    "                        x2 += self.title_feats_dct[str(item)]\n",
    "                    y = row[self.dv_col]\n",
    "                    yield (x1, x2, y)\n",
    "            else:\n",
    "                for i, row in data.iterrows():\n",
    "                    yield (row[self.user_col],\n",
    "                           row[self.item_col]), row[self.dv_col]\n",
    "\n",
    "    def get_stream(self, files):\n",
    "        return chain.from_iterable(map(self.process_data, files))\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.get_stream(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines the neural network for tabular data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_sizes, n_cont):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList(\n",
    "            [nn.Embedding(categories, size) for\n",
    "             categories, size in embedding_sizes])\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeddings)\n",
    "        self.n_emb, self.n_cont = n_emb, n_cont\n",
    "        self.lin1 = nn.Linear(self.n_emb + self.n_cont, 300)\n",
    "        self.lin2 = nn.Linear(300, 100)\n",
    "        self.lin3 = nn.Linear(100, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(self.n_cont)\n",
    "        self.bn2 = nn.BatchNorm1d(300)\n",
    "        self.bn3 = nn.BatchNorm1d(100)\n",
    "        self.emb_drop = nn.Dropout(0.6)\n",
    "        self.drops = nn.Dropout(0.3)\n",
    "\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        x = [e(x_cat[:, i]) for i, e in enumerate(self.embeddings)]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.emb_drop(x)\n",
    "        x2 = self.bn1(x_cont)\n",
    "        x = torch.cat([x, x2], 1)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.lin3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time\n",
    "import torch.optim as torch_optim\n",
    "import torch.nn.functional as F\n",
    "from torch import tensor\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "class Train(object):\n",
    "    \n",
    "    def __init__(self, loss_fn=nn.MSELoss(reduction='sum'), file_num=1,\n",
    "                 n_users=480189, n_items=17770, n_cont=15,\n",
    "                 min_emb_dim=100, cat_cols=['User', 'Movie'],\n",
    "                 lr=0.02, wd=0.00001):\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "                       else torch.device('cpu'))\n",
    "        self.file_num = file_num\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.n_cont = n_cont\n",
    "        self.cat_cols = cat_cols\n",
    "        self.min_emb_dim = min_emb_dim\n",
    "        self.embedding_sizes = self.choose_embedding_size(\n",
    "            self.cat_cols, [self.n_users, self.n_items],\n",
    "            self.min_emb_dim)\n",
    "        self.model = TabularModel(self.embedding_sizes, self.n_cont)\n",
    "        self.model.to(self.device)\n",
    "        self.lr = lr\n",
    "        self.wd = wd\n",
    "        self.optimizer = self.get_optimizer(self.model, lr=self.lr,\n",
    "                                            wd=self.wd)\n",
    "        \n",
    "    def choose_embedding_size(self, cat_cols, cat_num_values,\n",
    "                              min_emb_dim=100):\n",
    "        \"\"\"\n",
    "        cat_cols: list of categorical columns\n",
    "        cat_num_values: list of number of unique values for each\n",
    "        categorical column\n",
    "        \"\"\"\n",
    "        embedded_cols = dict(zip(cat_cols, cat_num_values))\n",
    "        embedding_sizes = [\n",
    "            (n_categories, min(min_emb_dim, (n_categories+1)//2))\n",
    "             for _, n_categories in embedded_cols.items()]\n",
    "        return embedding_sizes\n",
    "    \n",
    "    def get_optimizer(self, model, lr = 0.001, wd = 0.0):\n",
    "        parameters = filter(lambda p: p.requires_grad,\n",
    "                            model.parameters())\n",
    "        optim = torch_optim.Adam(parameters, lr=lr, weight_decay=wd)\n",
    "        return optim\n",
    "    \n",
    "    def construct_tensor(self, a):\n",
    "        final = []\n",
    "        for i in a:\n",
    "            out = []\n",
    "            for j in i:\n",
    "                out.append(j.tolist())\n",
    "            out1 = []\n",
    "            for item in zip(*out):\n",
    "                out1.append(list(item))\n",
    "            final += out1\n",
    "        return tensor(final)\n",
    "\n",
    "\n",
    "    def construct_tensor_test(self, a):\n",
    "        out = []\n",
    "        for i in a:\n",
    "            out.append(i.tolist())\n",
    "            out1 = []\n",
    "            for item in zip(*out):\n",
    "                out1.append(list(item))\n",
    "        return tensor(out1)\n",
    "\n",
    "\n",
    "    def construct_tensor_y(self, a):\n",
    "        out = []\n",
    "        for i in a:\n",
    "            out += i.tolist()\n",
    "        return tensor(out)\n",
    "    \n",
    "    def train(self, train_dl, train_size, chunksize, batch_size):\n",
    "        self.model.train()\n",
    "        total = 0\n",
    "        sum_loss = 0\n",
    "        with tqdm(total=train_size // (batch_size * chunksize)) as pbar:\n",
    "            for x1, x2, y in train_dl:\n",
    "                x1, x2, y = (self.construct_tensor(x1),\n",
    "                             self.construct_tensor(x2),\n",
    "                             self.construct_tensor_y(y))\n",
    "                x1 = x1.to(self.device)\n",
    "                x2 = x2.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                batch = y.size()[0]\n",
    "                y = y.reshape((y.size()[0], 1))\n",
    "                output = self.model(x1, x2)\n",
    "                loss = self.loss_fn(output, y)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total += batch\n",
    "                sum_loss += loss.item()\n",
    "                pbar.update(1)\n",
    "        return sum_loss/total\n",
    "    \n",
    "    def evaluate(self, valid_dl, test_size, batch_size):\n",
    "        self.model.eval()\n",
    "        total = 0\n",
    "        sum_loss = 0\n",
    "        with tqdm(total=test_size // (batch_size)) as pbar:\n",
    "            for x1, x2, y in valid_dl:\n",
    "                x1, x2 = (self.construct_tensor_test(x1),\n",
    "                          self.construct_tensor_test(x2))\n",
    "                x1 = x1.to(self.device)\n",
    "                x2 = x2.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                current_batch_size = y.size()[0]\n",
    "                y = y.reshape((y.size()[0], 1))\n",
    "                y = y.float()\n",
    "                out = self.model(x1, x2)\n",
    "                loss = self.loss_fn(out, y)\n",
    "                sum_loss += loss.item()\n",
    "                total += current_batch_size\n",
    "                pbar.update(1)\n",
    "        print(\"valid loss %.3f\" % (sum_loss/total))\n",
    "\n",
    "        return sum_loss/total\n",
    "    \n",
    "    def batch_fit(self, train_dl, valid_dl, epochs, train_size,\n",
    "                  test_size, chunksize, batch_size):\n",
    "        start = time.time()\n",
    "        losses = []\n",
    "        for i in range(epochs):\n",
    "            stats = {'epoch': i+1}\n",
    "            train_loss = self.train(train_dl, train_size, chunksize,\n",
    "                                    batch_size)\n",
    "            print(\"training loss: \", train_loss)\n",
    "            stats['train_loss'] = train_loss\n",
    "            test_loss = self.evaluate(valid_dl, test_size, batch_size)\n",
    "            print('time taken: %0.2f' % (time.time() - start))\n",
    "            stats['test_loss'] = test_loss\n",
    "            losses.append(stats)\n",
    "        return losses\n",
    "    \n",
    "    def predict(self, test_dl):\n",
    "        preds = []\n",
    "        actuals = []\n",
    "        with torch.no_grad():\n",
    "            for x1, x2, y in test_dl:\n",
    "                x1, x2 = (self.construct_tensor_test(x1),\n",
    "                          self.construct_tensor_test(x2))\n",
    "                x1 = x1.to(self.device)\n",
    "                x2 = x2.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                y = y.reshape((y.size()[0], 1))\n",
    "                pred = self.model(x1, x2)\n",
    "                preds.append(pred.tolist())\n",
    "                actuals.append(y.tolist())\n",
    "        final_preds = [item for sublist in preds for item in sublist]\n",
    "        final_actuals = [item for sublist in actuals for item in sublist]\n",
    "        rmse = np.sqrt(mean_squared_error(y_true=final_actuals,\n",
    "                                          y_pred=final_preds))\n",
    "        return final_actuals, final_preds, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "1. Create embeddings for each movie based on tf-idf+SVD computed on titles and then use them in a NN along with other features\n",
    "2. Use pretrained word embeddings for each word in the movie title and combine them using an average. Then, use them in a NN along with other features\n",
    "3. Train embeddings from scratch on movie titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1 - TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(train, max_df=0.5, max_features=10000,\n",
    "                    min_df=20, ngram_range=(1, 3), n_components=100):\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_df=max_df, max_features=max_features,\n",
    "                              min_df=min_df, ngram_range=ngram_range)),\n",
    "    ('svd', TruncatedSVD(n_components=n_components, random_state=6062018)),\n",
    "    ('normalize', Normalizer(copy=False))])\n",
    "\n",
    "    start = time.time()\n",
    "    print('fitting begins')\n",
    "    X_train_trans = pipeline.fit_transform(train)\n",
    "    print('time taken: %0.f' % (time.time() - start))\n",
    "\n",
    "    print('shape of transformed train: ', X_train_trans.shape)\n",
    "\n",
    "    print('Record pipeline metrics')\n",
    "    svd = pipeline.steps[1][1]\n",
    "    num_components = len(svd.components_)\n",
    "    explained_var_ratio = svd.explained_variance_ratio_.sum()\n",
    "    tfidf = pipeline.steps[0][1]\n",
    "    feature_names = tfidf.get_feature_names()\n",
    "\n",
    "    comp_feats = {}\n",
    "    for t in range(5):\n",
    "        best_features = [feature_names[i] for i in svd.components_[t].argsort()[::-1]]\n",
    "        comp_feats['comp_{}'.format(t)] = best_features[:100]\n",
    "\n",
    "    comp_feats = pd.DataFrame(comp_feats)\n",
    "    print('shape of DF of top features from top 5 components: ',\n",
    "          comp_feats.shape)\n",
    "    comp_feats['num_components'] = num_components\n",
    "    comp_feats['explained_var_ratio'] = explained_var_ratio\n",
    "\n",
    "    print('create transformed train DF')\n",
    "    column_names = ['comp_{}'.format(i) for i in range(num_components)]\n",
    "    df_train_trans = pd.DataFrame(X_train_trans, columns=column_names)\n",
    "\n",
    "    return pipeline, comp_feats, df_train_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text, stop):\n",
    "    text = regex_cleaning(text)\n",
    "    text = \" \".join([w for w in nltk.word_tokenize(text)\n",
    "                     if w not in stop])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_titles.fillna({'Name': ''}, inplace=True)\n",
    "stop = set(stopwords.words('english'))\n",
    "movie_titles['cleaned_name'] = movie_titles['Name'].apply(\n",
    "    lambda x: preprocessing(str(x), stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17770,)\n"
     ]
    }
   ],
   "source": [
    "X_train = movie_titles['cleaned_name'].values\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "max_df = 0.9\n",
    "min_df = 1\n",
    "max_features = 8000\n",
    "n_components = 600\n",
    "ngram_range=(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting begins\n",
      "time taken: 7\n",
      "shape of transformed train:  (17770, 600)\n",
      "Record pipeline metrics\n",
      "shape of DF of top features from top 5 components:  (100, 5)\n",
      "create transformed train DF\n"
     ]
    }
   ],
   "source": [
    "pipeline, comp_feats, df_train_trans = create_pipeline(\n",
    "    train=X_train, max_df=max_df, ngram_range=ngram_range,\n",
    "    max_features=max_features, min_df=min_df,\n",
    "    n_components=n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 7)\n",
      "           comp_0          comp_1      comp_2      comp_3             comp_4  \\\n",
      "0          season        material        love         man                vol   \n",
      "1            show  bonus material         man  spider man           twilight   \n",
      "2     show season           bonus       story      spider               zone   \n",
      "3  friends season            love  love story      little      twilight zone   \n",
      "4         friends             man       death    thin man  twilight zone vol   \n",
      "\n",
      "   num_components  explained_var_ratio  \n",
      "0             600             0.424208  \n",
      "1             600             0.424208  \n",
      "2             600             0.424208  \n",
      "3             600             0.424208  \n",
      "4             600             0.424208  \n"
     ]
    }
   ],
   "source": [
    "print(comp_feats.shape)\n",
    "print(comp_feats.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17770, 602)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comp_0</th>\n",
       "      <th>comp_1</th>\n",
       "      <th>comp_2</th>\n",
       "      <th>comp_3</th>\n",
       "      <th>comp_4</th>\n",
       "      <th>comp_5</th>\n",
       "      <th>comp_6</th>\n",
       "      <th>comp_7</th>\n",
       "      <th>comp_8</th>\n",
       "      <th>comp_9</th>\n",
       "      <th>...</th>\n",
       "      <th>comp_592</th>\n",
       "      <th>comp_593</th>\n",
       "      <th>comp_594</th>\n",
       "      <th>comp_595</th>\n",
       "      <th>comp_596</th>\n",
       "      <th>comp_597</th>\n",
       "      <th>comp_598</th>\n",
       "      <th>comp_599</th>\n",
       "      <th>Id</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.006528</td>\n",
       "      <td>0.001948</td>\n",
       "      <td>0.009814</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.001848</td>\n",
       "      <td>0.004495</td>\n",
       "      <td>0.002681</td>\n",
       "      <td>0.002830</td>\n",
       "      <td>0.001798</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018909</td>\n",
       "      <td>-0.011516</td>\n",
       "      <td>0.018872</td>\n",
       "      <td>0.006439</td>\n",
       "      <td>-0.025365</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>-0.006840</td>\n",
       "      <td>-0.009207</td>\n",
       "      <td>1</td>\n",
       "      <td>2003.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.017549</td>\n",
       "      <td>0.104052</td>\n",
       "      <td>0.285407</td>\n",
       "      <td>0.936571</td>\n",
       "      <td>-0.022265</td>\n",
       "      <td>-0.024887</td>\n",
       "      <td>-0.040205</td>\n",
       "      <td>-0.011232</td>\n",
       "      <td>-0.000946</td>\n",
       "      <td>-0.004132</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006473</td>\n",
       "      <td>0.006893</td>\n",
       "      <td>-0.001241</td>\n",
       "      <td>-0.012508</td>\n",
       "      <td>-0.001735</td>\n",
       "      <td>-0.000228</td>\n",
       "      <td>0.007632</td>\n",
       "      <td>0.014738</td>\n",
       "      <td>2</td>\n",
       "      <td>2004.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000823</td>\n",
       "      <td>-0.002925</td>\n",
       "      <td>-0.002762</td>\n",
       "      <td>-0.001771</td>\n",
       "      <td>-0.012129</td>\n",
       "      <td>-0.013147</td>\n",
       "      <td>-0.020850</td>\n",
       "      <td>-0.015118</td>\n",
       "      <td>0.014392</td>\n",
       "      <td>-0.000394</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046170</td>\n",
       "      <td>-0.011522</td>\n",
       "      <td>0.009152</td>\n",
       "      <td>0.001655</td>\n",
       "      <td>0.032514</td>\n",
       "      <td>0.001405</td>\n",
       "      <td>0.003044</td>\n",
       "      <td>0.006212</td>\n",
       "      <td>3</td>\n",
       "      <td>1997.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>0.008167</td>\n",
       "      <td>0.008958</td>\n",
       "      <td>0.004294</td>\n",
       "      <td>0.004266</td>\n",
       "      <td>0.010011</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006488</td>\n",
       "      <td>0.003249</td>\n",
       "      <td>-0.003207</td>\n",
       "      <td>-0.001946</td>\n",
       "      <td>0.001793</td>\n",
       "      <td>-0.002254</td>\n",
       "      <td>-0.003513</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>4</td>\n",
       "      <td>1994.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002470</td>\n",
       "      <td>-0.000727</td>\n",
       "      <td>0.000769</td>\n",
       "      <td>0.004570</td>\n",
       "      <td>0.011066</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.001770</td>\n",
       "      <td>-0.000768</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001975</td>\n",
       "      <td>0.026948</td>\n",
       "      <td>0.005072</td>\n",
       "      <td>-0.007858</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>-0.002845</td>\n",
       "      <td>0.012002</td>\n",
       "      <td>0.007192</td>\n",
       "      <td>5</td>\n",
       "      <td>2004.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 602 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     comp_0    comp_1    comp_2    comp_3    comp_4    comp_5    comp_6  \\\n",
       "0  0.000290  0.006528  0.001948  0.009814  0.001207  0.001848  0.004495   \n",
       "1  0.017549  0.104052  0.285407  0.936571 -0.022265 -0.024887 -0.040205   \n",
       "2  0.000823 -0.002925 -0.002762 -0.001771 -0.012129 -0.013147 -0.020850   \n",
       "3  0.000265  0.000591  0.000562  0.000784  0.001527  0.008167  0.008958   \n",
       "4  0.000213  0.002631  0.002470 -0.000727  0.000769  0.004570  0.011066   \n",
       "\n",
       "     comp_7    comp_8    comp_9  ...  comp_592  comp_593  comp_594  comp_595  \\\n",
       "0  0.002681  0.002830  0.001798  ...  0.018909 -0.011516  0.018872  0.006439   \n",
       "1 -0.011232 -0.000946 -0.004132  ... -0.006473  0.006893 -0.001241 -0.012508   \n",
       "2 -0.015118  0.014392 -0.000394  ... -0.046170 -0.011522  0.009152  0.001655   \n",
       "3  0.004294  0.004266  0.010011  ... -0.006488  0.003249 -0.003207 -0.001946   \n",
       "4  0.001506  0.001770 -0.000768  ... -0.001975  0.026948  0.005072 -0.007858   \n",
       "\n",
       "   comp_596  comp_597  comp_598  comp_599  Id    Year  \n",
       "0 -0.025365  0.008200 -0.006840 -0.009207   1  2003.0  \n",
       "1 -0.001735 -0.000228  0.007632  0.014738   2  2004.0  \n",
       "2  0.032514  0.001405  0.003044  0.006212   3  1997.0  \n",
       "3  0.001793 -0.002254 -0.003513  0.000935   4  1994.0  \n",
       "4  0.000405 -0.002845  0.012002  0.007192   5  2004.0  \n",
       "\n",
       "[5 rows x 602 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_trans = pd.concat([df_train_trans, movie_titles[['Id', 'Year']]],\n",
    "                           axis=1)\n",
    "print(df_train_trans.shape)\n",
    "df_train_trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index          128\n",
       "comp_0      142160\n",
       "comp_1      142160\n",
       "comp_2      142160\n",
       "comp_3      142160\n",
       "             ...  \n",
       "comp_597    142160\n",
       "comp_598    142160\n",
       "comp_599    142160\n",
       "Id          142160\n",
       "Year        142160\n",
       "Length: 603, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_trans.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp_0      float32\n",
      "comp_1      float32\n",
      "comp_2      float32\n",
      "comp_3      float32\n",
      "comp_4      float32\n",
      "             ...   \n",
      "comp_595    float32\n",
      "comp_596    float32\n",
      "comp_597    float32\n",
      "comp_598    float32\n",
      "comp_599    float32\n",
      "Length: 600, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index          128\n",
       "comp_0       71080\n",
       "comp_1       71080\n",
       "comp_2       71080\n",
       "comp_3       71080\n",
       "             ...  \n",
       "comp_597     71080\n",
       "comp_598     71080\n",
       "comp_599     71080\n",
       "Id          142160\n",
       "Year        142160\n",
       "Length: 603, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [x for x in list(df_train_trans.columns) if x.startswith('comp_')]\n",
    "\n",
    "for col in cols:\n",
    "    df_train_trans[col] = df_train_trans[col].astype(np.float32)\n",
    "    \n",
    "print(df_train_trans[cols].dtypes)\n",
    "df_train_trans.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert df to a dict\n",
    "df_train_trans['comp_feat'] = df_train_trans[cols].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# read item2idx dct\n",
    "item2idx = json.load(open(ITEM2IDX_FN))\n",
    "table = df_train_trans[['Id', 'comp_feat']]\n",
    "table['Id'] = table['Id'].apply(lambda x: item2idx[str(x)])\n",
    "comp_dct = dict(zip(table['Id'], table['comp_feat']))\n",
    "del table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17760, 17761, 17762, 17763, 17764, 17765, 17766, 17767, 17768, 17769]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(comp_dct.keys())[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "MOVIE_METADATA_DIR = os.path.join(DATA_DIR, 'movie_metadata')\n",
    "MOVIE_TITLES_TFIDF_COMPS_FN = os.path.join(\n",
    "    MOVIE_METADATA_DIR, 'movie_titles_tfidf_comps.json')\n",
    "MOVIE_TITLES_TFIDF_FEAT_IMP_FN = os.path.join(\n",
    "    MOVIE_METADATA_DIR, 'movie_titles_tfidf_feat_imp.csv')\n",
    "MOVIE_TITLES_TFIDF_PIPELINE_FN = os.path.join(\n",
    "    MOVIE_METADATA_DIR, 'movie_titles_tfidf_pipeline.pkl')\n",
    "\n",
    "json.dump(comp_dct, open(MOVIE_TITLES_TFIDF_COMPS_FN, 'w'))\n",
    "comp_feats.to_csv(MOVIE_TITLES_TFIDF_FEAT_IMP_FN, index=False)\n",
    "joblib.dump(pipeline, open(MOVIE_TITLES_TFIDF_PIPELINE_FN, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2 - Bert Pretrained Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Using cached https://files.pythonhosted.org/packages/b9/46/b7d6c37d92d1bd65319220beabe4df845434930e3f30e42d3cfaecb74dc4/sentence-transformers-0.2.6.1.tar.gz\n",
      "Collecting transformers>=2.8.0 (from sentence-transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages (from sentence-transformers) (4.31.1)\n",
      "Requirement already satisfied, skipping upgrade: torch>=1.0.1 in /Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages (from sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages (from sentence-transformers) (1.17.0)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in /Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages (from sentence-transformers) (0.20.4)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages (from sentence-transformers) (1.2.1)\n",
      "Requirement already satisfied, skipping upgrade: nltk in /Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages (from sentence-transformers) (3.4)\n",
      "Requirement already satisfied, skipping upgrade: requests in /Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages (from transformers>=2.8.0->sentence-transformers) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages (from transformers>=2.8.0->sentence-transformers) (2018.1.10)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages (from transformers>=2.8.0->sentence-transformers) (0.0.35)\n",
      "Collecting tokenizers==0.7.0 (from transformers>=2.8.0->sentence-transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/9b/fefc49f80e3b5cc48f0b1c8aa2c25f673735b70b0984810f5cc3c8438175/tokenizers-0.7.0-cp36-cp36m-macosx_10_10_x86_64.whl (1.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.2MB 2.1MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: filelock in /Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages (from transformers>=2.8.0->sentence-transformers) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: sentencepiece in /Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages (from transformers>=2.8.0->sentence-transformers) (0.1.83)\n",
      "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages (from transformers>=2.8.0->sentence-transformers) (0.6)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages (from transformers>=2.8.0->sentence-transformers) (18.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages (from nltk->sentence-transformers) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: singledispatch in /Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages (from nltk->sentence-transformers) (3.4.0.3)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages (from requests->transformers>=2.8.0->sentence-transformers) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages (from requests->transformers>=2.8.0->sentence-transformers) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages (from requests->transformers>=2.8.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages (from requests->transformers>=2.8.0->sentence-transformers) (2019.3.9)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages (from sacremoses->transformers>=2.8.0->sentence-transformers) (0.13.2)\n",
      "Requirement already satisfied, skipping upgrade: click in /Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages (from sacremoses->transformers>=2.8.0->sentence-transformers) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages (from packaging->transformers>=2.8.0->sentence-transformers) (2.3.1)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/varunn/Library/Caches/pip/wheels/d7/fa/17/2b081a8cd8b0a86753fb0e9826b3cc19f0207062c0b2da7008\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: tokenizers, transformers, sentence-transformers\n",
      "  Found existing installation: transformers 2.1.1\n",
      "    Uninstalling transformers-2.1.1:\n",
      "      Successfully uninstalled transformers-2.1.1\n",
      "Successfully installed sentence-transformers-0.2.6.1 tokenizers-0.7.0 transformers-2.11.0\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/varunn/.virtualenvs/rasa/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_titles.fillna({'Name': ''}, inplace=True)\n",
    "movie_titles['cleaned_name'] = movie_titles['Name'].apply(\n",
    "    lambda x: regex_cleaning(str(x), only_alpha=False,\n",
    "                             only_alphanumeric=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Year</th>\n",
       "      <th>Name</th>\n",
       "      <th>cleaned_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>Dinosaur Planet</td>\n",
       "      <td>dinosaur planet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>Isle of Man TT 2004 Review</td>\n",
       "      <td>isle of man tt 2004 review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>Character</td>\n",
       "      <td>character</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>Paula Abdul's Get Up &amp; Dance</td>\n",
       "      <td>paula abduls get up &amp; dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>The Rise and Fall of ECW</td>\n",
       "      <td>the rise and fall of ecw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id    Year                          Name                 cleaned_name\n",
       "0   1  2003.0               Dinosaur Planet              dinosaur planet\n",
       "1   2  2004.0    Isle of Man TT 2004 Review   isle of man tt 2004 review\n",
       "2   3  1997.0                     Character                    character\n",
       "3   4  1994.0  Paula Abdul's Get Up & Dance  paula abduls get up & dance\n",
       "4   5  2004.0      The Rise and Fall of ECW     the rise and fall of ecw"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_titles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inps = movie_titles['cleaned_name'].tolist()\n",
    "embs = model.encode(inps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17770\n",
      "17770\n",
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "print(len(inps))\n",
    "print(len(embs))\n",
    "print(embs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17770\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "embs = [x.tolist() for x in embs]\n",
    "print(len(embs))\n",
    "print(len(embs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read item2idx dct\n",
    "item2idx = json.load(open(ITEM2IDX_FN))\n",
    "table = pd.DataFrame({'Id': movie_titles['Id'].tolist(), 'emb': embs})\n",
    "table['Id'] = table['Id'].apply(lambda x: item2idx[str(x)])\n",
    "comp_dct = dict(zip(table['Id'], table['emb']))\n",
    "del table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "MOVIE_TITLES_BERT_COMPS_FN = os.path.join(MOVIE_METADATA_DIR,\n",
    "                                          'movie_titles_bert_comps.json')\n",
    "json.dump(comp_dct, open(MOVIE_TITLES_BERT_COMPS_FN, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "FILE_NUM = 1\n",
    "N_USERS = 480189\n",
    "N_ITEMS = 17770\n",
    "N_CONT = 768+15\n",
    "BATCH_SIZE = 50\n",
    "CHUNKSIZE = 100\n",
    "TRAIN_SIZE = 22851074\n",
    "VAL_SIZE = 962152 \n",
    "TEST_SIZE = 240538"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/varunn/Documents/kaggle/netflix-prize-data/prepared_data_for_NN_modelling/user_train_data_1.h5']\n",
      "['/Users/varunn/Documents/kaggle/netflix-prize-data/prepared_data_for_NN_modelling/user_test_data_1.h5']\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = InteractionsStream(\n",
    "    file_num=FILE_NUM, baseline_feats=True, model_type='regression',\n",
    "    sample='train', chunksize=CHUNKSIZE, normalize=False,\n",
    "    title_features=True, title_feats_fn=MOVIE_TITLES_BERT_COMPS_FN)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                          shuffle=False)\n",
    "\n",
    "test_dataset = InteractionsStream(\n",
    "    file_num=FILE_NUM, baseline_feats=True, model_type='regression',\n",
    "    sample='test', normalize=False, title_features=True,\n",
    "    title_feats_fn=MOVIE_TITLES_BERT_COMPS_FN)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n",
      "create an iterable\n",
      "tensor([[161459,   2138],\n",
      "        [412629,   2861],\n",
      "        [362249,   3729],\n",
      "        ...,\n",
      "        [265782,   3289],\n",
      "        [122286,   4305],\n",
      "        [398320,   1072]])\n",
      "\n",
      "\n",
      "tensor([[ 2.3000e+01,  4.7958e+00,  2.5100e+02,  ...,  3.7790e-02,\n",
      "         -2.9324e-01,  2.6283e-01],\n",
      "        [ 0.0000e+00,  0.0000e+00,  1.2870e+03,  ...,  1.9136e+00,\n",
      "          7.0776e-01, -4.5362e-01],\n",
      "        [ 2.2000e+01,  4.6904e+00,  1.9290e+03,  ..., -3.3361e-01,\n",
      "         -2.6666e-01, -1.7506e-01],\n",
      "        ...,\n",
      "        [ 4.5000e+01,  6.7082e+00,  2.4300e+02,  ..., -3.0964e-01,\n",
      "          2.6775e-01, -4.6342e-01],\n",
      "        [ 2.4300e+02,  1.5588e+01,  5.0700e+02,  ...,  2.4133e-01,\n",
      "         -3.8084e-01, -2.6416e-01],\n",
      "        [ 4.9500e+02,  2.2249e+01,  5.8000e+02,  ...,  2.5979e-01,\n",
      "         -1.7485e-01, -3.9449e-01]])\n",
      "\n",
      "\n",
      "tensor([[4.],\n",
      "        [4.],\n",
      "        [5.],\n",
      "        ...,\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [5.]])\n",
      "torch.Size([5000, 783])\n",
      "torch.Size([5000, 1])\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "for x1, x2, y in islice(train_loader, 1):\n",
    "    x1, x2, y = (construct_tensor(x1), construct_tensor(x2),\n",
    "                 construct_tensor_y(y))\n",
    "    y = y.reshape((y.size()[0], 1))\n",
    "    print(x1)\n",
    "    print('\\n')\n",
    "    print(x2)\n",
    "    print('\\n')\n",
    "    print(y)\n",
    "    print(x2.shape)\n",
    "    print(y.shape)\n",
    "    print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor x1, x2, y in islice(test_loader, 1):\\n    x1, x2 = construct_tensor_test(x1), construct_tensor_test(x2)\\n    y = y.reshape((y.size()[0], 1))\\n    y = y.float()\\n    print(x1)\\n    print('\\n')\\n    print(x2)\\n    print('\\n')\\n    print(y)\\n    print(x2.shape)\\n    print(y.shape)\\n    out = model.model(x1, x2)\\n    print(out)\\n    loss = torch.nn.MSELoss(reduction='sum')(out, y)\\n    print(loss)\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for x1, x2, y in islice(test_loader, 1):\n",
    "    x1, x2 = construct_tensor_test(x1), construct_tensor_test(x2)\n",
    "    y = y.reshape((y.size()[0], 1))\n",
    "    y = y.float()\n",
    "    print(x1)\n",
    "    print('\\n')\n",
    "    print(x2)\n",
    "    print('\\n')\n",
    "    print(y)\n",
    "    print(x2.shape)\n",
    "    print(y.shape)\n",
    "    out = model.model(x1, x2)\n",
    "    print(out)\n",
    "    loss = torch.nn.MSELoss(reduction='sum')(out, y)\n",
    "    print(loss)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate train class\n",
    "\n",
    "model = Train(file_num=FILE_NUM, n_users=N_USERS, n_items=N_ITEMS,\n",
    "              n_cont=N_CONT, lr=0.02, wd=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabularModel(\n",
       "  (embeddings): ModuleList(\n",
       "    (0): Embedding(480189, 100)\n",
       "    (1): Embedding(17770, 100)\n",
       "  )\n",
       "  (lin1): Linear(in_features=983, out_features=300, bias=True)\n",
       "  (lin2): Linear(in_features=300, out_features=100, bias=True)\n",
       "  (lin3): Linear(in_features=100, out_features=1, bias=True)\n",
       "  (bn1): BatchNorm1d(783, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (emb_drop): Dropout(p=0.6, inplace=False)\n",
       "  (drops): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4570 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data\n",
      "create an iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4571it [4:49:00,  3.15s/it]                            \n",
      "  0%|          | 0/4810 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  0.8844701523971533\n",
      "read data\n",
      "create an iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4811it [31:31,  2.12it/s]                          \n",
      "  0%|          | 0/4570 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss 0.797\n",
      "time taken: 19232.30\n",
      "read data\n",
      "create an iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4571it [4:42:38,  3.09s/it]                            \n",
      "  0%|          | 0/4810 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  0.7908056428095602\n",
      "read data\n",
      "create an iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4811it [33:13,  1.38it/s]                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss 0.776\n",
      "time taken: 38184.52\n",
      "time taken: 38184.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "losses = model.batch_fit(train_dl=train_loader, valid_dl=test_loader,\n",
    "                         epochs=2, train_size=TRAIN_SIZE,\n",
    "                         test_size=TEST_SIZE, chunksize=CHUNKSIZE,\n",
    "                         batch_size=BATCH_SIZE)\n",
    "\n",
    "print('time taken: %0.2f' % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1,\n",
       "  'train_loss': 0.8844701523971533,\n",
       "  'test_loss': 0.7971235709575782},\n",
       " {'epoch': 2,\n",
       "  'train_loss': 0.7908056428095602,\n",
       "  'test_loss': 0.7762977449611403}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.881077604391997"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(losses[-1]['test_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = os.path.join(MODEL_DIR,\n",
    "                        \"NN_DenseFFNN_FBaselineAndTitleEmbBert_E2.pt\")\n",
    "torch.save(model.model.state_dict(), model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
